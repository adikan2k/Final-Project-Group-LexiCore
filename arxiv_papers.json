[
  {
    "paper_id": "2511.16671v1",
    "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "authors": [
      "Ziyu Guo",
      "Renrui Zhang",
      "Hongyu Li",
      "Manyuan Zhang",
      "Xinyan Chen",
      "Sifan Wang",
      "Yan Feng",
      "Peng Pei",
      "Pheng-Ann Heng"
    ],
    "abstract": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-20T18:59:52+00:00",
    "updated": "2025-11-20T18:59:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16671v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16664v1",
    "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "authors": [
      "Ali Taghibakhshi",
      "Sharath Turuvekere Sreenivas",
      "Saurav Muralidharan",
      "Ruisi Cai",
      "Marcin Chochowski",
      "Ameya Sunil Mahabaleshwarkar",
      "Yoshi Suhara",
      "Oluwatobi Olabiyi",
      "Daniel Korzekwa",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Jan Kautz",
      "Bryan Catanzaro",
      "Ashwath Aithal",
      "Nima Tajbakhsh",
      "Pavlo Molchanov"
    ],
    "abstract": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T18:59:21+00:00",
    "updated": "2025-11-20T18:59:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16664v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16654v1",
    "title": "Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems",
    "authors": [
      "Elias Lumer",
      "Alex Cardenas",
      "Matt Melich",
      "Myles Mason",
      "Sara Dieter",
      "Vamse Kumar Subbiah",
      "Pradeep Honaganahalli Basavaraju",
      "Roberto Hernandez"
    ],
    "abstract": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T18:56:49+00:00",
    "updated": "2025-11-20T18:56:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16654v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16639v1",
    "title": "Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs",
    "authors": [
      "Wei-Cheng Tseng",
      "David Harwath"
    ],
    "abstract": "Recent advancements in neural audio codecs have not only enabled superior audio compression but also enhanced speech synthesis techniques. Researchers are now exploring their potential as universal acoustic feature extractors for a broader range of speech processing tasks. Building on this trend, we introduce Codec2Vec, the first speech representation learning framework that relies exclusively on discrete audio codec units. This approach offers several advantages, including improved data storage and transmission efficiency, faster training, and enhanced data privacy. We explore masked prediction with various training target derivation strategies to thoroughly understand the effectiveness of this framework. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to continuous-input models while reducing storage requirements by up to 16.5x and training time by 2.3x, showcasing its scalability and efficiency.",
    "categories": [
      "eess.AS",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-20T18:46:15+00:00",
    "updated": "2025-11-20T18:46:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16639v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16635v1",
    "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction",
    "authors": [
      "Guolin Huang",
      "Wenting Chen",
      "Jiaqi Yang",
      "Xinheng Lyu",
      "Xiaoling Luo",
      "Sen Yang",
      "Xiaohan Xing",
      "Linlin Shen"
    ],
    "abstract": "Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-20T18:41:44+00:00",
    "updated": "2025-11-20T18:41:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16635v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16595v1",
    "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
    "authors": [
      "Boshen Xu",
      "Zihan Xiao",
      "Jiaze Li",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Jian Luan",
      "Qin Jin"
    ],
    "abstract": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-20T17:48:21+00:00",
    "updated": "2025-11-20T17:48:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16595v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16590v1",
    "title": "D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies",
    "authors": [
      "Sen Chen",
      "Tong Zhao",
      "Yi Bin",
      "Fei Ma",
      "Wenqi Shao",
      "Zheng Wang"
    ],
    "abstract": "Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-20T17:43:46+00:00",
    "updated": "2025-11-20T17:43:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16590v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16577v1",
    "title": "Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation",
    "authors": [
      "Kexin Zhao",
      "Ken Forbus"
    ],
    "abstract": "Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T17:32:15+00:00",
    "updated": "2025-11-20T17:32:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16577v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16544v1",
    "title": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue",
    "authors": [
      "Zachary Ellis",
      "Jared Joselowitz",
      "Yash Deo",
      "Yajie He",
      "Anna Kalygina",
      "Aisling Higham",
      "Mana Rahimzadeh",
      "Yan Jia",
      "Ibrahim Habli",
      "Ernest Lim"
    ],
    "abstract": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T16:59:20+00:00",
    "updated": "2025-11-20T16:59:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16544v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16543v1",
    "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation",
    "authors": [
      "Jiaheng Zhang",
      "Daqiang Zhang"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.   Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.   Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "published": "2025-11-20T16:59:16+00:00",
    "updated": "2025-11-20T16:59:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16543v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16540v1",
    "title": "Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks",
    "authors": [
      "Éloïse Benito-Rodriguez",
      "Einar Urdshals",
      "Jasmina Nasufi",
      "Nicky Pochinkov"
    ],
    "abstract": "Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T16:53:12+00:00",
    "updated": "2025-11-20T16:53:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16540v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16528v1",
    "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
    "authors": [
      "Özay Ezerceli",
      "Mahmoud El Hussieni",
      "Selva Taş",
      "Reyhan Bayraktar",
      "Fatma Betül Terzioğlu",
      "Yusuf Çelebi",
      "Yağız Asker"
    ],
    "abstract": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5$\\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\\times$ faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T16:42:21+00:00",
    "updated": "2025-11-20T16:42:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16528v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16518v1",
    "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
    "authors": [
      "Xiaoshuai Hao",
      "Lei Zhou",
      "Zhijian Huang",
      "Zhiwen Hou",
      "Yingbo Tang",
      "Lingfeng Zhang",
      "Guang Li",
      "Zheng Lu",
      "Shuhuai Ren",
      "Xianhui Meng",
      "Yuchen Zhang",
      "Jing Wu",
      "Jinghui Lu",
      "Chenxu Dang",
      "Jiayi Guan",
      "Jianhua Wu",
      "Zhiyi Hou",
      "Hanbing Li",
      "Shumeng Xia",
      "Mingliang Zhou",
      "Yinan Zheng",
      "Zihao Yue",
      "Shuhao Gu",
      "Hao Tian",
      "Yuannan Shen",
      "Jianwei Cui",
      "Wen Zhang",
      "Shaoqing Xu",
      "Bing Wang",
      "Haiyang Sun",
      "Zeyu Zhu",
      "Yuncheng Jiang",
      "Zibin Guo",
      "Chuhong Gong",
      "Chaofan Zhang",
      "Wenbo Ding",
      "Kun Ma",
      "Guang Chen",
      "Rui Cai",
      "Diyun Xiang",
      "Heng Qu",
      "Fuli Luo",
      "Hangjun Ye",
      "Long Chen"
    ],
    "abstract": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
    "categories": [
      "cs.RO",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-20T16:34:55+00:00",
    "updated": "2025-11-20T16:34:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16518v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16478v1",
    "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
    "authors": [
      "Elena V. Epure",
      "Yashar Deldjoo",
      "Bruno Sguerra",
      "Markus Schedl",
      "Manuel Moussallam"
    ],
    "abstract": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.   This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "published": "2025-11-20T15:46:27+00:00",
    "updated": "2025-11-20T15:46:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16478v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16470v1",
    "title": "Arctic-Extract Technical Report",
    "authors": [
      "Mateusz Chiliński",
      "Julita Ołtusek",
      "Wojciech Jaśkowski"
    ],
    "abstract": "Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T15:40:55+00:00",
    "updated": "2025-11-20T15:40:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16470v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16467v1",
    "title": "Anatomy of an Idiom: Tracing Non-Compositionality in Language Models",
    "authors": [
      "Andrew Gomes"
    ],
    "abstract": "We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T15:35:50+00:00",
    "updated": "2025-11-20T15:35:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16467v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16438v1",
    "title": "ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports",
    "authors": [
      "Sherine George",
      "Nithish Saji"
    ],
    "abstract": "We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T15:07:17+00:00",
    "updated": "2025-11-20T15:07:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16438v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16423v1",
    "title": "TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models",
    "authors": [
      "Li Zhang",
      "Zhongxuan Han",
      "XiaoHua Feng",
      "Jiaming Zhang",
      "Yuyuan Li",
      "Linbo Jiang",
      "Jianan Lin",
      "Chaochao Chen"
    ],
    "abstract": "Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-20T14:45:59+00:00",
    "updated": "2025-11-20T14:45:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16423v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16416v1",
    "title": "Classification of worldwide news articles by perceived quality, 2018-2024",
    "authors": [
      "Connor McElroy",
      "Thiago E. A. de Oliveira",
      "Chris Brogly"
    ],
    "abstract": "This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T14:41:41+00:00",
    "updated": "2025-11-20T14:41:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16416v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16397v1",
    "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
    "authors": [
      "Ren Ma",
      "Jiantao Qiu",
      "Chao Xu",
      "Pei Chu",
      "Kaiwen Liu",
      "Pengli Ren",
      "Yuan Qu",
      "Jiahui Peng",
      "Linfeng Hou",
      "Mengjie Liu",
      "Lindong Lu",
      "Wenchang Ning",
      "Jia Yu",
      "Rui Min",
      "Jin Shi",
      "Haojiong Chen",
      "Peng Zhang",
      "Wenjian Zhang",
      "Qian Jiang",
      "Zengjie Hu",
      "Guoqiang Yang",
      "Zhenxiang Li",
      "Fukai Shang",
      "Zhongying Tu",
      "Wentao Zhang",
      "Dahua Lin",
      "Conghui He"
    ],
    "abstract": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T14:15:23+00:00",
    "updated": "2025-11-20T14:15:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16397v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16353v1",
    "title": "Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies",
    "authors": [
      "Jonathan Kamp",
      "Lisa Beinborn",
      "Antske Fokkens"
    ],
    "abstract": "Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T13:39:20+00:00",
    "updated": "2025-11-20T13:39:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16353v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16345v1",
    "title": "NLP Datasets for Idiom and Figurative Language Tasks",
    "authors": [
      "Blake Matheny",
      "Phuong Minh Nguyen",
      "Minh Le Nguyen",
      "Stephanie Reynolds"
    ],
    "abstract": "Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T13:28:05+00:00",
    "updated": "2025-11-20T13:28:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16345v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16334v1",
    "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
    "authors": [
      "Kaichen Zhang",
      "Keming Wu",
      "Zuhao Yang",
      "Kairui Hu",
      "Bin Wang",
      "Ziwei Liu",
      "Xingxuan Li",
      "Lidong Bing"
    ],
    "abstract": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-20T13:11:45+00:00",
    "updated": "2025-11-20T13:11:45+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16334v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16331v1",
    "title": "Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement",
    "authors": [
      "Jiashu Yao",
      "Heyan Huang",
      "Shuang Zeng",
      "Chuwei Luo",
      "WangJie You",
      "Jie Tang",
      "Qingsong Liu",
      "Yuhang Guo",
      "Yangyang Kang"
    ],
    "abstract": "Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only \"simple\" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T13:10:52+00:00",
    "updated": "2025-11-20T13:10:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16331v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16324v1",
    "title": "SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning",
    "authors": [
      "Wei Xia",
      "Zhi-Hong Deng"
    ],
    "abstract": "With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T13:00:04+00:00",
    "updated": "2025-11-20T13:00:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16324v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16275v1",
    "title": "SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs",
    "authors": [
      "Xingtao Zhao",
      "Hao Peng",
      "Dingli Su",
      "Xianghua Zeng",
      "Chunyang Liu",
      "Jinzhi Liao",
      "Philip S. Yu"
    ],
    "abstract": "Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T11:54:12+00:00",
    "updated": "2025-11-20T11:54:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16275v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16221v1",
    "title": "Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions",
    "authors": [
      "Caixin Kang",
      "Yifei Huang",
      "Liangyang Ouyang",
      "Mingfang Zhang",
      "Ruicong Liu",
      "Yoichi Sato"
    ],
    "abstract": "Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-20T10:44:21+00:00",
    "updated": "2025-11-20T10:44:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16221v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16209v1",
    "title": "PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization",
    "authors": [
      "Huseein Jawad",
      "Nicolas Brunel"
    ],
    "abstract": "System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-20T10:25:45+00:00",
    "updated": "2025-11-20T10:25:45+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16209v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.16198v1",
    "title": "SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning",
    "authors": [
      "Sebastian Haan"
    ],
    "abstract": "Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.",
    "categories": [
      "cs.CL",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-20T10:05:21+00:00",
    "updated": "2025-11-20T10:05:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.16198v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15709v1",
    "title": "Tokenisation over Bounded Alphabets is Hard",
    "authors": [
      "Violeta Kastreva",
      "Philip Whittington",
      "Dennis Komm",
      "Tiago Pimentel"
    ],
    "abstract": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.",
    "categories": [
      "cs.CL",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T18:59:56+00:00",
    "updated": "2025-11-19T18:59:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15709v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15703v1",
    "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
    "authors": [
      "Beichen Zhang",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T18:59:04+00:00",
    "updated": "2025-11-19T18:59:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15703v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15690v1",
    "title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping",
    "authors": [
      "Yushi Huang",
      "Zining Wang",
      "Zhihang Yuan",
      "Yifu Ding",
      "Ruihao Gong",
      "Jinyang Guo",
      "Xianglong Liu",
      "Jun Zhang"
    ],
    "abstract": "Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T18:48:27+00:00",
    "updated": "2025-11-19T18:48:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15690v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15661v1",
    "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
    "authors": [
      "Yicheng He",
      "Chengsong Huang",
      "Zongxia Li",
      "Jiaxin Huang",
      "Yonghui Yang"
    ],
    "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T17:55:15+00:00",
    "updated": "2025-11-19T17:55:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15661v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15613v1",
    "title": "When to Think and When to Look: Uncertainty-Guided Lookback",
    "authors": [
      "Jing Bi",
      "Filippos Bellos",
      "Junjia Guo",
      "Yayuan Li",
      "Chao Huang",
      "Yunlong",
      "Tang",
      "Luchuan Song",
      "Susan Liang",
      "Zhongfei",
      "Zhang",
      "Jason J. Corso",
      "Chenliang Xu"
    ],
    "abstract": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T17:01:02+00:00",
    "updated": "2025-11-19T17:01:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15613v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15605v1",
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "authors": [
      "Senyu Fei",
      "Siyin Wang",
      "Li Ji",
      "Ao Li",
      "Shiduo Zhang",
      "Liming Liu",
      "Jinlong Hou",
      "Jingjing Gong",
      "Xianzhong Zhao",
      "Xipeng Qiu"
    ],
    "abstract": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
    "categories": [
      "cs.RO",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-19T16:52:23+00:00",
    "updated": "2025-11-19T16:52:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15605v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15574v1",
    "title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning",
    "authors": [
      "Qihao Yang",
      "Xuelin Wang",
      "Jiale Chen",
      "Xuelian Dong",
      "Yuxin Hao",
      "Tianyong Hao"
    ],
    "abstract": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T16:06:06+00:00",
    "updated": "2025-11-19T16:06:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15574v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15567v1",
    "title": "Computer-Use Agents as Judges for Generative User Interface",
    "authors": [
      "Kevin Qinghong Lin",
      "Siyuan Hu",
      "Linjie Li",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Philip Torr",
      "Mike Zheng Shou"
    ],
    "abstract": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T16:00:02+00:00",
    "updated": "2025-11-19T16:00:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15567v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15552v1",
    "title": "Multimodal Evaluation of Russian-language Architectures",
    "authors": [
      "Artem Chervyakov",
      "Ulyana Isaeva",
      "Anton Emelyanov",
      "Artem Safin",
      "Maria Tikhonova",
      "Alexander Kharitonov",
      "Yulia Lyakh",
      "Petr Surovtsev",
      "Denis Shevelev Vildan Saburov",
      "Vasily Konovalov",
      "Elisei Rykov",
      "Ivan Sviridov",
      "Amina Miftakhova",
      "Ilseyar Alimova",
      "Alexander Panchenko",
      "Alexander Kapitanov",
      "Alena Fenogenova"
    ],
    "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T15:43:53+00:00",
    "updated": "2025-11-19T15:43:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15552v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15512v1",
    "title": "Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis",
    "authors": [
      "Yves Pauli",
      "Jan-Bernard Marsman",
      "Finn Rabe",
      "Victoria Edkins",
      "Roya Hüppi",
      "Silvia Ciampelli",
      "Akhil Ratan Misra",
      "Nils Lang",
      "Wolfram Hinzen",
      "Iris Sommer",
      "Philipp Homan"
    ],
    "abstract": "The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T15:06:08+00:00",
    "updated": "2025-11-19T15:06:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15512v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15443v1",
    "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search",
    "authors": [
      "Ao Xie",
      "Jiahui Chen",
      "Quanzhi Zhu",
      "Xiaoze Jiang",
      "Zhiheng Qin",
      "Enyun Yu",
      "Han Li"
    ],
    "abstract": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "published": "2025-11-19T13:57:40+00:00",
    "updated": "2025-11-19T13:57:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15443v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15424v1",
    "title": "LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering",
    "authors": [
      "Yuanjie Zhu",
      "Liangwei Yang",
      "Ke Xu",
      "Weizhi Zhang",
      "Zihe Song",
      "Jindong Wang",
      "Philip S. Yu"
    ],
    "abstract": "Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T13:22:08+00:00",
    "updated": "2025-11-19T13:22:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15424v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15418v1",
    "title": "Building Robust and Scalable Multilingual ASR for Indian Languages",
    "authors": [
      "Arjun Gangwar",
      "Kaousheik Jayakumar",
      "S. Umesh"
    ],
    "abstract": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T13:17:16+00:00",
    "updated": "2025-11-19T13:17:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15418v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15408v1",
    "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
    "authors": [
      "Shanlin Zhou",
      "Xinpeng Wang",
      "Jianxun Lian",
      "Zhenghao Liu",
      "Laks V. S. Lakshmanan",
      "Xiaoyuan Yi",
      "Yongtao Hao"
    ],
    "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.MA",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T13:05:25+00:00",
    "updated": "2025-11-19T13:05:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15408v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15392v1",
    "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents",
    "authors": [
      "Sirui Chen",
      "Mengshi Zhao",
      "Lei Xu",
      "Yuying Zhao",
      "Beier Zhu",
      "Hanwang Zhang",
      "Shengjie Zhao",
      "Chaochao Lu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T12:38:43+00:00",
    "updated": "2025-11-19T12:38:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15392v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15383v1",
    "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
    "authors": [
      "Byungho Jo"
    ],
    "abstract": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T12:25:40+00:00",
    "updated": "2025-11-19T12:25:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15383v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15370v1",
    "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods",
    "authors": [
      "Guoqiang Liang",
      "Jingqian Gong",
      "Mengxuan Li",
      "Gege Lin",
      "Shuo Zhang"
    ],
    "abstract": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T11:57:22+00:00",
    "updated": "2025-11-19T11:57:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15370v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15355v1",
    "title": "HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning",
    "authors": [
      "Alexis Correa-Guillén",
      "Carlos Gómez-Rodríguez",
      "David Vilares"
    ],
    "abstract": "We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T11:31:32+00:00",
    "updated": "2025-11-19T11:31:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15355v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15323v1",
    "title": "SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs",
    "authors": [
      "Youwei Xiao",
      "Yuyang Zou",
      "Yun Liang"
    ],
    "abstract": "Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.   We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.",
    "categories": [
      "cs.PL",
      "cs.CL"
    ],
    "primary_category": "cs.PL",
    "published": "2025-11-19T10:39:45+00:00",
    "updated": "2025-11-19T10:39:45+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15323v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15304v1",
    "title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models",
    "authors": [
      "Piercosma Bisconti",
      "Matteo Prandi",
      "Federico Pierucci",
      "Francesco Giarrusso",
      "Marcantonio Bracale",
      "Marcello Galisai",
      "Vincenzo Suriani",
      "Olga Sorokoletova",
      "Federico Sartore",
      "Daniele Nardi"
    ],
    "abstract": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T10:14:08+00:00",
    "updated": "2025-11-19T10:14:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15304v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15291v1",
    "title": "MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews",
    "authors": [
      "Randa Zarnoufi"
    ],
    "abstract": "Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T09:56:16+00:00",
    "updated": "2025-11-19T09:56:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15291v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15266v1",
    "title": "ChartEditor: A Reinforcement Learning Framework for Robust Chart Editing",
    "authors": [
      "Liangyu Chen",
      "Yichen Xu",
      "Jianzhe Ma",
      "Yuqi Liu",
      "Donglu Yang",
      "Liang Zhang",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "abstract": "Chart editing reduces manual effort in visualization design. Typical benchmarks limited in data diversity and assume access to complete chart code, which is seldom in real-world scenarios. To address this gap, we present ChartEditVista, a comprehensive benchmark consisting of 7,964 samples spanning 31 chart categories. It encompasses diverse editing instructions and covers nearly all editable chart elements. The inputs in ChartEditVista include only the original chart image and natural language editing instructions, without the original chart codes. ChartEditVista is generated through a fully automated pipeline that produces, edits, and verifies charts, ensuring high-quality chart editing data. Besides, we introduce two novel fine-grained, rule-based evaluation metrics: the layout metric, which evaluates the position, size and color of graphical components; and the text metric, which jointly assesses textual content and font styling. Building on top of ChartEditVista, we present ChartEditor, a model trained using a reinforcement learning framework that incorporates a novel rendering reward to simultaneously enforce code executability and visual fidelity. Through extensive experiments and human evaluations, we demonstrate that ChartEditVista provides a robust evaluation, while ChartEditor consistently outperforms models with similar-scale and larger-scale on chart editing tasks.",
    "categories": [
      "cs.MM",
      "cs.CL"
    ],
    "primary_category": "cs.MM",
    "published": "2025-11-19T09:27:37+00:00",
    "updated": "2025-11-19T09:27:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15266v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15260v1",
    "title": "IndicGEC: Powerful Models, or a Measurement Mirage?",
    "authors": [
      "Sowmya Vajjala"
    ],
    "abstract": "In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T09:24:23+00:00",
    "updated": "2025-11-19T09:24:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15260v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15257v1",
    "title": "M, Toolchain and Language for Reusable Model Compilation",
    "authors": [
      "Hiep Hong Trinh",
      "Federico Ciccozzi",
      "Abu Naser Masud",
      "Marjan Sirjani",
      "Mikael Sjödin"
    ],
    "abstract": "Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "published": "2025-11-19T09:21:46+00:00",
    "updated": "2025-11-19T09:21:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15257v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15244v1",
    "title": "Context Cascade Compression: Exploring the Upper Limits of Text Compression",
    "authors": [
      "Fanfan Liu",
      "Haibo Qiu"
    ],
    "abstract": "Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T09:02:56+00:00",
    "updated": "2025-11-19T09:02:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15244v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15211v1",
    "title": "OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition",
    "authors": [
      "Xinli Tao",
      "Xin Dong",
      "Xuezhong Zhou"
    ],
    "abstract": "Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T08:02:55+00:00",
    "updated": "2025-11-19T08:02:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15211v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15210v1",
    "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
    "authors": [
      "Vladislav Pedashenko",
      "Laida Kushnareva",
      "Yana Khassan Nibal",
      "Eduard Tulchinskii",
      "Kristian Kuznetsov",
      "Vladislav Zharchinskii",
      "Yury Maximov",
      "Irina Piontkovskaya"
    ],
    "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T08:00:40+00:00",
    "updated": "2025-11-19T08:00:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15210v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15183v1",
    "title": "HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples",
    "authors": [
      "Rishikant Chigrupaatii",
      "Ponnada Sai Tulasi Kanishka",
      "Lalit Chandra Routhu",
      "Martin Patel Sama Supratheek Reddy",
      "Divyam Gupta",
      "Dasari Srikar",
      "Krishna Teja Kuchimanchi",
      "Rajiv Misra",
      "Rohun Tripathi"
    ],
    "abstract": "With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T07:11:00+00:00",
    "updated": "2025-11-19T07:11:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15183v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15163v1",
    "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs",
    "authors": [
      "Yang Wu",
      "Rujing Yao",
      "Tong Zhang",
      "Yufei Shi",
      "Zhuoren Jiang",
      "Zhushan Li",
      "Xiaozhong Liu"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T06:28:16+00:00",
    "updated": "2025-11-19T06:28:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15163v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15159v1",
    "title": "Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation",
    "authors": [
      "Firdavs Nasriddinov",
      "Rafal Kocielnik",
      "Anima Anandkumar",
      "Andrew J. Hung"
    ],
    "abstract": "High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T06:19:34+00:00",
    "updated": "2025-11-19T06:19:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15131v1",
    "title": "CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries",
    "authors": [
      "Hokuto Munakata",
      "Takehiro Imamura",
      "Taichi Nishimura",
      "Tatsuya Komatsu"
    ],
    "abstract": "We introduce CASTELLA, a human-annotated audio benchmark for the task of audio moment retrieval (AMR). Although AMR has various useful potential applications, there is still no established benchmark with real-world data. The early study of AMR trained the model with solely synthetic datasets. Moreover, the evaluation is based on annotated dataset of fewer than 100 samples. This resulted in less reliable reported performance. To ensure performance for applications in real-world environments, we present CASTELLA, a large-scale manually annotated AMR dataset. CASTELLA consists of 1,009, 213, and 640 audio recordings for train, valid, and test split, respectively, which is 24 times larger than the previous dataset. We also establish a baseline model for AMR using CASTELLA. Our experiments demonstrate that a model fine-tuned on CASTELLA after pre-training on the synthetic data outperformed a model trained solely on the synthetic data by 10.4 points in Recall1@0.7. CASTELLA is publicly available in https://h-munakata.github.io/CASTELLA-demo/.",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-19T05:19:34+00:00",
    "updated": "2025-11-19T05:19:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15131v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15074v1",
    "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents",
    "authors": [
      "Henrik Bradland",
      "Morten Goodwin",
      "Vladimir I. Zadorozhny",
      "Per-Arne Andersen"
    ],
    "abstract": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-19T03:27:14+00:00",
    "updated": "2025-11-19T03:27:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15074v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15069v1",
    "title": "ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression",
    "authors": [
      "Haoyong Wu",
      "Yongmei Liu"
    ],
    "abstract": "In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-19T03:20:06+00:00",
    "updated": "2025-11-19T03:20:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15069v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15059v1",
    "title": "Evaluating Multimodal Large Language Models on Vertically Written Japanese Text",
    "authors": [
      "Keito Sasagawa",
      "Shuhei Kurita",
      "Daisuke Kawahara"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T03:04:22+00:00",
    "updated": "2025-11-19T03:04:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15059v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15005v1",
    "title": "Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation",
    "authors": [
      "Moses Kiprono"
    ],
    "abstract": "Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T00:58:36+00:00",
    "updated": "2025-11-19T00:58:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15005v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14936v1",
    "title": "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding",
    "authors": [
      "Mathieu Dufour",
      "Andrew Duncan"
    ],
    "abstract": "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:51:04+00:00",
    "updated": "2025-11-18T21:51:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14936v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14900v1",
    "title": "Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis",
    "authors": [
      "Zehao Liu",
      "Wejieying Ren",
      "Jipeng Zhang",
      "Tianxiang Zhao",
      "Jingxi Zhu",
      "Xiaoting Li",
      "Vasant G. Honavar"
    ],
    "abstract": "The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.   To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T20:38:36+00:00",
    "updated": "2025-11-18T20:38:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14900v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14868v1",
    "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings",
    "authors": [
      "Xueying Ding",
      "Xingyue Huang",
      "Mingxuan Ju",
      "Liam Collins",
      "Yozen Liu",
      "Leman Akoglu",
      "Neil Shah",
      "Tong Zhao"
    ],
    "abstract": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T19:37:40+00:00",
    "updated": "2025-11-18T19:37:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14868v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14846v1",
    "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization",
    "authors": [
      "Yifeng Ding",
      "Hung Le",
      "Songyang Han",
      "Kangrui Ruan",
      "Zhenghui Jin",
      "Varun Kumar",
      "Zijian Wang",
      "Anoop Deoras"
    ],
    "abstract": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T19:01:16+00:00",
    "updated": "2025-11-18T19:01:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14846v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14709v1",
    "title": "Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance",
    "authors": [
      "Raha Aghaei",
      "Ali A. Kiaei",
      "Mahnaz Boush",
      "Mahan Rofoosheh",
      "Mohammad Zavvar"
    ],
    "abstract": "This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:50:39+00:00",
    "updated": "2025-11-18T17:50:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14709v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14696v1",
    "title": "Subword Tokenization Strategies for Kurdish Word Embeddings",
    "authors": [
      "Ali Salehi",
      "Cassandra L. Jacobs"
    ],
    "abstract": "We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\\% of test cases compared to 68.7\\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:33:32+00:00",
    "updated": "2025-11-18T17:33:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14696v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14693v1",
    "title": "Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances",
    "authors": [
      "Rishu Kumar Singh",
      "Navneet Shreya",
      "Sarmistha Das",
      "Apoorva Singh",
      "Sriparna Saha"
    ],
    "abstract": "Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:29:28+00:00",
    "updated": "2025-11-18T17:29:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14693v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14688v1",
    "title": "Ground Truth Generation for Multilingual Historical NLP using LLMs",
    "authors": [
      "Clovis Gladstone",
      "Zhao Fang",
      "Spencer Dean Stewart"
    ],
    "abstract": "Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:25:43+00:00",
    "updated": "2025-11-18T17:25:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14688v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14685v1",
    "title": "Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries",
    "authors": [
      "Kiera McCormick",
      "Rafael Martínez-Galarza"
    ],
    "abstract": "Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.",
    "categories": [
      "cs.CL",
      "astro-ph.IM"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:23:29+00:00",
    "updated": "2025-11-18T17:23:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14685v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14684v1",
    "title": "SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction",
    "authors": [
      "Biaojie Zeng",
      "Min Zhang",
      "Juan Zhou",
      "Fengrui Liu",
      "Ruiyang Huang",
      "Xin Lin"
    ],
    "abstract": "Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \\textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \\textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \\texttt{SMRC} (\\textit{\\underline{S}tudent \\underline{M}athematical \\underline{R}easoning \\underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \\texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \\textbf{solution accuracy} and \\textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \\texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:22:37+00:00",
    "updated": "2025-11-18T17:22:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14684v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14683v1",
    "title": "Quadratic Term Correction on Heaps' Law",
    "authors": [
      "Oscar Fontanelli",
      "Wentian Li"
    ],
    "abstract": "Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement\" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance\" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:22:00+00:00",
    "updated": "2025-11-18T17:22:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14683v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14671v1",
    "title": "Streamlining Industrial Contract Management with Retrieval-Augmented LLMs",
    "authors": [
      "Kristi Topollai",
      "Tolga Dimlioglu",
      "Anna Choromanska",
      "Simon Odie",
      "Reginald Hui"
    ],
    "abstract": "Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:10:57+00:00",
    "updated": "2025-11-18T17:10:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14671v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14662v1",
    "title": "Bias in, Bias out: Annotation Bias in Multilingual Large Language Models",
    "authors": [
      "Xia Cui",
      "Ziyi Huang",
      "Naeemeh Adel"
    ],
    "abstract": "Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T17:02:12+00:00",
    "updated": "2025-11-18T17:02:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14662v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14642v1",
    "title": "Graded strength of comparative illusions is explained by Bayesian inference",
    "authors": [
      "Yuhan Zhang",
      "Erxiao Wang",
      "Cory Shain"
    ],
    "abstract": "Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T16:33:19+00:00",
    "updated": "2025-11-18T16:33:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14642v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14638v1",
    "title": "A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases",
    "authors": [
      "Tao Yang",
      "Dandan Huang",
      "Yunting Lin",
      "Pengfei Wu",
      "Zhikun Wu",
      "Gangyuan Ma",
      "Yulan Lu",
      "Xinran Dong",
      "Dingpeng Li",
      "Junshuang Ge",
      "Zhiyan Zhang",
      "Xuanzhao Huang",
      "Wenyan Nong",
      "Yao Zhou",
      "Hui Tang",
      "Hongxi Yang",
      "Shijie Zhang",
      "Juan Li",
      "Xiaojun Cao",
      "Lin Yang",
      "Xia Gao",
      "Kaishou Xu",
      "Xiaoqiong Gu",
      "Wen Zhang",
      "Huimin Xia",
      "Li Liu",
      "Wenhao Zhou",
      "Mulin Jun Li"
    ],
    "abstract": "Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T16:29:19+00:00",
    "updated": "2025-11-18T16:29:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14638v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14631v1",
    "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities",
    "authors": [
      "Kahaan Gandhi",
      "Boris Bolliet",
      "Inigo Zubeldia"
    ],
    "abstract": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T16:23:02+00:00",
    "updated": "2025-11-18T16:23:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14631v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14606v1",
    "title": "Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models",
    "authors": [
      "Shreya Adrita Banik",
      "Niaz Nafi Rahman",
      "Tahsina Moiukh",
      "Farig Sadeque"
    ],
    "abstract": "Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:58:04+00:00",
    "updated": "2025-11-18T15:58:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14606v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14603v1",
    "title": "A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease",
    "authors": [
      "Yilu Fang",
      "Jordan G. Nestor",
      "Casey N. Ta",
      "Jerard Z. Kneifati-Hayek",
      "Chunhua Weng"
    ],
    "abstract": "Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:53:31+00:00",
    "updated": "2025-11-18T15:53:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14603v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14598v1",
    "title": "Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages",
    "authors": [
      "Noam Dahan",
      "Omer Kidron",
      "Gabriel Stanovsky"
    ],
    "abstract": "High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:39:48+00:00",
    "updated": "2025-11-18T15:39:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14598v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14566v1",
    "title": "Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak",
    "authors": [
      "Lucia Makaiová",
      "Martin Fajčík",
      "Antonín Jarolím"
    ],
    "abstract": "Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:09:09+00:00",
    "updated": "2025-11-18T15:09:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14566v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14531v1",
    "title": "LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation",
    "authors": [
      "David Carmel",
      "Simone Filice",
      "Guy Horowitz",
      "Yoelle Maarek",
      "Alex Shtoff",
      "Oren Somekh",
      "Ran Tavory"
    ],
    "abstract": "With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T14:34:35+00:00",
    "updated": "2025-11-18T14:34:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14531v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14460v1",
    "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
    "authors": [
      "Mingyue Cheng",
      "Jie Ouyang",
      "Shuo Yu",
      "Ruiran Yan",
      "Yucong Luo",
      "Zirui Liu",
      "Daoyu Wang",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T13:03:15+00:00",
    "updated": "2025-11-18T13:03:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14460v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14445v1",
    "title": "Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning",
    "authors": [
      "Trishala Jayesh Ahalpara"
    ],
    "abstract": "We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T12:43:04+00:00",
    "updated": "2025-11-18T12:43:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14445v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14439v2",
    "title": "MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents",
    "authors": [
      "Jinru Ding",
      "Lu Lu",
      "Chao Ding",
      "Mouxiao Bian",
      "Jiayuan Chen",
      "Wenrao Pang",
      "Ruiyao Chen",
      "Xinwei Peng",
      "Renjie Lu",
      "Sijie Ren",
      "Guanxu Zhu",
      "Xiaoqin Wu",
      "Zhiqiang Liu",
      "Rongzhao Zhang",
      "Luyi Jiang",
      "Bing Han",
      "Yunqiu Wang",
      "Jie Xu"
    ],
    "abstract": "Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T12:37:32+00:00",
    "updated": "2025-11-19T04:04:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14439v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14423v1",
    "title": "Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education",
    "authors": [
      "Xin Yi",
      "Yue Li",
      "Dongsheng Shi",
      "Linlin Wang",
      "Xiaoling Wang",
      "Liang He"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T12:27:51+00:00",
    "updated": "2025-11-18T12:27:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14423v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14385v1",
    "title": "Mitigating Label Length Bias in Large Language Models",
    "authors": [
      "Mario Sanz-Guerrero",
      "Katharina von der Wense"
    ],
    "abstract": "Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T11:45:24+00:00",
    "updated": "2025-11-18T11:45:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14385v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14368v1",
    "title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model",
    "authors": [
      "Rishi Gupta",
      "Mukilan Karuppasamy",
      "Shyam Marjit",
      "Aditay Tripathi",
      "Anirban Chakraborty"
    ],
    "abstract": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T11:18:08+00:00",
    "updated": "2025-11-18T11:18:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14368v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14366v1",
    "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning",
    "authors": [
      "Hongwei Liu",
      "Junnan Liu",
      "Shudong Liu",
      "Haodong Duan",
      "Yuqiang Li",
      "Mao Su",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Xinyu Fang",
      "Qianhong Ma",
      "Taolin Zhang",
      "Zihan Ma",
      "Yufeng Zhao",
      "Peiheng Zhou",
      "Linchen Xiao",
      "Wenlong Zhang",
      "Shijie Zhou",
      "Xingjian Ma",
      "Siqi Sun",
      "Jiaye Ge",
      "Meng Li",
      "Yuhong Liu",
      "Jianxin Dong",
      "Jiaying Li",
      "Hui Wu",
      "Hanwen Liang",
      "Jintai Lin",
      "Yanting Wang",
      "Jie Dong",
      "Tong Zhu",
      "Tianfan Fu",
      "Conghui He",
      "Qi Zhang",
      "Songyang Zhang",
      "Lei Bai",
      "Kai Chen"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T11:13:06+00:00",
    "updated": "2025-11-18T11:13:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14366v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14365v1",
    "title": "The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models",
    "authors": [
      "Prathamesh Kalamkar",
      "Ned Letcher",
      "Meissane Chami",
      "Sahger Lad",
      "Shayan Mohanty",
      "Prasanna Pendse"
    ],
    "abstract": "The application of large language models (LLMs) to chemistry is frequently hampered by a \"tokenization bottleneck\", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T11:12:35+00:00",
    "updated": "2025-11-18T11:12:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14365v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14362v1",
    "title": "SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature",
    "authors": [
      "Hang Ding",
      "Yilun Zhao",
      "Tiansheng Hu",
      "Manasi Patwardhan",
      "Arman Cohan"
    ],
    "abstract": "The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.",
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "published": "2025-11-18T11:09:19+00:00",
    "updated": "2025-11-18T11:09:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14362v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14342v2",
    "title": "ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions",
    "authors": [
      "Xingwei He",
      "Qianru Zhang",
      "Pengfei Chen",
      "Guanhua Chen",
      "Linlin Yu",
      "Yuan Yuan",
      "Siu-Ming Yiu"
    ],
    "abstract": "Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T10:49:37+00:00",
    "updated": "2025-11-19T09:06:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14342v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14301v1",
    "title": "Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion",
    "authors": [
      "Eric Xue",
      "Ruiyi Zhang",
      "Zijun Zhang",
      "Pengtao Xie"
    ],
    "abstract": "Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T09:56:16+00:00",
    "updated": "2025-11-18T09:56:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14301v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14299v1",
    "title": "DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning",
    "authors": [
      "Xiaochuan Liu",
      "Yuanfeng Song",
      "Xiaoming Yin",
      "Xing Chen"
    ],
    "abstract": "In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T09:54:13+00:00",
    "updated": "2025-11-18T09:54:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14299v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14295v1",
    "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
    "authors": [
      "Mohammad Zbib",
      "Hasan Abed Al Kader Hammoud",
      "Sina Mukalled",
      "Nadine Rizk",
      "Fatima Karnib",
      "Issam Lakkis",
      "Ammar Mohanna",
      "Bernard Ghanem"
    ],
    "abstract": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T09:47:01+00:00",
    "updated": "2025-11-18T09:47:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14295v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14275v1",
    "title": "Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space",
    "authors": [
      "Ante Wang",
      "Weizhi Ma",
      "Yang Liu"
    ],
    "abstract": "Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T09:09:23+00:00",
    "updated": "2025-11-18T09:09:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14275v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14258v1",
    "title": "Entropy-Guided Reasoning Compression",
    "authors": [
      "Hourun Zhu",
      "Yang Gao",
      "Wenlong Fei",
      "Jiawei Li",
      "Huashan Sun"
    ],
    "abstract": "Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T08:48:58+00:00",
    "updated": "2025-11-18T08:48:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14258v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13593v2",
    "title": "O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
    "authors": [
      "Piaohong Wang",
      "Motong Tian",
      "Jiaxian Li",
      "Yuan Liang",
      "Yuqing Wang",
      "Qianben Chen",
      "Tiannan Wang",
      "Zhicong Lu",
      "Jiawei Ma",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "abstract": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T16:55:19+00:00",
    "updated": "2025-11-18T13:20:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13593v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13590v1",
    "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
    "authors": [
      "Hao Wang",
      "Yuanfeng Song",
      "Xiaoming Yin",
      "Xing Chen"
    ],
    "abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T16:52:19+00:00",
    "updated": "2025-11-17T16:52:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13590v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13548v1",
    "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
    "authors": [
      "Siyang Cheng",
      "Gaotian Liu",
      "Rui Mei",
      "Yilin Wang",
      "Kejia Zhang",
      "Kaishuo Wei",
      "Yuqi Yu",
      "Weiping Wen",
      "Xiaojie Wu",
      "Junhua Liu"
    ],
    "abstract": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-17T16:19:21+00:00",
    "updated": "2025-11-17T16:19:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13548v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13529v1",
    "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
    "authors": [
      "Máté Gedeon",
      "Piroska Zsófia Barta",
      "Péter Mihajlik",
      "Tekla Etelka Gráczi",
      "Anna Kohári",
      "Katalin Mády"
    ],
    "abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T16:02:08+00:00",
    "updated": "2025-11-17T16:02:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13529v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13505v1",
    "title": "Applying Large Language Models to Characterize Public Narratives",
    "authors": [
      "Elinor Poole-Dayan",
      "Daniel T Kessler",
      "Hannah Chiou",
      "Margaret Hughes",
      "Emily S Lin",
      "Marshall Ganz",
      "Deb Roy"
    ],
    "abstract": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T15:41:55+00:00",
    "updated": "2025-11-17T15:41:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13505v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13481v1",
    "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns",
    "authors": [
      "Attapol T. Rutherford",
      "Sirisak Chueykamhang",
      "Thachaparn Bunditlurdruk",
      "Nanthicha Angsuwichitkul"
    ],
    "abstract": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T15:17:46+00:00",
    "updated": "2025-11-17T15:17:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13481v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13467v1",
    "title": "Non-Linear Scoring Model for Translation Quality Evaluation",
    "authors": [
      "Serge Gladkoff",
      "Lifeng Han",
      "Katerina Gasova"
    ],
    "abstract": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.   Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.   Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model   E(x) = a * ln(1 + b * x), a, b > 0,   anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.   The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T15:09:22+00:00",
    "updated": "2025-11-17T15:09:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13467v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13418v1",
    "title": "Exploring Multi-Table Retrieval Through Iterative Search",
    "authors": [
      "Allaa Boutaleb",
      "Bernd Amann",
      "Rafael Angarita",
      "Hubert Naacke"
    ],
    "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "published": "2025-11-17T14:31:33+00:00",
    "updated": "2025-11-17T14:31:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13418v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13415v1",
    "title": "Attention Grounded Enhancement for Visual Document Retrieval",
    "authors": [
      "Wanqing Cui",
      "Wei Huang",
      "Yazhi Guo",
      "Yibo Hu",
      "Meiguang Jin",
      "Junfeng Ma",
      "Keping Bi"
    ],
    "abstract": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "published": "2025-11-17T14:28:41+00:00",
    "updated": "2025-11-17T14:28:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13415v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13410v1",
    "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction",
    "authors": [
      "Zhaopei Huang",
      "Qifeng Dai",
      "Guozheng Wu",
      "Xiaopeng Wu",
      "Kehan Chen",
      "Chuan Yu",
      "Xubin Li",
      "Tiezheng Ge",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "abstract": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T14:22:32+00:00",
    "updated": "2025-11-17T14:22:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13410v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13381v1",
    "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts",
    "authors": [
      "Siyu Zhu",
      "Mouxiao Bian",
      "Yue Xie",
      "Yongyu Tang",
      "Zhikang Yu",
      "Tianbin Li",
      "Pengcheng Chen",
      "Bing Han",
      "Jie Xu",
      "Xiaoyan Dong"
    ],
    "abstract": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T13:54:00+00:00",
    "updated": "2025-11-17T13:54:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13381v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13368v1",
    "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
    "authors": [
      "Kajetan Dymkiewicz",
      "Ivan Vulic",
      "Helen Yannakoudakis",
      "Eilam Shapira",
      "Roi Reichart",
      "Anna Korhonen"
    ],
    "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T13:41:31+00:00",
    "updated": "2025-11-17T13:41:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13368v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13335v1",
    "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects",
    "authors": [
      "Maram Alharbi",
      "Salmane Chafik",
      "Saad Ezzini",
      "Ruslan Mitkov",
      "Tharindu Ranasinghe",
      "Hansi Hettiarachchi"
    ],
    "abstract": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T13:06:55+00:00",
    "updated": "2025-11-17T13:06:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13335v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13333v1",
    "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
    "authors": [
      "Alexandru-Mihai Apostu",
      "Andrei Preda",
      "Alexandra Daniela Damir",
      "Diana Bolocan",
      "Radu Tudor Ionescu",
      "Ioana Croitoru",
      "Mihaela Gaman"
    ],
    "abstract": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-17T13:05:25+00:00",
    "updated": "2025-11-17T13:05:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13333v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13329v1",
    "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
    "authors": [
      "Shufan Yang",
      "Zifeng Cheng",
      "Zhiwei Jiang",
      "Yafeng Yin",
      "Cong Wang",
      "Shiping Ge",
      "Yuchen Fu",
      "Qing Gu"
    ],
    "abstract": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T13:04:36+00:00",
    "updated": "2025-11-17T13:04:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13329v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13290v1",
    "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
    "authors": [
      "Jea Kwon",
      "Luiz Felipe Vecchietti",
      "Sungwon Park",
      "Meeyoung Cha"
    ],
    "abstract": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-17T12:13:15+00:00",
    "updated": "2025-11-17T12:13:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13290v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13254v1",
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "authors": [
      "Shalini Maiti",
      "Amar Budhiraja",
      "Bhavul Gauri",
      "Gaurav Chaurasia",
      "Anton Protopopov",
      "Alexis Audran-Reiss",
      "Michael Slater",
      "Despoina Magka",
      "Tatiana Shavrina",
      "Roberta Raileanu",
      "Yoram Bachrach"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T11:13:34+00:00",
    "updated": "2025-11-17T11:13:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13254v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13238v1",
    "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
    "authors": [
      "Patrick Parschan",
      "Charlott Jakob"
    ],
    "abstract": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T11:01:09+00:00",
    "updated": "2025-11-17T11:01:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13238v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13225v1",
    "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms",
    "authors": [
      "Tyler Loakman",
      "Joseph James",
      "Chenghua Lin"
    ],
    "abstract": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T10:41:07+00:00",
    "updated": "2025-11-17T10:41:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13225v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13801v1",
    "title": "Rdgai: Classifying transcriptional changes using Large Language Models with a test case from an Arabic Gospel tradition",
    "authors": [
      "Robert Turnbull"
    ],
    "abstract": "Application of phylogenetic methods to textual traditions has traditionally treated all changes as equivalent even though it is widely recognized that certain types of variants were more likely to be introduced than others. While it is possible to give weights to certain changes using a maximum parsimony evaluation criterion, it is difficult to state a priori what these weights should be. Probabilistic methods, such as Bayesian phylogenetics, allow users to create categories of changes, and the transition rates for each category can be estimated as part of the analysis. This classification of types of changes in readings also allows for inspecting the probability of these categories across each branch in the resulting trees. However, classification of readings is time-consuming, as it requires categorizing each reading against every other reading at each variation unit, presenting a significant barrier to entry for this kind of analysis. This paper presents Rdgai, a software package that automates this classification task using multi-lingual large language models (LLMs). The tool allows users to easily manually classify changes in readings and then it uses these annotations in the prompt for an LLM to automatically classify the remaining reading transitions. These classifications are stored in TEI XML and ready for downstream phylogenetic analysis. This paper demonstrates the application with data an Arabic translation of the Gospels.",
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "published": "2025-11-17T10:03:12+00:00",
    "updated": "2025-11-17T10:03:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13801v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13182v2",
    "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
    "authors": [
      "Mihai Dan Nadas",
      "Laura Diosan"
    ],
    "abstract": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T09:43:54+00:00",
    "updated": "2025-11-18T13:16:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13182v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13180v1",
    "title": "Translation Entropy: A Statistical Framework for Evaluating Translation Systems",
    "authors": [
      "Ronit D. Gross",
      "Yanir Harel",
      "Ido Kanter"
    ],
    "abstract": "The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T09:42:15+00:00",
    "updated": "2025-11-17T09:42:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13180v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13169v1",
    "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine",
    "authors": [
      "Tianai Huang",
      "Jiayuan Chen",
      "Lu Lu",
      "Pengcheng Chen",
      "Tianbin Li",
      "Bing Han",
      "Wenchao Tang",
      "Jie Xu",
      "Ming Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T09:15:41+00:00",
    "updated": "2025-11-17T09:15:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13169v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13159v1",
    "title": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis",
    "authors": [
      "Zaara Zabeen Arpa",
      "Sadnam Sakib Apurbo",
      "Nazia Karim Khan Oishee",
      "Ajwad Abrar"
    ],
    "abstract": "Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T09:06:01+00:00",
    "updated": "2025-11-17T09:06:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13152v1",
    "title": "Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels",
    "authors": [
      "Sourya Dipta Das",
      "Shubham Kumar",
      "Kuldeep Yadav"
    ],
    "abstract": "Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T09:00:26+00:00",
    "updated": "2025-11-17T09:00:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13152v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13126v1",
    "title": "A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition",
    "authors": [
      "Nigar Alishzade",
      "Gulchin Abdullayeva"
    ],
    "abstract": "This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T08:28:35+00:00",
    "updated": "2025-11-17T08:28:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13126v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13118v1",
    "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction",
    "authors": [
      "Quanjiang Guo",
      "Sijie Wang",
      "Jinchuan Zhang",
      "Ben Zhang",
      "Zhao Kang",
      "Ling Tian",
      "Ke Yan"
    ],
    "abstract": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T08:17:15+00:00",
    "updated": "2025-11-17T08:17:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13118v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13107v1",
    "title": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study",
    "authors": [
      "Zhichao He",
      "Mouxiao Bian",
      "Jianhong Zhu",
      "Jiayuan Chen",
      "Yunqiu Wang",
      "Wenxia Zhao",
      "Tianbin Li",
      "Bing Han",
      "Jie Xu",
      "Junyan Wu"
    ],
    "abstract": "The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T08:05:15+00:00",
    "updated": "2025-11-17T08:05:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13107v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13095v1",
    "title": "BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models",
    "authors": [
      "Chuyuan Li",
      "Giuseppe Carenini"
    ],
    "abstract": "We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T07:50:12+00:00",
    "updated": "2025-11-17T07:50:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13095v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13091v1",
    "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization",
    "authors": [
      "Yuhan Chen",
      "Yuxuan Liu",
      "Long Zhang",
      "Pengzhi Gao",
      "Jian Luan",
      "Wei Liu"
    ],
    "abstract": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-17T07:43:15+00:00",
    "updated": "2025-11-17T07:43:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13091v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13043v2",
    "title": "Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training",
    "authors": [
      "Xinyuan Zhou",
      "Yi Lei",
      "Xiaoyu Zhou",
      "Jingyi Sun",
      "Yu Zhu",
      "Zhongyi Ye",
      "Weitai Zhang",
      "Quan Liu",
      "Si Wei",
      "Cong Liu"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a \"CoT-augmented state prediction\" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover achieves state-of-the-art performance among similarly-sized open-source models within the \"Whole-Proof Generation\" paradigm. It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at: https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T06:44:02+00:00",
    "updated": "2025-11-18T11:35:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13043v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13040v1",
    "title": "How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm",
    "authors": [
      "Kasun Wickramasinghe",
      "Nisansa de Silva"
    ],
    "abstract": "Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T06:41:41+00:00",
    "updated": "2025-11-17T06:41:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13040v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13029v1",
    "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models",
    "authors": [
      "Declan Jackson",
      "William Keating",
      "George Cameron",
      "Micah Hill-Smith"
    ],
    "abstract": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T06:27:16+00:00",
    "updated": "2025-11-17T06:27:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13029v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13021v1",
    "title": "PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics",
    "authors": [
      "Sachin Vashistha",
      "Aryan Bibhuti",
      "Atharva Naik",
      "Martin Tutek",
      "Somak Aditya"
    ],
    "abstract": "Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-17T06:17:17+00:00",
    "updated": "2025-11-17T06:17:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13021v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12997v1",
    "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
    "authors": [
      "Genglin Liu",
      "Shijie Geng",
      "Sha Li",
      "Hejie Cui",
      "Sarah Zhang",
      "Xin Liu",
      "Tianyi Liu"
    ],
    "abstract": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-17T05:38:50+00:00",
    "updated": "2025-11-17T05:38:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12997v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12991v1",
    "title": "Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty",
    "authors": [
      "Zeyu Shi",
      "Ziming Wang",
      "Tianyu Chen",
      "Shiqi Gao",
      "Haoyi Zhou",
      "Qingyun Sun",
      "Jianxin Li"
    ],
    "abstract": "The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T05:30:48+00:00",
    "updated": "2025-11-17T05:30:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12991v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12928v1",
    "title": "Visual Room 2.0: Seeing is Not Understanding for MLLMs",
    "authors": [
      "Haokun Li",
      "Yazhou Zhang",
      "Jizhi Ding",
      "Qiuchi Li",
      "Peng Zhang"
    ],
    "abstract": "Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \\textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\\%$\\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T03:34:52+00:00",
    "updated": "2025-11-17T03:34:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12928v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12920v1",
    "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
    "authors": [
      "Desheng Hu",
      "Joachim Baumann",
      "Aleksandra Urman",
      "Elsa Lichtenegger",
      "Robin Forsberg",
      "Aniko Hannak",
      "Christo Wilson"
    ],
    "abstract": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T03:16:36+00:00",
    "updated": "2025-11-17T03:16:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12920v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12874v1",
    "title": "Classification of Hope in Textual Data using Transformer-Based Models",
    "authors": [
      "Chukwuebuka Fortunate Ijezue",
      "Tania-Amanda Fredrick Eneye",
      "Maaz Amjad"
    ],
    "abstract": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T02:07:24+00:00",
    "updated": "2025-11-17T02:07:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12874v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12861v2",
    "title": "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models",
    "authors": [
      "Wenxin Zhu",
      "Andong Chen",
      "Yuchen Song",
      "Kehai Chen",
      "Conghui Zhu",
      "Ziyan Chen",
      "Tiejun Zhao"
    ],
    "abstract": "With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on \"Multimodal Chain-of-Thought\" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T01:22:37+00:00",
    "updated": "2025-11-18T05:45:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12861v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12851v1",
    "title": "NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation",
    "authors": [
      "Kang Yin",
      "Hye-Bin Shin"
    ],
    "abstract": "Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T00:44:35+00:00",
    "updated": "2025-11-17T00:44:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12851v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12850v1",
    "title": "Quantifying consistency and accuracy of Latent Dirichlet Allocation",
    "authors": [
      "Saranzaya Magsarjav",
      "Melissa Humphries",
      "Jonathan Tuke",
      "Lewis Mitchell"
    ],
    "abstract": "Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-17T00:44:27+00:00",
    "updated": "2025-11-17T00:44:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12850v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12832v1",
    "title": "From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation",
    "authors": [
      "Niranjan Chebrolu",
      "Gerard Christopher Yeo",
      "Kokil Jaidka"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T23:33:06+00:00",
    "updated": "2025-11-16T23:33:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12832v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12821v1",
    "title": "BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals",
    "authors": [
      "Ruiyu Wang",
      "Yuzhang Xie",
      "Xiao Hu",
      "Carl Yang",
      "Jiaying Lu"
    ],
    "abstract": "Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T23:03:15+00:00",
    "updated": "2025-11-16T23:03:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12821v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12784v1",
    "title": "Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing",
    "authors": [
      "Hayden Moore",
      "Asfahan Shah"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.",
    "categories": [
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T21:25:59+00:00",
    "updated": "2025-11-16T21:25:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12784v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12782v1",
    "title": "LLM Reinforcement in Context",
    "authors": [
      "Thomas Rivasseau"
    ],
    "abstract": "Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T21:24:42+00:00",
    "updated": "2025-11-16T21:24:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12782v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12768v1",
    "title": "Evidence of Phase Transitions in Small Transformer-Based Language Models",
    "authors": [
      "Noah Hong",
      "Tao Hong"
    ],
    "abstract": "Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T20:37:12+00:00",
    "updated": "2025-11-16T20:37:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12768v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14796v1",
    "title": "Opinion Mining and Analysis Using Hybrid Deep Neural Networks",
    "authors": [
      "Adel Hidri",
      "Suleiman Ali Alsaif",
      "Muteeb Alahmari",
      "Eman AlShehri",
      "Minyar Sassi Hidri"
    ],
    "abstract": "Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T19:04:44+00:00",
    "updated": "2025-11-16T19:04:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14796v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12728v1",
    "title": "On the Brittleness of LLMs: A Journey around Set Membership",
    "authors": [
      "Lea Hergert",
      "Gábor Berend",
      "Mario Szegedy",
      "Gyorgy Turan",
      "Márk Jelasity"
    ],
    "abstract": "Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \\{pear, plum, apple, raspberry\\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T18:52:18+00:00",
    "updated": "2025-11-16T18:52:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12728v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12712v1",
    "title": "Adaptive Focus Memory for Language Models",
    "authors": [
      "Christopher Cruz"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T17:52:32+00:00",
    "updated": "2025-11-16T17:52:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12712v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12710v1",
    "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs",
    "authors": [
      "Yunhao Chen",
      "Xin Wang",
      "Juncheng Li",
      "Yixu Wang",
      "Jie Li",
      "Yan Teng",
      "Yingchun Wang",
      "Xingjun Ma"
    ],
    "abstract": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \\textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T17:52:07+00:00",
    "updated": "2025-11-16T17:52:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12710v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12690v1",
    "title": "Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data",
    "authors": [
      "Sina Rashidi",
      "Hossein Sameti"
    ],
    "abstract": "Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T17:14:23+00:00",
    "updated": "2025-11-16T17:14:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12690v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12661v1",
    "title": "Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing",
    "authors": [
      "Yuchen Wu",
      "Liang Ding",
      "Li Shen",
      "Dacheng Tao"
    ],
    "abstract": "Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a \"faithfulness gap\": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning \"Houston\" from \"NASA\" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T15:49:01+00:00",
    "updated": "2025-11-16T15:49:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12661v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13788v1",
    "title": "Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments",
    "authors": [
      "Samuel Nathanson",
      "Rebecca Williams",
      "Cynthia Matuszek"
    ],
    "abstract": "Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-16T15:16:33+00:00",
    "updated": "2025-11-16T15:16:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13788v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12630v1",
    "title": "Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing",
    "authors": [
      "Maoqi Liu",
      "Quan Fang",
      "Yang Yang",
      "Can Zhao",
      "Kaiquan Cai"
    ],
    "abstract": "Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T14:52:24+00:00",
    "updated": "2025-11-16T14:52:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12630v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12609v1",
    "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
    "authors": [
      "Yunxin Li",
      "Xinyu Chen",
      "Shenyuan Jiang",
      "Haoyuan Shi",
      "Zhenyu Liu",
      "Xuanyu Zhang",
      "Nanhao Deng",
      "Zhenran Xu",
      "Yicheng Ma",
      "Meishan Zhang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "abstract": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T14:10:55+00:00",
    "updated": "2025-11-16T14:10:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12609v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12596v1",
    "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models",
    "authors": [
      "Oron Anschel",
      "Alon Shoshan",
      "Adam Botach",
      "Shunit Haviv Hakimi",
      "Asaf Gendler",
      "Emanuel Ben Baruch",
      "Nadav Bhonker",
      "Igor Kviatkovsky",
      "Manoj Aggarwal",
      "Gerard Medioni"
    ],
    "abstract": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T13:42:55+00:00",
    "updated": "2025-11-16T13:42:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12596v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12586v1",
    "title": "MMWOZ: Building Multimodal Agent for Task-oriented Dialogue",
    "authors": [
      "Pu-Hai Yang",
      "Heyan Huang",
      "Heng-Da Xu",
      "Fanshu Sun",
      "Xian-Ling Mao",
      "Chaoxu Mu"
    ],
    "abstract": "Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T13:08:03+00:00",
    "updated": "2025-11-16T13:08:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12586v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12573v1",
    "title": "Mitigating Length Bias in RLHF through a Causal Lens",
    "authors": [
      "Hyeonji Kim",
      "Sujeong Oh",
      "Sanghack Lee"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T12:25:10+00:00",
    "updated": "2025-11-16T12:25:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12573v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12565v1",
    "title": "A Content-Preserving Secure Linguistic Steganography",
    "authors": [
      "Lingyun Xiang",
      "Chengfu Ou",
      "Xu He",
      "Zhongliang Yang",
      "Yuling Liu"
    ],
    "abstract": "Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\\textit{C}ontent-preserving \\textit{L}inguistic \\textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-16T11:50:13+00:00",
    "updated": "2025-11-16T11:50:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12565v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12529v1",
    "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing",
    "authors": [
      "Sanchaita Hazra",
      "Doeun Lee",
      "Bodhisattwa Prasad Majumder",
      "Sachin Kumar"
    ],
    "abstract": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "published": "2025-11-16T09:49:01+00:00",
    "updated": "2025-11-16T09:49:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12529v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12520v1",
    "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction",
    "authors": [
      "Jie Zhang",
      "Bo Tang",
      "Wanzi Shao",
      "Wenqiang Wei",
      "Jihao Zhao",
      "Jianqing Zhu",
      "Zhiyu li",
      "Wen Xi",
      "Zehao Lin",
      "Feiyu Xiong",
      "Yanchao Tan"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T09:23:09+00:00",
    "updated": "2025-11-16T09:23:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12520v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12504v1",
    "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs",
    "authors": [
      "Maria Tseytlin",
      "Paul Roit",
      "Omri Abend",
      "Ido Dagan",
      "Ayal Klein"
    ],
    "abstract": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T08:32:38+00:00",
    "updated": "2025-11-16T08:32:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12504v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12497v1",
    "title": "SGuard-v1: Safety Guardrail for Large Language Models",
    "authors": [
      "JoonHo Lee",
      "HyeonMin Cho",
      "Jaewoong Yun",
      "Hyunjae Lee",
      "JunKyu Lee",
      "Juree Seok"
    ],
    "abstract": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T08:15:54+00:00",
    "updated": "2025-11-16T08:15:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12497v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12487v1",
    "title": "Evolving Prompts for Toxicity Search in Large Language Models",
    "authors": [
      "Onkar Shelar",
      "Travis Desell"
    ],
    "abstract": "Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "published": "2025-11-16T07:47:31+00:00",
    "updated": "2025-11-16T07:47:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12487v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12474v1",
    "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout",
    "authors": [
      "Chucheng Xiang",
      "Ruchao Bao",
      "Biyin Feng",
      "Wenzheng Wu",
      "Zhongyuan Liu",
      "Yirui Guan",
      "Ligang Liu"
    ],
    "abstract": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-16T06:20:55+00:00",
    "updated": "2025-11-16T06:20:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12474v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12472v1",
    "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
    "authors": [
      "Mengying Wang",
      "Chenhui Ma",
      "Ao Jiao",
      "Tuo Liang",
      "Pengjun Lu",
      "Shrinidhi Hegde",
      "Yu Yin",
      "Evren Gurkan-Cavusoglu",
      "Yinghui Wu"
    ],
    "abstract": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T06:19:53+00:00",
    "updated": "2025-11-16T06:19:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12472v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12464v1",
    "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models",
    "authors": [
      "Chenglong Wang",
      "Yifu Huo",
      "Yang Gan",
      "Yongyu Mu",
      "Qiaozhi He",
      "Murun Yang",
      "Bei Li",
      "Chunliang Zhang",
      "Tongran Liu",
      "Anxiang Ma",
      "Zhengtao Yu",
      "Jingbo Zhu",
      "Tong Xiao"
    ],
    "abstract": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T05:29:29+00:00",
    "updated": "2025-11-16T05:29:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12464v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12452v1",
    "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
    "authors": [
      "Xiaoyu Lin",
      "Aniket Ghorpade",
      "Hansheng Zhu",
      "Justin Qiu",
      "Dea Rrozhani",
      "Monica Lama",
      "Mick Yang",
      "Zixuan Bian",
      "Ruohan Ren",
      "Alan B. Hong",
      "Jiatao Gu",
      "Chris Callison-Burch"
    ],
    "abstract": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-16T04:46:06+00:00",
    "updated": "2025-11-16T04:46:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12452v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12387v1",
    "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil",
    "authors": [
      "Jeyarajalingam Varsha",
      "Menan Velayuthan",
      "Sumirtha Karunakaran",
      "Rasan Nivethiga",
      "Kengatharaiyer Sarveswaran"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T23:41:16+00:00",
    "updated": "2025-11-15T23:41:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12387v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12381v1",
    "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load",
    "authors": [
      "Logan Mann",
      "Nayan Saxena",
      "Sarah Tandon",
      "Chenhao Sun",
      "Savar Toteja",
      "Kevin Zhu"
    ],
    "abstract": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T23:00:56+00:00",
    "updated": "2025-11-15T23:00:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12381v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12347v1",
    "title": "VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing",
    "authors": [
      "Zhisheng Zheng",
      "Puyuan Peng",
      "Anuj Diwan",
      "Cong Phuoc Huynh",
      "Xiaohang Sun",
      "Zhu Liu",
      "Vimal Bhat",
      "David Harwath"
    ],
    "abstract": "We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-15T20:27:25+00:00",
    "updated": "2025-11-15T20:27:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12347v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12300v1",
    "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering",
    "authors": [
      "Naoya Sugiura",
      "Kosuke Yamada",
      "Yasuhiro Ogawa",
      "Katsuhiko Toyama",
      "Ryohei Sasano"
    ],
    "abstract": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T17:23:00+00:00",
    "updated": "2025-11-15T17:23:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12300v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12290v1",
    "title": "AugAbEx : Way Forward for Extractive Case Summarization",
    "authors": [
      "Purnima Bindal",
      "Vikas Kumar",
      "Sagar Rathore",
      "Vasudha Bhatnagar"
    ],
    "abstract": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.   Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T16:49:42+00:00",
    "updated": "2025-11-15T16:49:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12290v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12285v1",
    "title": "How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer",
    "authors": [
      "Minu Kim",
      "Ji Sub Um",
      "Hoirin Kim"
    ],
    "abstract": "Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.",
    "categories": [
      "eess.AS",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-15T16:38:09+00:00",
    "updated": "2025-11-15T16:38:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12285v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12281v1",
    "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor",
    "authors": [
      "Ivan Zakazov",
      "Alexander Sharipov",
      "Berke Argin",
      "Oussama Gabouj",
      "Kamel Charaf",
      "Alexi Semiz",
      "Lorenzo Drudi",
      "Nicolas Baldwin",
      "Robert West"
    ],
    "abstract": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T16:28:03+00:00",
    "updated": "2025-11-15T16:28:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12281v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12280v1",
    "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs",
    "authors": [
      "Shuochen Chang",
      "Xiaofeng Zhang",
      "Qingyang Liu",
      "Li Niu"
    ],
    "abstract": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-15T16:24:12+00:00",
    "updated": "2025-11-15T16:24:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12280v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12249v1",
    "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations",
    "authors": [
      "Khang T. Huynh",
      "Dung H. Nguyen",
      "Binh T. Nguyen"
    ],
    "abstract": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T15:11:52+00:00",
    "updated": "2025-11-15T15:11:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12249v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12236v1",
    "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
    "authors": [
      "Raavi Gupta",
      "Pranav Hari Panicker",
      "Sumit Bhatia",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T14:33:02+00:00",
    "updated": "2025-11-15T14:33:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12236v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12213v1",
    "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues",
    "authors": [
      "Liang Xue",
      "Haoyu Liu",
      "Yajun Tian",
      "Xinyu Zhong",
      "Yang Liu"
    ],
    "abstract": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T13:35:55+00:00",
    "updated": "2025-11-15T13:35:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12213v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12159v1",
    "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic",
    "authors": [
      "Yaocheng Zhang",
      "Haohuan Huang",
      "Zijun Song",
      "Yuanheng Zhu",
      "Qichao Zhang",
      "Zijie Zhao",
      "Dongbin Zhao"
    ],
    "abstract": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T11:06:57+00:00",
    "updated": "2025-11-15T11:06:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12140v1",
    "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding",
    "authors": [
      "Pinxue Guo",
      "Chongruo Wu",
      "Xinyu Zhou",
      "Lingyi Hong",
      "Zhaoyu Chen",
      "Jinglun Li",
      "Kaixun Jiang",
      "Sen-ching Samson Cheung",
      "Wei Zhang",
      "Wenqiang Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T10:11:13+00:00",
    "updated": "2025-11-15T10:11:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12140v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12133v1",
    "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
    "authors": [
      "Qingyu Zhang",
      "Chunlei Xin",
      "Xuanang Chen",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Qing Ye",
      "Qianlong Xie",
      "Xingxing Wang"
    ],
    "abstract": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:44:42+00:00",
    "updated": "2025-11-15T09:44:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12133v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12130v1",
    "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection",
    "authors": [
      "Bingbing Wang",
      "Zhixin Bai",
      "Zhengda Jin",
      "Zihan Wang",
      "Xintong Song",
      "Jingjie Lin",
      "Sixuan Li",
      "Jing Li",
      "Ruifeng Xu"
    ],
    "abstract": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:35:58+00:00",
    "updated": "2025-11-15T09:35:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12130v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12116v1",
    "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models",
    "authors": [
      "Piotr Pęzik",
      "Konrad Kaczyński",
      "Maria Szymańska",
      "Filip Żarnecki",
      "Zuzanna Deckert",
      "Jakub Kwiatkowski",
      "Wojciech Janowski"
    ],
    "abstract": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:08:10+00:00",
    "updated": "2025-11-15T09:08:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12116v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12109v1",
    "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task",
    "authors": [
      "Felipe Fujita",
      "Hideyuki Takada"
    ],
    "abstract": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T08:59:18+00:00",
    "updated": "2025-11-15T08:59:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12109v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12036v1",
    "title": "Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys",
    "authors": [
      "Satanu Ghosh",
      "Collin Holgate",
      "Neal R. Brodnik",
      "Doug Downey",
      "Samantha Daly",
      "Tresa M. Pollock",
      "Samuel Carton"
    ],
    "abstract": "We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.",
    "categories": [
      "cs.CE",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CE",
    "published": "2025-11-15T05:08:22+00:00",
    "updated": "2025-11-15T05:08:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12036v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12014v1",
    "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs",
    "authors": [
      "Truong Vo",
      "Sanmi Koyejo"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T03:39:13+00:00",
    "updated": "2025-11-15T03:39:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12014v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12010v1",
    "title": "Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles",
    "authors": [
      "Palakorn Achananuparp",
      "Connie Xu",
      "Yao Lu",
      "Xavier Jayaraj Siddarth Ashok",
      "Ee-Peng Lim"
    ],
    "abstract": "We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "published": "2025-11-15T03:26:57+00:00",
    "updated": "2025-11-15T03:26:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12010v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12001v2",
    "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations",
    "authors": [
      "Eunkyu Park",
      "Wesley Hanwen Deng",
      "Vasudha Varadarajan",
      "Mingxi Yan",
      "Gunhee Kim",
      "Maarten Sap",
      "Motahhare Eslami"
    ],
    "abstract": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T02:38:49+00:00",
    "updated": "2025-11-19T05:49:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12001v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11978v1",
    "title": "A Reasoning Paradigm for Named Entity Recognition",
    "authors": [
      "Hui Huang",
      "Yanping Chen",
      "Ruizhang Huang",
      "Chuan Lin",
      "Yongbin Qin"
    ],
    "abstract": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T01:31:43+00:00",
    "updated": "2025-11-15T01:31:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11978v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11966v1",
    "title": "On the Entropy Calibration of Language Models",
    "authors": [
      "Steven Cao",
      "Gregory Valiant",
      "Percy Liang"
    ],
    "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T00:33:03+00:00",
    "updated": "2025-11-15T00:33:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11966v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11946v1",
    "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization",
    "authors": [
      "Hadi Sheikhi",
      "Chenyang Huang",
      "Osmar R. Zaïane"
    ],
    "abstract": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:37:35+00:00",
    "updated": "2025-11-14T23:37:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11946v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11933v1",
    "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "Bhuwan Dhingra",
      "David Edwin Carlson"
    ],
    "abstract": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:15:41+00:00",
    "updated": "2025-11-14T23:15:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11933v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11922v1",
    "title": "Additive Large Language Models for Semi-Structured Text",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "David Carlson"
    ],
    "abstract": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:06:16+00:00",
    "updated": "2025-11-14T23:06:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11922v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11914v1",
    "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization",
    "authors": [
      "Shizhou Xu",
      "Yuan Ni",
      "Stefan Broecker",
      "Thomas Strohmer"
    ],
    "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T22:48:39+00:00",
    "updated": "2025-11-14T22:48:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11914v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11884v1",
    "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support",
    "authors": [
      "Eric Hua Qing Zhang",
      "Julia Ive"
    ],
    "abstract": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:32:10+00:00",
    "updated": "2025-11-14T21:32:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11884v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11883v1",
    "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "David Carlson"
    ],
    "abstract": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:21:16+00:00",
    "updated": "2025-11-14T21:21:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11883v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11881v2",
    "title": "Better LLM Reasoning via Dual-Play",
    "authors": [
      "Zhengxin Zhang",
      "Chengyu Huang",
      "Aochong Oliver Li",
      "Claire Cardie"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T21:19:07+00:00",
    "updated": "2025-11-19T01:20:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11881v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11878v1",
    "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers",
    "authors": [
      "Fernanda Bufon Färber",
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "abstract": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:13:28+00:00",
    "updated": "2025-11-14T21:13:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11878v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12596v1",
    "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models",
    "authors": [
      "Oron Anschel",
      "Alon Shoshan",
      "Adam Botach",
      "Shunit Haviv Hakimi",
      "Asaf Gendler",
      "Emanuel Ben Baruch",
      "Nadav Bhonker",
      "Igor Kviatkovsky",
      "Manoj Aggarwal",
      "Gerard Medioni"
    ],
    "abstract": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T13:42:55+00:00",
    "updated": "2025-11-16T13:42:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12596v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12586v1",
    "title": "MMWOZ: Building Multimodal Agent for Task-oriented Dialogue",
    "authors": [
      "Pu-Hai Yang",
      "Heyan Huang",
      "Heng-Da Xu",
      "Fanshu Sun",
      "Xian-Ling Mao",
      "Chaoxu Mu"
    ],
    "abstract": "Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T13:08:03+00:00",
    "updated": "2025-11-16T13:08:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12586v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12573v1",
    "title": "Mitigating Length Bias in RLHF through a Causal Lens",
    "authors": [
      "Hyeonji Kim",
      "Sujeong Oh",
      "Sanghack Lee"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T12:25:10+00:00",
    "updated": "2025-11-16T12:25:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12573v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12565v1",
    "title": "A Content-Preserving Secure Linguistic Steganography",
    "authors": [
      "Lingyun Xiang",
      "Chengfu Ou",
      "Xu He",
      "Zhongliang Yang",
      "Yuling Liu"
    ],
    "abstract": "Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\\textit{C}ontent-preserving \\textit{L}inguistic \\textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-16T11:50:13+00:00",
    "updated": "2025-11-16T11:50:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12565v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12529v1",
    "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing",
    "authors": [
      "Sanchaita Hazra",
      "Doeun Lee",
      "Bodhisattwa Prasad Majumder",
      "Sachin Kumar"
    ],
    "abstract": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "published": "2025-11-16T09:49:01+00:00",
    "updated": "2025-11-16T09:49:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12529v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12520v1",
    "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction",
    "authors": [
      "Jie Zhang",
      "Bo Tang",
      "Wanzi Shao",
      "Wenqiang Wei",
      "Jihao Zhao",
      "Jianqing Zhu",
      "Zhiyu li",
      "Wen Xi",
      "Zehao Lin",
      "Feiyu Xiong",
      "Yanchao Tan"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T09:23:09+00:00",
    "updated": "2025-11-16T09:23:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12520v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12504v1",
    "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs",
    "authors": [
      "Maria Tseytlin",
      "Paul Roit",
      "Omri Abend",
      "Ido Dagan",
      "Ayal Klein"
    ],
    "abstract": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T08:32:38+00:00",
    "updated": "2025-11-16T08:32:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12504v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12497v1",
    "title": "SGuard-v1: Safety Guardrail for Large Language Models",
    "authors": [
      "JoonHo Lee",
      "HyeonMin Cho",
      "Jaewoong Yun",
      "Hyunjae Lee",
      "JunKyu Lee",
      "Juree Seok"
    ],
    "abstract": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T08:15:54+00:00",
    "updated": "2025-11-16T08:15:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12497v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12487v1",
    "title": "Evolving Prompts for Toxicity Search in Large Language Models",
    "authors": [
      "Onkar Shelar",
      "Travis Desell"
    ],
    "abstract": "Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "published": "2025-11-16T07:47:31+00:00",
    "updated": "2025-11-16T07:47:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12487v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12474v1",
    "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout",
    "authors": [
      "Chucheng Xiang",
      "Ruchao Bao",
      "Biyin Feng",
      "Wenzheng Wu",
      "Zhongyuan Liu",
      "Yirui Guan",
      "Ligang Liu"
    ],
    "abstract": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-16T06:20:55+00:00",
    "updated": "2025-11-16T06:20:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12474v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12472v1",
    "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
    "authors": [
      "Mengying Wang",
      "Chenhui Ma",
      "Ao Jiao",
      "Tuo Liang",
      "Pengjun Lu",
      "Shrinidhi Hegde",
      "Yu Yin",
      "Evren Gurkan-Cavusoglu",
      "Yinghui Wu"
    ],
    "abstract": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T06:19:53+00:00",
    "updated": "2025-11-16T06:19:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12472v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12464v1",
    "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models",
    "authors": [
      "Chenglong Wang",
      "Yifu Huo",
      "Yang Gan",
      "Yongyu Mu",
      "Qiaozhi He",
      "Murun Yang",
      "Bei Li",
      "Chunliang Zhang",
      "Tongran Liu",
      "Anxiang Ma",
      "Zhengtao Yu",
      "Jingbo Zhu",
      "Tong Xiao"
    ],
    "abstract": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-16T05:29:29+00:00",
    "updated": "2025-11-16T05:29:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12464v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12452v1",
    "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
    "authors": [
      "Xiaoyu Lin",
      "Aniket Ghorpade",
      "Hansheng Zhu",
      "Justin Qiu",
      "Dea Rrozhani",
      "Monica Lama",
      "Mick Yang",
      "Zixuan Bian",
      "Ruohan Ren",
      "Alan B. Hong",
      "Jiatao Gu",
      "Chris Callison-Burch"
    ],
    "abstract": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-16T04:46:06+00:00",
    "updated": "2025-11-16T04:46:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12452v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12387v1",
    "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil",
    "authors": [
      "Jeyarajalingam Varsha",
      "Menan Velayuthan",
      "Sumirtha Karunakaran",
      "Rasan Nivethiga",
      "Kengatharaiyer Sarveswaran"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T23:41:16+00:00",
    "updated": "2025-11-15T23:41:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12387v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12381v1",
    "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load",
    "authors": [
      "Logan Mann",
      "Nayan Saxena",
      "Sarah Tandon",
      "Chenhao Sun",
      "Savar Toteja",
      "Kevin Zhu"
    ],
    "abstract": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T23:00:56+00:00",
    "updated": "2025-11-15T23:00:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12381v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12347v1",
    "title": "VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing",
    "authors": [
      "Zhisheng Zheng",
      "Puyuan Peng",
      "Anuj Diwan",
      "Cong Phuoc Huynh",
      "Xiaohang Sun",
      "Zhu Liu",
      "Vimal Bhat",
      "David Harwath"
    ],
    "abstract": "We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-15T20:27:25+00:00",
    "updated": "2025-11-15T20:27:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12347v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12300v1",
    "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering",
    "authors": [
      "Naoya Sugiura",
      "Kosuke Yamada",
      "Yasuhiro Ogawa",
      "Katsuhiko Toyama",
      "Ryohei Sasano"
    ],
    "abstract": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T17:23:00+00:00",
    "updated": "2025-11-15T17:23:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12300v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12290v1",
    "title": "AugAbEx : Way Forward for Extractive Case Summarization",
    "authors": [
      "Purnima Bindal",
      "Vikas Kumar",
      "Sagar Rathore",
      "Vasudha Bhatnagar"
    ],
    "abstract": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.   Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T16:49:42+00:00",
    "updated": "2025-11-15T16:49:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12290v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12285v1",
    "title": "How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer",
    "authors": [
      "Minu Kim",
      "Ji Sub Um",
      "Hoirin Kim"
    ],
    "abstract": "Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.",
    "categories": [
      "eess.AS",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-15T16:38:09+00:00",
    "updated": "2025-11-15T16:38:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12285v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12281v1",
    "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor",
    "authors": [
      "Ivan Zakazov",
      "Alexander Sharipov",
      "Berke Argin",
      "Oussama Gabouj",
      "Kamel Charaf",
      "Alexi Semiz",
      "Lorenzo Drudi",
      "Nicolas Baldwin",
      "Robert West"
    ],
    "abstract": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T16:28:03+00:00",
    "updated": "2025-11-15T16:28:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12281v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12280v1",
    "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs",
    "authors": [
      "Shuochen Chang",
      "Xiaofeng Zhang",
      "Qingyang Liu",
      "Li Niu"
    ],
    "abstract": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-15T16:24:12+00:00",
    "updated": "2025-11-15T16:24:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12280v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12249v1",
    "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations",
    "authors": [
      "Khang T. Huynh",
      "Dung H. Nguyen",
      "Binh T. Nguyen"
    ],
    "abstract": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T15:11:52+00:00",
    "updated": "2025-11-15T15:11:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12249v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12236v1",
    "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
    "authors": [
      "Raavi Gupta",
      "Pranav Hari Panicker",
      "Sumit Bhatia",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T14:33:02+00:00",
    "updated": "2025-11-15T14:33:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12236v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12213v1",
    "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues",
    "authors": [
      "Liang Xue",
      "Haoyu Liu",
      "Yajun Tian",
      "Xinyu Zhong",
      "Yang Liu"
    ],
    "abstract": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T13:35:55+00:00",
    "updated": "2025-11-15T13:35:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12213v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12159v1",
    "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic",
    "authors": [
      "Yaocheng Zhang",
      "Haohuan Huang",
      "Zijun Song",
      "Yuanheng Zhu",
      "Qichao Zhang",
      "Zijie Zhao",
      "Dongbin Zhao"
    ],
    "abstract": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T11:06:57+00:00",
    "updated": "2025-11-15T11:06:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12140v1",
    "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding",
    "authors": [
      "Pinxue Guo",
      "Chongruo Wu",
      "Xinyu Zhou",
      "Lingyi Hong",
      "Zhaoyu Chen",
      "Jinglun Li",
      "Kaixun Jiang",
      "Sen-ching Samson Cheung",
      "Wei Zhang",
      "Wenqiang Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T10:11:13+00:00",
    "updated": "2025-11-15T10:11:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12140v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12133v1",
    "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
    "authors": [
      "Qingyu Zhang",
      "Chunlei Xin",
      "Xuanang Chen",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Qing Ye",
      "Qianlong Xie",
      "Xingxing Wang"
    ],
    "abstract": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:44:42+00:00",
    "updated": "2025-11-15T09:44:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12133v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12130v1",
    "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection",
    "authors": [
      "Bingbing Wang",
      "Zhixin Bai",
      "Zhengda Jin",
      "Zihan Wang",
      "Xintong Song",
      "Jingjie Lin",
      "Sixuan Li",
      "Jing Li",
      "Ruifeng Xu"
    ],
    "abstract": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:35:58+00:00",
    "updated": "2025-11-15T09:35:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12130v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12116v1",
    "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models",
    "authors": [
      "Piotr Pęzik",
      "Konrad Kaczyński",
      "Maria Szymańska",
      "Filip Żarnecki",
      "Zuzanna Deckert",
      "Jakub Kwiatkowski",
      "Wojciech Janowski"
    ],
    "abstract": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T09:08:10+00:00",
    "updated": "2025-11-15T09:08:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12116v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12109v1",
    "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task",
    "authors": [
      "Felipe Fujita",
      "Hideyuki Takada"
    ],
    "abstract": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T08:59:18+00:00",
    "updated": "2025-11-15T08:59:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12109v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12036v1",
    "title": "Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys",
    "authors": [
      "Satanu Ghosh",
      "Collin Holgate",
      "Neal R. Brodnik",
      "Doug Downey",
      "Samantha Daly",
      "Tresa M. Pollock",
      "Samuel Carton"
    ],
    "abstract": "We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.",
    "categories": [
      "cs.CE",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CE",
    "published": "2025-11-15T05:08:22+00:00",
    "updated": "2025-11-15T05:08:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12036v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12014v1",
    "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs",
    "authors": [
      "Truong Vo",
      "Sanmi Koyejo"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T03:39:13+00:00",
    "updated": "2025-11-15T03:39:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12014v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12010v1",
    "title": "Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles",
    "authors": [
      "Palakorn Achananuparp",
      "Connie Xu",
      "Yao Lu",
      "Xavier Jayaraj Siddarth Ashok",
      "Ee-Peng Lim"
    ],
    "abstract": "We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "published": "2025-11-15T03:26:57+00:00",
    "updated": "2025-11-15T03:26:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12010v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12001v2",
    "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations",
    "authors": [
      "Eunkyu Park",
      "Wesley Hanwen Deng",
      "Vasudha Varadarajan",
      "Mingxi Yan",
      "Gunhee Kim",
      "Maarten Sap",
      "Motahhare Eslami"
    ],
    "abstract": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T02:38:49+00:00",
    "updated": "2025-11-19T05:49:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12001v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11978v1",
    "title": "A Reasoning Paradigm for Named Entity Recognition",
    "authors": [
      "Hui Huang",
      "Yanping Chen",
      "Ruizhang Huang",
      "Chuan Lin",
      "Yongbin Qin"
    ],
    "abstract": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T01:31:43+00:00",
    "updated": "2025-11-15T01:31:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11978v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11966v1",
    "title": "On the Entropy Calibration of Language Models",
    "authors": [
      "Steven Cao",
      "Gregory Valiant",
      "Percy Liang"
    ],
    "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T00:33:03+00:00",
    "updated": "2025-11-15T00:33:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11966v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11946v1",
    "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization",
    "authors": [
      "Hadi Sheikhi",
      "Chenyang Huang",
      "Osmar R. Zaïane"
    ],
    "abstract": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:37:35+00:00",
    "updated": "2025-11-14T23:37:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11946v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11933v1",
    "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "Bhuwan Dhingra",
      "David Edwin Carlson"
    ],
    "abstract": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:15:41+00:00",
    "updated": "2025-11-14T23:15:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11933v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11922v1",
    "title": "Additive Large Language Models for Semi-Structured Text",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "David Carlson"
    ],
    "abstract": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T23:06:16+00:00",
    "updated": "2025-11-14T23:06:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11922v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11914v1",
    "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization",
    "authors": [
      "Shizhou Xu",
      "Yuan Ni",
      "Stefan Broecker",
      "Thomas Strohmer"
    ],
    "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T22:48:39+00:00",
    "updated": "2025-11-14T22:48:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11914v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11884v1",
    "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support",
    "authors": [
      "Eric Hua Qing Zhang",
      "Julia Ive"
    ],
    "abstract": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:32:10+00:00",
    "updated": "2025-11-14T21:32:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11884v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11883v1",
    "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts",
    "authors": [
      "Karthikeyan K",
      "Raghuveer Thirukovalluru",
      "David Carlson"
    ],
    "abstract": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:21:16+00:00",
    "updated": "2025-11-14T21:21:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11883v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11881v2",
    "title": "Better LLM Reasoning via Dual-Play",
    "authors": [
      "Zhengxin Zhang",
      "Chengyu Huang",
      "Aochong Oliver Li",
      "Claire Cardie"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T21:19:07+00:00",
    "updated": "2025-11-19T01:20:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11881v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11878v1",
    "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers",
    "authors": [
      "Fernanda Bufon Färber",
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "abstract": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T21:13:28+00:00",
    "updated": "2025-11-14T21:13:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11878v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11867v1",
    "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches",
    "authors": [
      "Namu Park",
      "Giridhar Kaushik Ramachandran",
      "Kevin Lybarger",
      "Fei Xia",
      "Ozlem Uzuner",
      "Meliha Yetisgen",
      "Martin Gunn"
    ],
    "abstract": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T20:55:44+00:00",
    "updated": "2025-11-14T20:55:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11867v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11857v1",
    "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection",
    "authors": [
      "Taimur Khan",
      "Ramoza Ahsan",
      "Mohib Hameed"
    ],
    "abstract": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T20:30:18+00:00",
    "updated": "2025-11-14T20:30:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11857v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11829v1",
    "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification",
    "authors": [
      "Mihir Gupte",
      "Ramesh S"
    ],
    "abstract": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.FL",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T19:45:17+00:00",
    "updated": "2025-11-14T19:45:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11829v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11821v1",
    "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis",
    "authors": [
      "Hong-Jun Yoon",
      "Faisal Ashraf",
      "Thomas A. Ruggles",
      "Debjani Singh"
    ],
    "abstract": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.   Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.   We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.   These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T19:23:25+00:00",
    "updated": "2025-11-14T19:23:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11821v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11816v1",
    "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy",
    "authors": [
      "Andrea Brunello",
      "Luca Geatti",
      "Michele Mignani",
      "Angelo Montanari",
      "Nicola Saccomanno"
    ],
    "abstract": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T19:11:41+00:00",
    "updated": "2025-11-14T19:11:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11816v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11810v1",
    "title": "On the Notion that Language Models Reason",
    "authors": [
      "Bertram Højer"
    ],
    "abstract": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T19:04:24+00:00",
    "updated": "2025-11-14T19:04:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11810v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11571v1",
    "title": "Optimizing Mixture of Block Attention",
    "authors": [
      "Guangxuan Xiao",
      "Junxian Guo",
      "Kasra Mazaheri",
      "Song Han"
    ],
    "abstract": "Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T18:59:59+00:00",
    "updated": "2025-11-14T18:59:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11571v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11562v1",
    "title": "PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning",
    "authors": [
      "Afra Feyza Akyürek",
      "Advait Gosai",
      "Chen Bo Calvin Zhang",
      "Vipul Gupta",
      "Jaehwan Jeong",
      "Anisha Gunjal",
      "Tahseen Rabbani",
      "Maria Mazzone",
      "David Randolph",
      "Mohammad Mahmoudi Meymand",
      "Gurshaan Chattha",
      "Paula Rodriguez",
      "Diego Mares",
      "Pavit Singh",
      "Michael Liu",
      "Subodh Chawla",
      "Pete Cline",
      "Lucy Ogaz",
      "Ernesto Hernandez",
      "Zihao Wang",
      "Pavi Bhatter",
      "Marcos Ayestaran",
      "Bing Liu",
      "Yunzhong He"
    ],
    "abstract": "Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T18:55:12+00:00",
    "updated": "2025-11-14T18:55:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11562v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11793v2",
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "authors": [
      "MiroMind Team",
      "Song Bai",
      "Lidong Bing",
      "Carson Chen",
      "Guanzheng Chen",
      "Yuntao Chen",
      "Zhe Chen",
      "Ziyi Chen",
      "Jifeng Dai",
      "Xuan Dong",
      "Wenhan Dou",
      "Yue Deng",
      "Yunjie Fu",
      "Junqi Ge",
      "Chenxia Han",
      "Tammy Huang",
      "Zhenhang Huang",
      "Jerry Jiao",
      "Shilei Jiang",
      "Tianyu Jiao",
      "Xiaoqi Jian",
      "Lei Lei",
      "Ruilin Li",
      "Ryan Luo",
      "Tiantong Li",
      "Xiang Lin",
      "Ziyuan Liu",
      "Zhiqi Li",
      "Jie Ni",
      "Qiang Ren",
      "Pax Sun",
      "Shiqian Su",
      "Chenxin Tao",
      "Bin Wang",
      "Hellen Wang",
      "Haonan Wang",
      "James Wang",
      "Jin Wang",
      "Jojo Wang",
      "Letian Wang",
      "Shizun Wang",
      "Weizhi Wang",
      "Zixuan Wang",
      "Jinfan Xu",
      "Sen Xing",
      "Chenyu Yang",
      "Hai Ye",
      "Jiaheng Yu",
      "Yue Yu",
      "Muyan Zhong",
      "Tianchen Zhao",
      "Xizhou Zhu",
      "Yanpeng Zhou",
      "Yifan Zhang",
      "Zhi Zhu"
    ],
    "abstract": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T18:52:07+00:00",
    "updated": "2025-11-18T15:45:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11793v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11551v2",
    "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
    "authors": [
      "Dena Mujtaba",
      "Brian Hu",
      "Anthony Hoogs",
      "Arslan Basharat"
    ],
    "abstract": "The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining alignment. For pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T18:42:18+00:00",
    "updated": "2025-11-17T04:49:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11551v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11552v1",
    "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
    "authors": [
      "Dawei Zhu",
      "Rui Meng",
      "Jiefeng Chen",
      "Sujian Li",
      "Tomas Pfister",
      "Jinsung Yoon"
    ],
    "abstract": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-14T18:42:18+00:00",
    "updated": "2025-11-14T18:42:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11552v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11518v1",
    "title": "W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search",
    "authors": [
      "Zhenyu Ding",
      "Yuhao Wang",
      "Tengyue Xiao",
      "Haoying Wang",
      "Guojun Ma",
      "Mingyang Wan",
      "Caigui Jiang",
      "Ning Ding"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T17:42:02+00:00",
    "updated": "2025-11-14T17:42:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11518v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11473v1",
    "title": "Proactive Hearing Assistants that Isolate Egocentric Conversations",
    "authors": [
      "Guilin Hu",
      "Malek Itani",
      "Tuochao Chen",
      "Shyamnath Gollakota"
    ],
    "abstract": "We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T16:44:48+00:00",
    "updated": "2025-11-14T16:44:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11473v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11440v1",
    "title": "From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs",
    "authors": [
      "Massimo Rizzoli",
      "Simone Alghisi",
      "Seyed Mahed Mousavi",
      "Giuseppe Riccardi"
    ],
    "abstract": "Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-14T16:07:18+00:00",
    "updated": "2025-11-14T16:07:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11440v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11412v3",
    "title": "MajinBook: An open catalogue of digital world literature with likes",
    "authors": [
      "Antoine Mazières",
      "Thierry Poibeau"
    ],
    "abstract": "This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "stat.OT"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T15:44:27+00:00",
    "updated": "2025-11-20T07:27:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11412v3",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11389v1",
    "title": "Studies with impossible languages falsify LMs as models of human language",
    "authors": [
      "Jeffrey S. Bowers",
      "Jeff Mitchell"
    ],
    "abstract": "According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T15:18:26+00:00",
    "updated": "2025-11-14T15:18:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11389v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11362v1",
    "title": "On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization",
    "authors": [
      "Prabodh Katti",
      "Sangwoo Park",
      "Bipin Rajendran",
      "Osvaldo Simeone"
    ],
    "abstract": "On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T14:46:29+00:00",
    "updated": "2025-11-14T14:46:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11362v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11340v1",
    "title": "M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text",
    "authors": [
      "Salima Lamsiyah",
      "Saad Ezzini",
      "Abdelkader El Mahdaouy",
      "Hamza Alami",
      "Abdessamad Benlahbib",
      "Samir El Amrany",
      "Salmane Chafik",
      "Hicham Hammouchi"
    ],
    "abstract": "The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T14:26:31+00:00",
    "updated": "2025-11-14T14:26:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11340v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11334v1",
    "title": "LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models",
    "authors": [
      "Jian Gao",
      "Richeng Xuan",
      "Zhaolu Kang",
      "Dingshi Liao",
      "Wenxin Huang",
      "Zongmou Huang",
      "Yangdi Xu",
      "Bowen Qin",
      "Zheqi He",
      "Xi Yang",
      "Changjin Li"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T14:13:07+00:00",
    "updated": "2025-11-14T14:13:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11334v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11324v1",
    "title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery",
    "authors": [
      "Anurag J. Vaidya",
      "Felix Meissen",
      "Daniel C. Castro",
      "Shruthi Bannur",
      "Tristan Lazard",
      "Drew F. K. Williamson",
      "Faisal Mahmood",
      "Javier Alvarez-Valle",
      "Stephanie L. Hyland",
      "Kenza Bouzid"
    ],
    "abstract": "Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T14:01:18+00:00",
    "updated": "2025-11-14T14:01:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11324v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11315v1",
    "title": "LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models",
    "authors": [
      "Jawad Ibn Ahad",
      "Muhammad Rafsan Kabir",
      "Robin Krambroeckers",
      "Sifat Momen",
      "Nabeel Mohammed",
      "Shafin Rahman"
    ],
    "abstract": "Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T13:57:46+00:00",
    "updated": "2025-11-14T13:57:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11315v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11306v1",
    "title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference",
    "authors": [
      "Wei Fan",
      "JinYi Yoon",
      "Bo Ji"
    ],
    "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T13:50:51+00:00",
    "updated": "2025-11-14T13:50:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11306v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11287v1",
    "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
    "authors": [
      "Sven Schultze",
      "Meike Verena Kietzmann",
      "Nils-Lucas Schönfeld",
      "Ruth Stock-Homburg"
    ],
    "abstract": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "published": "2025-11-14T13:23:34+00:00",
    "updated": "2025-11-14T13:23:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11287v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11285v1",
    "title": "Language-Aided State Estimation",
    "authors": [
      "Yuki Miyoshi",
      "Masaki Inoue",
      "Yusuke Fujimoto"
    ],
    "abstract": "Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.",
    "categories": [
      "eess.SY",
      "cs.CL"
    ],
    "primary_category": "eess.SY",
    "published": "2025-11-14T13:18:37+00:00",
    "updated": "2025-11-14T13:18:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11285v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11265v1",
    "title": "SQuaD: The Software Quality Dataset",
    "authors": [
      "Mikel Robredo",
      "Matteo Esposito",
      "Davide Taibi",
      "Rafael Peñaloza",
      "Valentina Lenarduzzi"
    ],
    "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "published": "2025-11-14T12:57:22+00:00",
    "updated": "2025-11-14T12:57:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11265v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11262v1",
    "title": "Discovering Meaningful Units with Visually Grounded Semantics from Image Captions",
    "authors": [
      "Melika Behjati",
      "James Henderson"
    ],
    "abstract": "Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-14T12:56:18+00:00",
    "updated": "2025-11-14T12:56:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11262v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11258v1",
    "title": "KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement",
    "authors": [
      "Sania Nayab",
      "Marco Simoni",
      "Giulio Rossolini",
      "Andrea Saracino"
    ],
    "abstract": "The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T12:54:01+00:00",
    "updated": "2025-11-14T12:54:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11258v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11234v1",
    "title": "LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation",
    "authors": [
      "Jader Martins Camboim de Sá",
      "Jooyoung Lee",
      "Cédric Pruski",
      "Marcos Da Silveira"
    ],
    "abstract": "Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T12:37:20+00:00",
    "updated": "2025-11-14T12:37:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11234v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11214v1",
    "title": "Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy",
    "authors": [
      "Jooyoung Lee",
      "Jader Martins Camboim de Sá"
    ],
    "abstract": "WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T12:12:10+00:00",
    "updated": "2025-11-14T12:12:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11214v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11182v1",
    "title": "Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning",
    "authors": [
      "Dayong Liang",
      "Xiao-Yong Wei",
      "Changmeng Zheng"
    ],
    "abstract": "Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like \"Who is Undercover?\". MUG reframes MAD as a process of detecting \"undercover\" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T11:27:55+00:00",
    "updated": "2025-11-14T11:27:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11182v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11141v1",
    "title": "PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases",
    "authors": [
      "Udo Schlegel",
      "Franziska Weeber",
      "Jian Lan",
      "Thomas Seidl"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T10:19:04+00:00",
    "updated": "2025-11-14T10:19:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11141v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11139v1",
    "title": "Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition",
    "authors": [
      "Yiming Rong",
      "Yixin Zhang",
      "Ziyi Wang",
      "Deyang Jiang",
      "Yunlong Zhao",
      "Haoran Wu",
      "Shiyu Zhou",
      "Bo Xu"
    ],
    "abstract": "Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T10:15:16+00:00",
    "updated": "2025-11-14T10:15:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11139v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11126v1",
    "title": "Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion",
    "authors": [
      "Yi Shi",
      "Wenlong Meng",
      "Zhenyuan Guo",
      "Chengkun Wei",
      "Wenzhi Chen"
    ],
    "abstract": "With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\\% on MET-MEME and 3.4\\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T09:59:08+00:00",
    "updated": "2025-11-14T09:59:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11126v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11124v1",
    "title": "AV-Dialog: Spoken Dialogue Models with Audio-Visual Input",
    "authors": [
      "Tuochao Chen",
      "Bandhav Veluri",
      "Hongyu Gong",
      "Shyamnath Gollakota"
    ],
    "abstract": "Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T09:56:26+00:00",
    "updated": "2025-11-14T09:56:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11124v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11108v1",
    "title": "Analysing Personal Attacks in U.S. Presidential Debates",
    "authors": [
      "Ruban Goyal",
      "Rohitash Chandra",
      "Sonit Singh"
    ],
    "abstract": "Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T09:36:26+00:00",
    "updated": "2025-11-14T09:36:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11108v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11104v1",
    "title": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
    "authors": [
      "Crystal Min Hui Poon",
      "Pai Chet Ng",
      "Xiaoxiao Miao",
      "Immanuel Jun Kai Loh",
      "Bowen Zhang",
      "Haoyu Song",
      "Ian Mcloughlin"
    ],
    "abstract": "Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.",
    "categories": [
      "cs.SD",
      "cs.CL"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-14T09:29:10+00:00",
    "updated": "2025-11-14T09:29:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11104v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11087v1",
    "title": "Can LLMs Detect Their Own Hallucinations?",
    "authors": [
      "Sora Kadotani",
      "Kosuke Nishida",
      "Kyosuke Nishida"
    ],
    "abstract": "Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T09:03:09+00:00",
    "updated": "2025-11-14T09:03:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11087v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11066v1",
    "title": "S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation",
    "authors": [
      "Jiechao Gao",
      "Chang Liu",
      "Yuangang Li"
    ],
    "abstract": "Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \\textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \\textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \\textsc{MIMIC-CXR} and \\textsc{IU X-Ray} benchmarks, where \\textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-14T08:34:06+00:00",
    "updated": "2025-11-14T08:34:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11066v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11041v1",
    "title": "Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB",
    "authors": [
      "Xingyu Ren",
      "Youran Sun",
      "Haoyu Liang"
    ],
    "abstract": "We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T07:51:59+00:00",
    "updated": "2025-11-14T07:51:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11041v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11018v1",
    "title": "Automata-Based Steering of Large Language Models for Diverse Structured Generation",
    "authors": [
      "Xiaokun Luan",
      "Zeming Wei",
      "Yihao Zhang",
      "Meng Sun"
    ],
    "abstract": "Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T07:10:23+00:00",
    "updated": "2025-11-14T07:10:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11018v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10985v1",
    "title": "When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets",
    "authors": [
      "Aladin Djuhera",
      "Farhan Ahmed",
      "Swanand Ravindra Kadhe",
      "Syed Zawad",
      "Heiko Ludwig",
      "Holger Boche"
    ],
    "abstract": "Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T06:12:16+00:00",
    "updated": "2025-11-14T06:12:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10985v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10984v1",
    "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
    "authors": [
      "Xiying Zhao",
      "Zhoufutu Wen",
      "Zhixuan Chen",
      "Jingzhe Ding",
      "Jianpeng Jiao",
      "Shuai Li",
      "Xi Li",
      "Danni Liang",
      "Shengda Long",
      "Qianqian Liu",
      "Xianbo Wu",
      "Hongwan Gao",
      "Xiang Gao",
      "Liang Hu",
      "Jiashuo Liu",
      "Mengyun Liu",
      "Weiran Shi",
      "Chenghao Yang",
      "Qianyu Yang",
      "Xuanliang Zhang",
      "Ge Zhang",
      "Wenhao Huang"
    ],
    "abstract": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T06:09:37+00:00",
    "updated": "2025-11-14T06:09:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10984v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10930v1",
    "title": "CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology",
    "authors": [
      "Richard J. Young",
      "Alice M. Matthews"
    ],
    "abstract": "Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T03:38:48+00:00",
    "updated": "2025-11-14T03:38:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10930v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10912v1",
    "title": "Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D",
    "authors": [
      "Arsh Gupta",
      "Ajay Narayanan Sridhar",
      "Bonam Mingole",
      "Amulya Yadav"
    ],
    "abstract": "Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T02:54:58+00:00",
    "updated": "2025-11-14T02:54:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10912v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10903v1",
    "title": "Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy",
    "authors": [
      "Ramya Kumar",
      "Dhruv Gulwani",
      "Sonit Singh"
    ],
    "abstract": "This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T02:31:12+00:00",
    "updated": "2025-11-14T02:31:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10903v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10902v1",
    "title": "Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions",
    "authors": [
      "Mengze Hong",
      "Di Jiang",
      "Weiwei Zhao",
      "Yawen Li",
      "Yihang Wang",
      "Xinyuan Luo",
      "Yanjie Sun",
      "Chen Jason Zhang"
    ],
    "abstract": "While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T02:29:23+00:00",
    "updated": "2025-11-14T02:29:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10902v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10900v2",
    "title": "Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering",
    "authors": [
      "Xueren Ge",
      "Sahil Murtaza",
      "Anthony Cortez",
      "Homa Alemzadeh"
    ],
    "abstract": "Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T02:21:48+00:00",
    "updated": "2025-11-18T21:50:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10900v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10899v1",
    "title": "From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models",
    "authors": [
      "Farima Fatahi Bayat",
      "Pouya Pezeshkpour",
      "Estevam Hruschka"
    ],
    "abstract": "Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.",
    "categories": [
      "cs.CL",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T02:21:34+00:00",
    "updated": "2025-11-14T02:21:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10899v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10887v1",
    "title": "MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking",
    "authors": [
      "Nishant Mishra",
      "Wilker Aziz",
      "Iacer Calixto"
    ],
    "abstract": "Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.",
    "categories": [
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T01:49:24+00:00",
    "updated": "2025-11-14T01:49:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10887v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10881v1",
    "title": "A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge",
    "authors": [
      "Jongyoon Song",
      "Sangwon Yu",
      "Sungroh Yoon"
    ],
    "abstract": "Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an \"I don't know\" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T01:18:18+00:00",
    "updated": "2025-11-14T01:18:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10881v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10879v1",
    "title": "ICX360: In-Context eXplainability 360 Toolkit",
    "authors": [
      "Dennis Wei",
      "Ronny Luss",
      "Xiaomeng Hu",
      "Lucas Monteiro Paes",
      "Pin-Yu Chen",
      "Karthikeyan Natesan Ramamurthy",
      "Erik Miehling",
      "Inge Vejsbjerg",
      "Hendrik Strobelt"
    ],
    "abstract": "Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T01:17:55+00:00",
    "updated": "2025-11-14T01:17:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10879v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10871v1",
    "title": "From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems",
    "authors": [
      "Parisa Rabbani",
      "Nimet Beyza Bozdag",
      "Dilek Hakkani-Tür"
    ],
    "abstract": "LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from \"Is this statement correct?\" to \"Is this speaker correct?\". Furthermore, we apply pressure in the form of a simple rebuttal (\"The previous answer is incorrect.\") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-14T00:55:28+00:00",
    "updated": "2025-11-14T00:55:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10871v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10850v1",
    "title": "Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs",
    "authors": [
      "Stefan Horoi",
      "Sangwoo Cho",
      "Supriyo Chakraborty",
      "Shi-Xiong Zhang",
      "Sambit Sahu",
      "Guy Wolf",
      "Genta Indra Winata"
    ],
    "abstract": "Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-13T23:20:57+00:00",
    "updated": "2025-11-13T23:20:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10850v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10846v1",
    "title": "Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English",
    "authors": [
      "Rebecca Dorn",
      "Christina Chance",
      "Casandra Rusti",
      "Charles Bickham",
      "Kai-Wei Chang",
      "Fred Morstatter",
      "Kristina Lerman"
    ],
    "abstract": "Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed \"silver\" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-13T23:13:08+00:00",
    "updated": "2025-11-13T23:13:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10846v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10840v1",
    "title": "Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders",
    "authors": [
      "Abir Harrasse",
      "Florent Draye",
      "Zhijing Jin",
      "Bernhard Schölkopf"
    ],
    "abstract": "Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-13T22:51:06+00:00",
    "updated": "2025-11-13T22:51:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10840v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10837v1",
    "title": "The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns",
    "authors": [
      "Elyes Hajji",
      "Aymen Bouguerra",
      "Fabio Arnez"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T22:42:18+00:00",
    "updated": "2025-11-13T22:42:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10837v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15709v1",
    "title": "Tokenisation over Bounded Alphabets is Hard",
    "authors": [
      "Violeta Kastreva",
      "Philip Whittington",
      "Dennis Komm",
      "Tiago Pimentel"
    ],
    "abstract": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.",
    "categories": [
      "cs.CL",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T18:59:56+00:00",
    "updated": "2025-11-19T18:59:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15709v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15698v1",
    "title": "RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue",
    "authors": [
      "Naveen Raman",
      "Jingwu Tang",
      "Zhiyu Chen",
      "Zheyuan Ryan Shi",
      "Sean Hudson",
      "Ameesh Kapoor",
      "Fei Fang"
    ],
    "abstract": "Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.",
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "published": "2025-11-19T18:55:16+00:00",
    "updated": "2025-11-19T18:55:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15698v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15694v1",
    "title": "The Impact of Quantization on Large Reasoning Model Reinforcement Learning",
    "authors": [
      "Medha Kumar",
      "Zifei Xu",
      "Xin Wang",
      "Tristan Webb"
    ],
    "abstract": "Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T18:50:58+00:00",
    "updated": "2025-11-19T18:50:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15694v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15684v1",
    "title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics",
    "authors": [
      "Michael McCabe",
      "Payel Mukhopadhyay",
      "Tanya Marwah",
      "Bruno Regaldo-Saint Blancard",
      "Francois Rozet",
      "Cristiana Diaconu",
      "Lucas Meyer",
      "Kaze W. K. Wong",
      "Hadi Sotoudeh",
      "Alberto Bietti",
      "Irina Espejo",
      "Rio Fear",
      "Siavash Golkar",
      "Tom Hehir",
      "Keiya Hirashima",
      "Geraud Krawezik",
      "Francois Lanusse",
      "Rudy Morel",
      "Ruben Ohana",
      "Liam Parker",
      "Mariel Pettee",
      "Jeff Shen",
      "Kyunghyun Cho",
      "Miles Cranmer",
      "Shirley Ho"
    ],
    "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T18:36:03+00:00",
    "updated": "2025-11-19T18:36:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15684v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15679v1",
    "title": "Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion",
    "authors": [
      "Jianqiao Mao",
      "Max A. Little"
    ],
    "abstract": "Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\\left(\\boldsymbol{X}^{*},\\boldsymbol{Y}^{*},\\boldsymbol{M}^{*}\\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T18:26:55+00:00",
    "updated": "2025-11-19T18:26:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15679v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15661v1",
    "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
    "authors": [
      "Yicheng He",
      "Chengsong Huang",
      "Zongxia Li",
      "Jiaxin Huang",
      "Yonghui Yang"
    ],
    "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T17:55:15+00:00",
    "updated": "2025-11-19T17:55:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15661v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15652v1",
    "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges",
    "authors": [
      "Kim N. Nolle",
      "Ivana Dusparic",
      "Rhodri Cusack",
      "Vinny Cahill"
    ],
    "abstract": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.   This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.   Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T17:40:13+00:00",
    "updated": "2025-11-19T17:40:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15652v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15634v1",
    "title": "Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities",
    "authors": [
      "Benjamin Dupuis",
      "Mert Gürbüzbalaban",
      "Umut Şimşekli",
      "Jian Wang",
      "Sinan Yildirim",
      "Lingjiong Zhu"
    ],
    "abstract": "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T17:18:54+00:00",
    "updated": "2025-11-19T17:18:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15634v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15633v1",
    "title": "Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning",
    "authors": [
      "Tao Hu",
      "Lan Li",
      "Zhen-Hao Xie",
      "Da-Wei Zhou"
    ],
    "abstract": "Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T17:14:47+00:00",
    "updated": "2025-11-19T17:14:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15633v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15632v1",
    "title": "CODE-II: A large-scale dataset for artificial intelligence in ECG analysis",
    "authors": [
      "Petrus E. O. G. B. Abreu",
      "Gabriela M. M. Paixão",
      "Jiawei Li",
      "Paulo R. Gomes",
      "Peter W. Macfarlane",
      "Ana C. S. Oliveira",
      "Vinicius T. Carvalho",
      "Thomas B. Schön",
      "Antonio Luiz P. Ribeiro",
      "Antônio H. Ribeiro"
    ],
    "abstract": "Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "published": "2025-11-19T17:14:05+00:00",
    "updated": "2025-11-19T17:14:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15632v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15619v1",
    "title": "CODE: A global approach to ODE dynamics learning",
    "authors": [
      "Nils Wildt",
      "Daniel M. Tartakovsky",
      "Sergey Oladyshkin",
      "Wolfgang Nowak"
    ],
    "abstract": "Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T17:04:24+00:00",
    "updated": "2025-11-19T17:04:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15615v1",
    "title": "Near-optimal delta-convex estimation of Lipschitz functions",
    "authors": [
      "Gábor Balázs"
    ],
    "abstract": "This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T17:02:30+00:00",
    "updated": "2025-11-19T17:02:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15615v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15600v1",
    "title": "US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery",
    "authors": [
      "Miruna-Alexandra Gafencu",
      "Yordanka Velikova",
      "Nassir Navab",
      "Mohammad Farid Azampour"
    ],
    "abstract": "Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T16:45:04+00:00",
    "updated": "2025-11-19T16:45:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15600v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15543v1",
    "title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation",
    "authors": [
      "Georgios Venianakis",
      "Constantinos Theodoropoulos",
      "Michail Kavousanakis"
    ],
    "abstract": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T15:37:17+00:00",
    "updated": "2025-11-19T15:37:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15543v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15530v1",
    "title": "Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss",
    "authors": [
      "Max Hirsch",
      "Federico Pichi"
    ],
    "abstract": "In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK",
    "categories": [
      "math.NA",
      "cs.LG"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-19T15:29:42+00:00",
    "updated": "2025-11-19T15:29:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15530v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15529v1",
    "title": "Decentralized Gaussian Process Classification and an Application in Subsea Robotics",
    "authors": [
      "Yifei Gao",
      "Hans J. He",
      "Daniel J. Stilwell",
      "James McMahon"
    ],
    "abstract": "Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-19T15:26:47+00:00",
    "updated": "2025-11-19T15:26:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15529v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15522v1",
    "title": "PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles",
    "authors": [
      "Yinan Yu",
      "Samuel Scheidegger"
    ],
    "abstract": "Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T15:19:32+00:00",
    "updated": "2025-11-19T15:19:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15522v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15507v1",
    "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
    "authors": [
      "Nika Haghtalab",
      "Omar Montasser",
      "Mingda Qiao"
    ],
    "abstract": "We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\\widetilde O((d + k) / ε^2)$ within $\\widetilde O(\\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\\widetilde O(\\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T14:59:47+00:00",
    "updated": "2025-11-19T14:59:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15507v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15503v1",
    "title": "A Tensor Compiler for Processing-In-Memory Architectures",
    "authors": [
      "Peiming Yang",
      "Sankeerth Durvasula",
      "Ivan Fernandez",
      "Mohammad Sadrosadati",
      "Onur Mutlu",
      "Gennady Pekhimenko",
      "Christina Giannoula"
    ],
    "abstract": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.",
    "categories": [
      "cs.AR",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "published": "2025-11-19T14:58:16+00:00",
    "updated": "2025-11-19T14:58:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15503v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15487v1",
    "title": "NTK-Guided Implicit Neural Teaching",
    "authors": [
      "Chen Zhang",
      "Wei Zuo",
      "Bingyang Cheng",
      "Yikun Wang",
      "Wei-Bin Kou",
      "Yik Chung WU",
      "Ngai Wong"
    ],
    "abstract": "Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T14:43:04+00:00",
    "updated": "2025-11-19T14:43:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15487v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15476v1",
    "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
    "authors": [
      "Rashid Iqbal",
      "Saddam Hussain Khan"
    ],
    "abstract": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T14:32:34+00:00",
    "updated": "2025-11-19T14:32:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15476v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15464v1",
    "title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome",
    "authors": [
      "Dabin Jeong",
      "Amirhossein Vahidi",
      "Ciro Ramírez-Suástegui",
      "Marie Moullet",
      "Kevin Ly",
      "Mohammad Vali Sanian",
      "Sebastian Birk",
      "Yinshui Chang",
      "Adam Boxall",
      "Daniyal Jafree",
      "Lloyd Steele",
      "Vijaya Baskar MS",
      "Muzlifah Haniffa",
      "Mohammad Lotfollahi"
    ],
    "abstract": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T14:22:23+00:00",
    "updated": "2025-11-19T14:22:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15464v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15454v1",
    "title": "FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning",
    "authors": [
      "Ouiame Marnissi",
      "Hajar EL Hammouti",
      "El Houcine Bergou"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\\% compared to baseline strategies.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T14:11:44+00:00",
    "updated": "2025-11-19T14:11:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15454v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15447v1",
    "title": "TSFM in-context learning for time-series classification of bearing-health status",
    "authors": [
      "Michel Tokic",
      "Slobodan Djukanović",
      "Anja von Beuningen",
      "Cheng Feng"
    ],
    "abstract": "This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T14:01:12+00:00",
    "updated": "2025-11-19T14:01:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15447v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15446v1",
    "title": "Gini Score under Ties and Case Weights",
    "authors": [
      "Alexej Brauer",
      "Mario V. Wüthrich"
    ],
    "abstract": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T14:01:12+00:00",
    "updated": "2025-11-19T14:01:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15446v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15445v1",
    "title": "Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation",
    "authors": [
      "Victorita Dolean",
      "Daria Hrebenshchykova",
      "Stéphane Lanteri",
      "Victor Michel-Dansac"
    ],
    "abstract": "Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.",
    "categories": [
      "math.NA",
      "cs.LG"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-19T13:58:32+00:00",
    "updated": "2025-11-19T13:58:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15445v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15432v1",
    "title": "Towards Understanding Layer Contributions in Tabular In-Context Learning Models",
    "authors": [
      "Amir Rezaei Balef",
      "Mykhailo Koshil",
      "Katharina Eggensperger"
    ],
    "abstract": "Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the \"layers as painters\" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T13:39:30+00:00",
    "updated": "2025-11-19T13:39:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15432v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15411v1",
    "title": "D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models",
    "authors": [
      "Wenlun Zhang",
      "Yunshan Zhong",
      "Zihao Ding",
      "Xinyu Li",
      "Kentaro Yoshioka"
    ],
    "abstract": "Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T13:08:25+00:00",
    "updated": "2025-11-19T13:08:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15411v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15409v1",
    "title": "Proximal Approximate Inference in State-Space Models",
    "authors": [
      "Hany Abdulsamad",
      "Ángel F. García-Fernández",
      "Simo Särkkä"
    ],
    "abstract": "We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T13:06:08+00:00",
    "updated": "2025-11-19T13:06:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15409v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15406v1",
    "title": "Controlling False Positives in Image Segmentation via Conformal Prediction",
    "authors": [
      "Luca Mossina",
      "Corentin Friedrich"
    ],
    "abstract": "Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T13:02:50+00:00",
    "updated": "2025-11-19T13:02:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15406v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15393v1",
    "title": "EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG",
    "authors": [
      "Kunyu Zhang",
      "Mingxuan Wang",
      "Xiangjie Shi",
      "Haoxing Xu",
      "Chao Zhang"
    ],
    "abstract": "The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T12:39:19+00:00",
    "updated": "2025-11-19T12:39:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15393v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15375v1",
    "title": "Parameter Importance-Driven Continual Learning for Foundation Models",
    "authors": [
      "Lingxiang Wang",
      "Hainan Zhang",
      "Zhiming Zheng"
    ],
    "abstract": "Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T12:07:53+00:00",
    "updated": "2025-11-19T12:07:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15375v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15371v1",
    "title": "CID: Measuring Feature Importance Through Counterfactual Distributions",
    "authors": [
      "Eddie Conti",
      "Álvaro Parafita",
      "Axel Brando"
    ],
    "abstract": "Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T11:57:59+00:00",
    "updated": "2025-11-19T11:57:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15371v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15357v1",
    "title": "Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction",
    "authors": [
      "Yinan Yu",
      "Falk Dippel",
      "Christina E. Lundberg",
      "Martin Lindgren",
      "Annika Rosengren",
      "Martin Adiels",
      "Helen Sjöland"
    ],
    "abstract": "Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T11:34:47+00:00",
    "updated": "2025-11-19T11:34:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15357v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15350v1",
    "title": "Multi-layer Stack Ensembles for Time Series Forecasting",
    "authors": [
      "Nathanael Bosch",
      "Oleksandr Shchur",
      "Nick Erickson",
      "Michael Bohlke-Schneider",
      "Caner Türkmen"
    ],
    "abstract": "Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T11:21:00+00:00",
    "updated": "2025-11-19T11:21:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15350v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15343v1",
    "title": "Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection",
    "authors": [
      "Spyridon Loukovitis",
      "Vasileios Karampinis",
      "Athanasios Voulodimos"
    ],
    "abstract": "Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T11:03:47+00:00",
    "updated": "2025-11-19T11:03:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15343v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15339v1",
    "title": "STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection",
    "authors": [
      "Kadir-Kaan Özer",
      "René Ebeling",
      "Markus Enzweiler"
    ],
    "abstract": "Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.   In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.   Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T10:58:40+00:00",
    "updated": "2025-11-19T10:58:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15339v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15332v1",
    "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss",
    "authors": [
      "The Tien Mai"
    ],
    "abstract": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.   Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T10:50:46+00:00",
    "updated": "2025-11-19T10:50:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15332v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15328v1",
    "title": "LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials",
    "authors": [
      "Huseyin Goksu"
    ],
    "abstract": "Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on \"heterophilic\" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T10:47:23+00:00",
    "updated": "2025-11-19T10:47:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15328v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15327v1",
    "title": "KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials",
    "authors": [
      "Huseyin Goksu"
    ],
    "abstract": "Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on \"heterophilic\" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \\textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T10:47:15+00:00",
    "updated": "2025-11-19T10:47:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15327v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15324v1",
    "title": "On the Internal Semantics of Time-Series Foundation Models",
    "authors": [
      "Atharva Pandey",
      "Abhilash Neog",
      "Gautam Jajoo"
    ],
    "abstract": "Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T10:41:02+00:00",
    "updated": "2025-11-19T10:41:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15324v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15315v1",
    "title": "Robust Bayesian Optimisation with Unbounded Corruptions",
    "authors": [
      "Abdelhamid Ezzerg",
      "Ilija Bogunovic",
      "Jeremias Knoblauch"
    ],
    "abstract": "Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T10:28:56+00:00",
    "updated": "2025-11-19T10:28:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15315v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15300v1",
    "title": "Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs",
    "authors": [
      "Rayen Dhahri",
      "Steffen Urban"
    ],
    "abstract": "Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T10:09:02+00:00",
    "updated": "2025-11-19T10:09:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15300v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15276v1",
    "title": "SNAP: Low-Latency Test-Time Adaptation with Sparse Updates",
    "authors": [
      "Hyeongheon Cha",
      "Dong Min Kim",
      "Hye Won Chung",
      "Taesik Gong",
      "Sung-Ju Lee"
    ],
    "abstract": "Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:40:24+00:00",
    "updated": "2025-11-19T09:40:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15276v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15271v1",
    "title": "Graph Query Networks for Object Detection with Automotive Radar",
    "authors": [
      "Loveneet Saini",
      "Hasan Tercan",
      "Tobias Meisen"
    ],
    "abstract": "Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T09:36:49+00:00",
    "updated": "2025-11-19T09:36:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15271v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15262v1",
    "title": "Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution",
    "authors": [
      "Tomas Espana",
      "Yadh Hafsi",
      "Fabrizio Lillo",
      "Edoardo Vittori"
    ],
    "abstract": "We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.",
    "categories": [
      "q-fin.TR",
      "cs.LG"
    ],
    "primary_category": "q-fin.TR",
    "published": "2025-11-19T09:26:23+00:00",
    "updated": "2025-11-19T09:26:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15262v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15256v1",
    "title": "GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning",
    "authors": [
      "Yanchen Xu",
      "Ziheng Jiao",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "abstract": "The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:19:39+00:00",
    "updated": "2025-11-19T09:19:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15256v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15251v1",
    "title": "PLATONT: Learning a Platonic Representation for Unified Network Tomography",
    "authors": [
      "Chengze Du",
      "Heng Xu",
      "Zhiwei Yu",
      "Bo Liu",
      "Jialong Li"
    ],
    "abstract": "Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:10:32+00:00",
    "updated": "2025-11-19T09:10:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15251v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15250v1",
    "title": "Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling",
    "authors": [
      "Jin Ye",
      "Lingmei Wang",
      "Shujian Zhang",
      "Haihang WU"
    ],
    "abstract": "With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:10:02+00:00",
    "updated": "2025-11-19T09:10:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15250v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15248v1",
    "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "authors": [
      "Kai Yang",
      "Xin Xu",
      "Yangkun Chen",
      "Weijie Liu",
      "Jiafei Lyu",
      "Zichuan Lin",
      "Deheng Ye",
      "Saiyong Yang"
    ],
    "abstract": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:06:42+00:00",
    "updated": "2025-11-19T09:06:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15248v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15246v1",
    "title": "D2D Power Allocation via Quantum Graph Neural Network",
    "authors": [
      "Tung Giang Le",
      "Xuan Tung Nguyen",
      "Won-Joo Hwang"
    ],
    "abstract": "Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T09:05:29+00:00",
    "updated": "2025-11-19T09:05:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15246v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15222v1",
    "title": "Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets",
    "authors": [
      "Pol Benítez",
      "Cibrán López",
      "Edgardo Saucedo",
      "Teruyasu Mizoguchi",
      "Claudio Cazorla"
    ],
    "abstract": "Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "published": "2025-11-19T08:16:10+00:00",
    "updated": "2025-11-19T08:16:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15222v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15210v1",
    "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
    "authors": [
      "Vladislav Pedashenko",
      "Laida Kushnareva",
      "Yana Khassan Nibal",
      "Eduard Tulchinskii",
      "Kristian Kuznetsov",
      "Vladislav Zharchinskii",
      "Yury Maximov",
      "Irina Piontkovskaya"
    ],
    "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T08:00:40+00:00",
    "updated": "2025-11-19T08:00:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15210v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15208v1",
    "title": "Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones",
    "authors": [
      "Ranfei Chen",
      "Ming Chen",
      "Kaifei Wang"
    ],
    "abstract": "Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured \"zones of confusion\": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T07:59:34+00:00",
    "updated": "2025-11-19T07:59:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15208v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15199v1",
    "title": "Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking",
    "authors": [
      "Jiajun Zhan",
      "Zeyuan Ma",
      "Yue-Jiao Gong",
      "Kay Chen Tan"
    ],
    "abstract": "Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "published": "2025-11-19T07:38:09+00:00",
    "updated": "2025-11-19T07:38:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15199v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15196v1",
    "title": "Particle Monte Carlo methods for Lattice Field Theory",
    "authors": [
      "David Yallup"
    ],
    "abstract": "High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "hep-lat"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T07:31:46+00:00",
    "updated": "2025-11-19T07:31:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15196v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15190v1",
    "title": "Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning",
    "authors": [
      "Yuxuan Gu",
      "Weimin Bai",
      "Yifei Wang",
      "Weijian Luo",
      "He Sun"
    ],
    "abstract": "Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T07:24:07+00:00",
    "updated": "2025-11-19T07:24:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15190v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15188v1",
    "title": "BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI",
    "authors": [
      "Wasif Jalal",
      "Md Nafiu Rahman",
      "M. Sohel Rahman"
    ],
    "abstract": "Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T07:20:23+00:00",
    "updated": "2025-11-19T07:20:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15188v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15183v1",
    "title": "HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples",
    "authors": [
      "Rishikant Chigrupaatii",
      "Ponnada Sai Tulasi Kanishka",
      "Lalit Chandra Routhu",
      "Martin Patel Sama Supratheek Reddy",
      "Divyam Gupta",
      "Dasari Srikar",
      "Krishna Teja Kuchimanchi",
      "Rajiv Misra",
      "Rohun Tripathi"
    ],
    "abstract": "With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T07:11:00+00:00",
    "updated": "2025-11-19T07:11:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15183v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15175v1",
    "title": "Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning",
    "authors": [
      "Le Tung Giang",
      "Vu Hoang Viet",
      "Nguyen Xuan Tung",
      "Trinh Van Chien",
      "Won-Joo Hwang"
    ],
    "abstract": "The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.",
    "categories": [
      "cs.LG",
      "cs.IT",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T06:54:38+00:00",
    "updated": "2025-11-19T06:54:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15175v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15174v1",
    "title": "FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model",
    "authors": [
      "Yi Xu",
      "Zhigang Chen",
      "Rui Wang",
      "Yangfan Li",
      "Fengxiao Tang",
      "Ming Zhao",
      "Jiaqi Liu"
    ],
    "abstract": "In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T06:53:15+00:00",
    "updated": "2025-11-19T06:53:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15174v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15173v1",
    "title": "Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits",
    "authors": [
      "Jun Hyeun Kang",
      "Jung Eek Son",
      "Tae In Ahn"
    ],
    "abstract": "The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "published": "2025-11-19T06:51:27+00:00",
    "updated": "2025-11-19T06:51:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15173v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15172v1",
    "title": "Complex variational autoencoders admit Kähler structure",
    "authors": [
      "Andrew Gracyk"
    ],
    "abstract": "It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T06:51:03+00:00",
    "updated": "2025-11-19T06:51:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15172v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15163v1",
    "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs",
    "authors": [
      "Yang Wu",
      "Rujing Yao",
      "Tong Zhang",
      "Yufei Shi",
      "Zhuoren Jiang",
      "Zhushan Li",
      "Xiaozhong Liu"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-19T06:28:16+00:00",
    "updated": "2025-11-19T06:28:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15163v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15162v1",
    "title": "Multimodal Wireless Foundation Models",
    "authors": [
      "Ahmed Aboulfotouh",
      "Hatem Abou-Zeid"
    ],
    "abstract": "Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "published": "2025-11-19T06:26:49+00:00",
    "updated": "2025-11-19T06:26:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15162v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15159v1",
    "title": "Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation",
    "authors": [
      "Firdavs Nasriddinov",
      "Rafal Kocielnik",
      "Anima Anandkumar",
      "Andrew J. Hung"
    ],
    "abstract": "High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T06:19:34+00:00",
    "updated": "2025-11-19T06:19:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15151v1",
    "title": "DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging",
    "authors": [
      "Meihua Zhou",
      "Xinyu Tong",
      "Jiarui Zhao",
      "Min Cheng",
      "Li Yang",
      "Lei Tian",
      "Nan Wan"
    ],
    "abstract": "High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T06:10:31+00:00",
    "updated": "2025-11-19T06:10:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15151v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15146v1",
    "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings",
    "authors": [
      "Eugene Ndiaye"
    ],
    "abstract": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T05:59:01+00:00",
    "updated": "2025-11-19T05:59:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15146v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15139v1",
    "title": "CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery",
    "authors": [
      "Amit Kumar",
      "Maninder Kaur",
      "Raghvendra Mall",
      "Sukrit Gupta"
    ],
    "abstract": "Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "published": "2025-11-19T05:36:05+00:00",
    "updated": "2025-11-19T05:36:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15139v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15138v1",
    "title": "Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems",
    "authors": [
      "Hyo-Jeong Jang",
      "Hye-Bin Shin",
      "Kang Yin"
    ],
    "abstract": "Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.",
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T05:33:48+00:00",
    "updated": "2025-11-19T05:33:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15138v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15137v1",
    "title": "From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs",
    "authors": [
      "Xiaoxuan Wang",
      "Bo Liu",
      "Song Jiang",
      "Jingzhou Liu",
      "Jingyuan Qi",
      "Xia Chen",
      "Baosheng He"
    ],
    "abstract": "The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T05:27:06+00:00",
    "updated": "2025-11-19T05:27:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15137v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15136v1",
    "title": "Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature",
    "authors": [
      "Andrew Amos",
      "Joanne Lee",
      "Tarun Sen Gupta",
      "Bunmi S. Malau-Aduli"
    ],
    "abstract": "Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T05:26:04+00:00",
    "updated": "2025-11-19T05:26:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15136v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15132v1",
    "title": "WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images",
    "authors": [
      "Nishchala Thakur",
      "Swati Kochhar",
      "Deepti R. Bathula",
      "Sukrit Gupta"
    ],
    "abstract": "Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T05:23:23+00:00",
    "updated": "2025-11-19T05:23:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15132v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15125v1",
    "title": "Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling",
    "authors": [
      "Huifan Zhang",
      "Pingqiang Zhou"
    ],
    "abstract": "Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T05:04:09+00:00",
    "updated": "2025-11-19T05:04:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15125v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15120v1",
    "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit",
    "authors": [
      "Bohan Zhang",
      "Zihao Wang",
      "Hengyu Fu",
      "Jason D. Lee"
    ],
    "abstract": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T04:46:47+00:00",
    "updated": "2025-11-19T04:46:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15120v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15112v1",
    "title": "Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data",
    "authors": [
      "Wei-hsiang Yen",
      "Lyn Chao-ling Chen"
    ],
    "abstract": "The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T04:36:02+00:00",
    "updated": "2025-11-19T04:36:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15112v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15083v1",
    "title": "Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection",
    "authors": [
      "Xiancheng Wang",
      "Lin Wang",
      "Rui Wang",
      "Zhibo Zhang",
      "Minghang Zhao"
    ],
    "abstract": "Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.   Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T03:45:06+00:00",
    "updated": "2025-11-19T03:45:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15083v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15076v1",
    "title": "GPU-Initiated Networking for NCCL",
    "authors": [
      "Khaled Hamidouche",
      "John Bachan",
      "Pak Markthub",
      "Peter-Jan Gootzen",
      "Elena Agostini",
      "Sylvain Jeaugey",
      "Aamir Shafi",
      "Georgios Theodorakis",
      "Manjunath Gorentla Venkata"
    ],
    "abstract": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.   NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "published": "2025-11-19T03:36:03+00:00",
    "updated": "2025-11-19T03:36:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15076v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15067v1",
    "title": "Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer",
    "authors": [
      "Zisong Wang",
      "Xuanyu Wang",
      "Hang Chen",
      "Haizhou Wang",
      "Yuxin Chen",
      "Yihang Xu",
      "Yunhe Yuan",
      "Lihuan Luo",
      "Xitong Ling",
      "Xiaoping Liu"
    ],
    "abstract": "Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T03:19:43+00:00",
    "updated": "2025-11-19T03:19:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15067v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15062v1",
    "title": "Interpretable temporal fusion network of multi- and multi-class arrhythmia classification",
    "authors": [
      "Yun Kwan Kim"
    ],
    "abstract": "Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T03:09:42+00:00",
    "updated": "2025-11-19T03:09:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15062v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15061v1",
    "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
    "authors": [
      "Haodong Chen",
      "Guido Zuccon",
      "Teerapong Leelanupab"
    ],
    "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.   In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.   OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-19T03:08:20+00:00",
    "updated": "2025-11-19T03:08:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15061v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15055v1",
    "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
    "authors": [
      "Jian-Ting Guo",
      "Yu-Cheng Chen",
      "Ping-Chun Hsieh",
      "Kuo-Hao Ho",
      "Po-Wei Huang",
      "Ti-Rong Wu",
      "I-Chen Wu"
    ],
    "abstract": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-19T02:59:47+00:00",
    "updated": "2025-11-19T02:59:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15055v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15048v1",
    "title": "Oversampling techniques for predicting COVID-19 patient length of stay",
    "authors": [
      "Zachariah Farahany",
      "Jiawei Wu",
      "K M Sajjadul Islam",
      "Praveen Madiraju"
    ],
    "abstract": "COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T02:38:10+00:00",
    "updated": "2025-11-19T02:38:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15048v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15032v1",
    "title": "Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment",
    "authors": [
      "Jeffrey Jiang",
      "Kevin Hong",
      "Emily Kuczynski",
      "Gregory Pottie"
    ],
    "abstract": "While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T01:57:52+00:00",
    "updated": "2025-11-19T01:57:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15032v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15022v1",
    "title": "Complex-Valued 2D Gaussian Representation for Computer-Generated Holography",
    "authors": [
      "Yicheng Zhan",
      "Xiangjun Gao",
      "Long Quan",
      "Kaan Akşit"
    ],
    "abstract": "We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T01:41:14+00:00",
    "updated": "2025-11-19T01:41:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15022v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15015v1",
    "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference",
    "authors": [
      "Kexin Chu",
      "Dawei Xiang",
      "Zixu Shen",
      "Yiwei Yang",
      "Zecheng Liu",
      "Wei Zhang"
    ],
    "abstract": "Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.   Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PF",
    "published": "2025-11-19T01:27:54+00:00",
    "updated": "2025-11-19T01:27:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15015v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15010v1",
    "title": "Latent space analysis and generalization to out-of-distribution data",
    "authors": [
      "Katie Rainey",
      "Erin Hausmann",
      "Donald Waagen",
      "David Gray",
      "Donald Hulsey"
    ],
    "abstract": "Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \\textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T01:23:34+00:00",
    "updated": "2025-11-19T01:23:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15010v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15004v1",
    "title": "IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics",
    "authors": [
      "Halil S. Kelebek",
      "Linnea M. Wolniewicz",
      "Michael D. Vergalla",
      "Simone Mestici",
      "Giacomo Acciarini",
      "Bala Poduval",
      "Olga Verkhoglyadova",
      "Madhulika Guhathakurta",
      "Thomas E. Berger",
      "Frank Soboczenski",
      "Atılım Güneş Baydin"
    ],
    "abstract": "The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.",
    "categories": [
      "cs.LG",
      "astro-ph.EP"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T00:58:17+00:00",
    "updated": "2025-11-19T00:58:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15004v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15003v1",
    "title": "Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks",
    "authors": [
      "Reza Mirjalili",
      "Behrad Braghi",
      "Shahram Shadrokh Sikari"
    ],
    "abstract": "Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.   We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.",
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "primary_category": "stat.AP",
    "published": "2025-11-19T00:56:12+00:00",
    "updated": "2025-11-19T00:56:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15003v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15002v1",
    "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning",
    "authors": [
      "Fatemeh Lotfi",
      "Hossein Rajoli",
      "Fatemeh Afghah"
    ],
    "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-19T00:55:24+00:00",
    "updated": "2025-11-19T00:55:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15002v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14993v1",
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "authors": [
      "Vladimir Arkhipkin",
      "Vladimir Korviakov",
      "Nikolai Gerasimenko",
      "Denis Parkhomenko",
      "Viacheslav Vasilev",
      "Alexey Letunovskiy",
      "Maria Kovaleva",
      "Nikolai Vaulin",
      "Ivan Kirillov",
      "Lev Novitskiy",
      "Denis Koposov",
      "Nikita Kiselev",
      "Alexander Varlamov",
      "Dmitrii Mikhailov",
      "Vladimir Polovnikov",
      "Andrey Shutkin",
      "Ilya Vasiliev",
      "Julia Agafonova",
      "Anastasiia Kargapoltseva",
      "Anna Dmitrienko",
      "Anastasia Maltseva",
      "Anna Averchenkova",
      "Olga Kim",
      "Tatiana Nikulina",
      "Denis Dimitrov"
    ],
    "abstract": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T00:23:22+00:00",
    "updated": "2025-11-19T00:23:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14993v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14981v1",
    "title": "Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation",
    "authors": [
      "Nicholas Cooper",
      "Lijun Chen",
      "Sailesh Dwivedy",
      "Danna Gurari"
    ],
    "abstract": "Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T23:50:31+00:00",
    "updated": "2025-11-18T23:50:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14981v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14980v1",
    "title": "Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework",
    "authors": [
      "Ahmet Umur Özsoy"
    ],
    "abstract": "Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.",
    "categories": [
      "q-fin.MF",
      "cs.LG"
    ],
    "primary_category": "q-fin.MF",
    "published": "2025-11-18T23:47:49+00:00",
    "updated": "2025-11-18T23:47:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14980v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14969v1",
    "title": "Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion",
    "authors": [
      "Zanxu Wang",
      "Homayoon Beigi"
    ],
    "abstract": "This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-18T23:24:27+00:00",
    "updated": "2025-11-18T23:24:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14969v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14967v1",
    "title": "MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation",
    "authors": [
      "Basel Shbita",
      "Farhan Ahmed",
      "Chad DeLuca"
    ],
    "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "published": "2025-11-18T23:14:44+00:00",
    "updated": "2025-11-18T23:14:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14967v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14962v1",
    "title": "Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks",
    "authors": [
      "Haizhou Wen",
      "He Li",
      "Zhen Li"
    ],
    "abstract": "Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.",
    "categories": [
      "physics.comp-ph",
      "cs.LG",
      "eess.IV",
      "physics.bio-ph",
      "q-bio.QM"
    ],
    "primary_category": "physics.comp-ph",
    "published": "2025-11-18T23:04:13+00:00",
    "updated": "2025-11-18T23:04:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14962v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14961v1",
    "title": "Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference",
    "authors": [
      "Artur A. Oliveira",
      "Mateus Espadoto",
      "Roberto M. Cesar",
      "Roberto Hirata"
    ],
    "abstract": "We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T23:02:59+00:00",
    "updated": "2025-11-18T23:02:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14961v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14953v1",
    "title": "Compiling to recurrent neurons",
    "authors": [
      "Joey Velez-Ginorio",
      "Nada Amin",
      "Konrad Kording",
      "Steve Zdancewic"
    ],
    "abstract": "Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\\textsf{Cajal}\\scriptstyle(\\mathbb{\\multimap}, \\mathbb{2}, \\mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.",
    "categories": [
      "cs.PL",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "published": "2025-11-18T22:26:27+00:00",
    "updated": "2025-11-18T22:26:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14953v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14952v1",
    "title": "Artificial intelligence approaches for energy-efficient laser cutting machines",
    "authors": [
      "Mohamed Abdallah Salem",
      "Hamdy Ahmed Ashour",
      "Ahmed Elshenawy"
    ],
    "abstract": "This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T22:25:58+00:00",
    "updated": "2025-11-18T22:25:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14952v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14939v1",
    "title": "Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report",
    "authors": [
      "Daniel Oliveira de Brito",
      "Letícia Gabriella de Souza",
      "Marcelo Matheus Gauy",
      "Marcelo Finger",
      "Arnaldo Candido Junior"
    ],
    "abstract": "This technical report investigates the performance of pre-trained audio models on COVID-19 detection tasks using established benchmark datasets. We fine-tuned Audio-MAE and three PANN architectures (CNN6, CNN10, CNN14) on the Coswara and COUGHVID datasets, evaluating both intra-dataset and cross-dataset generalization. We implemented a strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). Our experiments demonstrate that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage - a confounding factor that inflate performance metrics. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These findings highlight fundamental challenges in developing generalizable audio-based COVID-19 detection systems and underscore the importance of rigorous demographic controls for clinically robust model evaluation.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T21:54:20+00:00",
    "updated": "2025-11-18T21:54:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14939v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14936v1",
    "title": "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding",
    "authors": [
      "Mathieu Dufour",
      "Andrew Duncan"
    ],
    "abstract": "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:51:04+00:00",
    "updated": "2025-11-18T21:51:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14936v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14922v1",
    "title": "Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis",
    "authors": [
      "Pranay Kumar Peddi",
      "Dhrubajyoti Ghosh"
    ],
    "abstract": "Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:22:24+00:00",
    "updated": "2025-11-18T21:22:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14922v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14920v1",
    "title": "Structured Contrastive Learning for Interpretable Latent Representations",
    "authors": [
      "Zhengyang Shen",
      "Hua Tu",
      "Mayue Shi"
    ],
    "abstract": "Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as \"laissez-faire\" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:18:20+00:00",
    "updated": "2025-11-18T21:18:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14920v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14908v1",
    "title": "On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs",
    "authors": [
      "Gefté Almeida",
      "Marcio Pohlmann",
      "Alex Severo",
      "Diego Kreutz",
      "Tiago Heinrich",
      "Lourenço Pereira"
    ],
    "abstract": "In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T20:56:49+00:00",
    "updated": "2025-11-18T20:56:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14908v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14903v1",
    "title": "It's LIT! Reliability-Optimized LLMs with Inspectable Tools",
    "authors": [
      "Ruixin Zhang",
      "Jon Donnelly",
      "Zhicheng Guo",
      "Ghazal Khalighinejad",
      "Haiyang Huang",
      "Alina Jade Barnett",
      "Cynthia Rudin"
    ],
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:41:58+00:00",
    "updated": "2025-11-18T20:41:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14903v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14897v1",
    "title": "HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation",
    "authors": [
      "Pranav Indrakanti",
      "Ivor Simpson"
    ],
    "abstract": "We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T20:36:24+00:00",
    "updated": "2025-11-18T20:36:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14897v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14889v1",
    "title": "Bringing Federated Learning to Space",
    "authors": [
      "Grace Kim",
      "Filip Svoboda",
      "Nicholas Lane"
    ],
    "abstract": "As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive \"space-ification\" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:16:07+00:00",
    "updated": "2025-11-18T20:16:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14889v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14887v1",
    "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone",
    "authors": [
      "Nathan M. Roberts",
      "Xiaosong Du"
    ],
    "abstract": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:11:54+00:00",
    "updated": "2025-11-18T20:11:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14887v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14882v1",
    "title": "Exact Learning of Weighted Graphs Using Composite Queries",
    "authors": [
      "Michael T. Goodrich",
      "Songyu Liu",
      "Ioannis Panageas"
    ],
    "abstract": "In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.",
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "published": "2025-11-18T20:02:02+00:00",
    "updated": "2025-11-18T20:02:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14882v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14876v1",
    "title": "Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard",
    "authors": [
      "Henry Wong",
      "Clement Fung",
      "Weiran Lin",
      "Karen Li",
      "Stanley Chen",
      "Lujo Bauer"
    ],
    "abstract": "To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.   To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.   We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T19:49:46+00:00",
    "updated": "2025-11-18T19:49:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14876v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14868v1",
    "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings",
    "authors": [
      "Xueying Ding",
      "Xingyue Huang",
      "Mingxuan Ju",
      "Liam Collins",
      "Yozen Liu",
      "Leman Akoglu",
      "Neil Shah",
      "Tong Zhao"
    ],
    "abstract": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T19:37:40+00:00",
    "updated": "2025-11-18T19:37:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14868v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14865v1",
    "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
    "authors": [
      "Dwipam Katariya",
      "Snehita Varma",
      "Akshat Shreemali",
      "Benjamin Wu",
      "Kalanand Mishra",
      "Pranab Mohanty"
    ],
    "abstract": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T19:32:38+00:00",
    "updated": "2025-11-18T19:32:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14865v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14846v1",
    "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization",
    "authors": [
      "Yifeng Ding",
      "Hung Le",
      "Songyang Han",
      "Kangrui Ruan",
      "Zhenghui Jin",
      "Varun Kumar",
      "Zijian Wang",
      "Anoop Deoras"
    ],
    "abstract": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T19:01:16+00:00",
    "updated": "2025-11-18T19:01:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14846v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14832v1",
    "title": "How to pick the best anomaly detector?",
    "authors": [
      "Marie Hein",
      "Gregor Kasieczka",
      "Michael Krämer",
      "Louis Moureaux",
      "Alexander Mück",
      "David Shih"
    ],
    "abstract": "Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.",
    "categories": [
      "hep-ph",
      "cs.LG",
      "hep-ex",
      "physics.data-an"
    ],
    "primary_category": "hep-ph",
    "published": "2025-11-18T19:00:01+00:00",
    "updated": "2025-11-18T19:00:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14832v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14761v1",
    "title": "ARC Is a Vision Problem!",
    "authors": [
      "Keya Hu",
      "Ali Cy",
      "Linlu Qiu",
      "Xiaoman Delores Ding",
      "Runqian Wang",
      "Yeyin Eva Zhu",
      "Jacob Andreas",
      "Kaiming He"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T18:59:49+00:00",
    "updated": "2025-11-18T18:59:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14761v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14759v2",
    "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
    "authors": [
      "Physical Intelligence",
      "Ali Amin",
      "Raichelle Aniceto",
      "Ashwin Balakrishna",
      "Kevin Black",
      "Ken Conley",
      "Grace Connors",
      "James Darpinian",
      "Karan Dhabalia",
      "Jared DiCarlo",
      "Danny Driess",
      "Michael Equi",
      "Adnan Esmail",
      "Yunhao Fang",
      "Chelsea Finn",
      "Catherine Glossop",
      "Thomas Godden",
      "Ivan Goryachev",
      "Lachy Groom",
      "Hunter Hancock",
      "Karol Hausman",
      "Gashon Hussein",
      "Brian Ichter",
      "Szymon Jakubczak",
      "Rowan Jen",
      "Tim Jones",
      "Ben Katz",
      "Liyiming Ke",
      "Chandra Kuchi",
      "Marinda Lamb",
      "Devin LeBlanc",
      "Sergey Levine",
      "Adrian Li-Bell",
      "Yao Lu",
      "Vishnu Mano",
      "Mohith Mothukuri",
      "Suraj Nair",
      "Karl Pertsch",
      "Allen Z. Ren",
      "Charvi Sharma",
      "Lucy Xiaoyang Shi",
      "Laura Smith",
      "Jost Tobias Springenberg",
      "Kyle Stachowicz",
      "Will Stoeckle",
      "Alex Swerdlow",
      "James Tanner",
      "Marcel Torne",
      "Quan Vuong",
      "Anna Walling",
      "Haohuan Wang",
      "Blake Williams",
      "Sukwon Yoo",
      "Lili Yu",
      "Ury Zhilinsky",
      "Zhiyuan Zhou"
    ],
    "abstract": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:58:55+00:00",
    "updated": "2025-11-19T04:34:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14759v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14755v1",
    "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis",
    "authors": [
      "Albert Lin",
      "Alessandro Pinto",
      "Somil Bansal"
    ],
    "abstract": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.",
    "categories": [
      "cs.RO",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-18T18:55:20+00:00",
    "updated": "2025-11-18T18:55:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14755v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14753v1",
    "title": "SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction",
    "authors": [
      "Junfeng Wu",
      "Hadjer Benmeziane",
      "Kaoutar El Maghraoui",
      "Liu Liu",
      "Yinan Wang"
    ],
    "abstract": "Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:53:37+00:00",
    "updated": "2025-11-18T18:53:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14753v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14827v1",
    "title": "Implicit Bias of the JKO Scheme",
    "authors": [
      "Peter Halmos",
      "Boris Hanin"
    ],
    "abstract": "Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \\emph{modified} energy \\[ J^η(ρ) = J(ρ) - \\fracη{4}\\int_M \\Big\\lVert \\nabla_g \\frac{δJ}{δρ} (ρ) \\Big\\rVert_{2}^{2} \\,ρ(dx), \\] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \\textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \\emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T18:48:37+00:00",
    "updated": "2025-11-18T18:48:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14827v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14745v1",
    "title": "Look-Ahead Reasoning on Learning Platforms",
    "authors": [
      "Haiqing Zhu",
      "Tijana Zrnic",
      "Celestine Mendler-Dünner"
    ],
    "abstract": "On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:45:32+00:00",
    "updated": "2025-11-18T18:45:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14745v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14744v1",
    "title": "Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge",
    "authors": [
      "Antonia Ebner",
      "Christoph Bartmann",
      "Sonja Topf",
      "Sohvi Luukkonen",
      "Johannes Schimunek",
      "Günter Klambauer"
    ],
    "abstract": "Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's \"ImageNet moment\" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:43:42+00:00",
    "updated": "2025-11-18T18:43:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14744v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14743v1",
    "title": "Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction",
    "authors": [
      "Christof Naumzik",
      "Abdurahman Maarouf",
      "Stefan Feuerriegel",
      "Markus Weinmann"
    ],
    "abstract": "Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:43:29+00:00",
    "updated": "2025-11-18T18:43:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14743v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14738v1",
    "title": "LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data",
    "authors": [
      "Tzu-Hsuan Chou",
      "Chun-Nan Chou"
    ],
    "abstract": "Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:31:00+00:00",
    "updated": "2025-11-18T18:31:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14738v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14721v1",
    "title": "AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training",
    "authors": [
      "Fu-Ming Guo",
      "Yingfang Fan"
    ],
    "abstract": "Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.   We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:08:20+00:00",
    "updated": "2025-11-18T18:08:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14721v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14715v2",
    "title": "FLARE: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning",
    "authors": [
      "Abolfazl Younesi",
      "Leon Kiss",
      "Zahra Najafabadi Samani",
      "Juan Aznar Poveda",
      "Thomas Fahringer"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T17:57:40+00:00",
    "updated": "2025-11-19T11:55:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14715v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14710v1",
    "title": "Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization",
    "authors": [
      "Zonghao Chen",
      "Atsushi Nitanda",
      "Arthur Gretton",
      "Taiji Suzuki"
    ],
    "abstract": "We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \\emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \\texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T17:51:17+00:00",
    "updated": "2025-11-18T17:51:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14710v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14698v1",
    "title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring",
    "authors": [
      "Sriram Srinivasan",
      "Srinivasan Aruchamy",
      "Siva Ram Krisha Vadali"
    ],
    "abstract": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T17:37:38+00:00",
    "updated": "2025-11-18T17:37:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14698v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14694v1",
    "title": "Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models",
    "authors": [
      "Rui Zhu",
      "Xiaopu Zhou",
      "Haixu Tang",
      "Stephen W. Scherer",
      "Lucila Ohno-Machado"
    ],
    "abstract": "Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental \"grammar\" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG",
      "q-bio.PE"
    ],
    "primary_category": "q-bio.GN",
    "published": "2025-11-18T17:29:39+00:00",
    "updated": "2025-11-18T17:29:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14694v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14682v1",
    "title": "Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk",
    "authors": [
      "Vaskar Chakma",
      "MD Jaheid Hasan Nerab",
      "Abdur Rouf",
      "Abu Sayed",
      "Hossem MD Saim",
      "Md. Nournabi Khan"
    ],
    "abstract": "Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.",
    "categories": [
      "cs.LG",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T17:21:32+00:00",
    "updated": "2025-11-18T17:21:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14682v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14651v1",
    "title": "Derivative of the truncated singular value and eigen decomposition",
    "authors": [
      "Jan Naumann"
    ],
    "abstract": "Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.",
    "categories": [
      "math.NA",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-18T16:45:46+00:00",
    "updated": "2025-11-18T16:45:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14651v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14640v1",
    "title": "Doppler Invariant CNN for Signal Classification",
    "authors": [
      "Avi Bagchi",
      "Dwight Hutchenson"
    ],
    "abstract": "Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "published": "2025-11-18T16:31:13+00:00",
    "updated": "2025-11-18T16:31:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14640v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14632v1",
    "title": "Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting",
    "authors": [
      "Yuchen Luo",
      "Xinyu Li",
      "Liuhua Peng",
      "Mingming Gong"
    ],
    "abstract": "In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \\textbf{channel-independent} (CI) or \\textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \\textbf{A}daptive \\textbf{C}hannel \\textbf{E}nhancer (\\textbf{ACE}) for enriching embedding processes and the \\textbf{A}daptive \\textbf{C}hannel \\textbf{F}orecaster (\\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:24:05+00:00",
    "updated": "2025-11-18T16:24:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14632v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14630v1",
    "title": "Failure to Mix: Large language models struggle to answer according to desired probability distributions",
    "authors": [
      "Ivy Yuqian Yang",
      "David Yu Zhang"
    ],
    "abstract": "Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of \"1\" 49% of the time produces an answer of \"0\" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:22:26+00:00",
    "updated": "2025-11-18T16:22:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14630v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14619v1",
    "title": "Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare",
    "authors": [
      "Marco Locatelli",
      "Arjen Hommersom",
      "Roberto Clemens Cerioli",
      "Daniela Besozzi",
      "Fabio Stella"
    ],
    "abstract": "Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:12:44+00:00",
    "updated": "2025-11-18T16:12:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14617v1",
    "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning",
    "authors": [
      "Ruoyu Qin",
      "Weiran He",
      "Weixiao Huang",
      "Yangkun Zhang",
      "Yikai Zhao",
      "Bo Pang",
      "Xinran Xu",
      "Yingdi Shan",
      "Yongwei Wu",
      "Mingxing Zhang"
    ],
    "abstract": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "published": "2025-11-18T16:12:21+00:00",
    "updated": "2025-11-18T16:12:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14617v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14823v1",
    "title": "Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence",
    "authors": [
      "Akbar Anbar Jafari",
      "Cagri Ozcinar",
      "Gholamreza Anbarjafari"
    ],
    "abstract": "Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:59:44+00:00",
    "updated": "2025-11-18T15:59:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14823v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14606v1",
    "title": "Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models",
    "authors": [
      "Shreya Adrita Banik",
      "Niaz Nafi Rahman",
      "Tahsina Moiukh",
      "Farig Sadeque"
    ],
    "abstract": "Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:58:04+00:00",
    "updated": "2025-11-18T15:58:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14606v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14603v1",
    "title": "A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease",
    "authors": [
      "Yilu Fang",
      "Jordan G. Nestor",
      "Casey N. Ta",
      "Jerard Z. Kneifati-Hayek",
      "Chunhua Weng"
    ],
    "abstract": "Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:53:31+00:00",
    "updated": "2025-11-18T15:53:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14603v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14584v1",
    "title": "ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents",
    "authors": [
      "Ankush Kadu",
      "Ashwanth Krishnan"
    ],
    "abstract": "Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:25:05+00:00",
    "updated": "2025-11-18T15:25:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14584v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14581v1",
    "title": "Online learning of subgrid-scale models for quasi-geostrophic turbulence in planetary interiors",
    "authors": [
      "Hugo Frezat",
      "Thomas Gastine",
      "Alexandre Fournier"
    ],
    "abstract": "The use of machine learning to represent subgrid-scale (SGS) dynamics is now well established in weather forecasting and climate modelling. Recent advances have demonstrated that SGS models trained via ``online'' end-to-end learning -- where the dynamical solver operating on the filtered equations participates in the training -- can outperform traditional physics-based approaches. Most studies, however, have focused on idealised periodic domains, neglecting the mechanical boundaries present e.g. in planetary interiors. To address this issue, we consider two-dimensional quasi-geostrophic turbulent flow in an axisymmetric bounded domain that we model using a pseudo-spectral differentiable solver, thereby enabling online learning. We examine three configurations, varying the geometry (between an exponential container and a spherical shell) and the rotation rate. Flow is driven by a prescribed analytical forcing, allowing for precise control over the energy injection scale and an exact estimate of the power input. We evaluate the accuracy of the online-trained SGS model against the reference direct numerical simulation using integral quantities and spectral diagnostics. In all configurations, we show that an SGS model trained on data spanning only one turnover time remains stable and accurate over integrations at least a hundred times longer than the training period. Moreover, we demonstrate the model's remarkable ability to reproduce slow processes occurring on time scales far exceeding the training duration, such as the inward drift of jets in the spherical shell. These results suggest a promising path towards developing SGS models for planetary and stellar interior dynamics, including dynamo processes.",
    "categories": [
      "physics.flu-dyn",
      "astro-ph.EP",
      "cs.LG"
    ],
    "primary_category": "physics.flu-dyn",
    "published": "2025-11-18T15:21:38+00:00",
    "updated": "2025-11-18T15:21:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14581v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14569v1",
    "title": "Task Addition and Weight Disentanglement in Closed-Vocabulary Models",
    "authors": [
      "Adam Hazimeh",
      "Alessandro Favero",
      "Pascal Frossard"
    ],
    "abstract": "Task arithmetic has recently emerged as a promising method for editing pre-trained \\textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \\textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \\textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:12:21+00:00",
    "updated": "2025-11-18T15:12:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14569v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14559v1",
    "title": "Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models",
    "authors": [
      "Xinzhe Zheng",
      "Shiyu Jiang",
      "Gustavo Seabra",
      "Chenglong Li",
      "Yanjun Li"
    ],
    "abstract": "Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM",
    "published": "2025-11-18T15:01:27+00:00",
    "updated": "2025-11-18T15:01:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14559v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14554v1",
    "title": "ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection",
    "authors": [
      "Mohammad Romani"
    ],
    "abstract": "Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T14:56:34+00:00",
    "updated": "2025-11-18T14:56:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14554v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14545v1",
    "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time",
    "authors": [
      "Haorui Ma",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ],
    "abstract": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T14:49:03+00:00",
    "updated": "2025-11-18T14:49:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14545v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14544v1",
    "title": "Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction",
    "authors": [
      "Jaume Ros",
      "Alessio Arleo",
      "Fernando Paulovich"
    ],
    "abstract": "Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:45:19+00:00",
    "updated": "2025-11-18T14:45:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14544v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14543v1",
    "title": "MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation",
    "authors": [
      "Youran Zhou",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "abstract": "Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.   We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.   Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:44:49+00:00",
    "updated": "2025-11-18T14:44:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14543v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14530v1",
    "title": "DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation",
    "authors": [
      "Xiangchen Yin",
      "Jiahui Yuan",
      "Zhangchi Hu",
      "Wenzhang Sun",
      "Jie Chen",
      "Xiaozhen Qiao",
      "Hao Li",
      "Xiaoyan Sun"
    ],
    "abstract": "Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T14:34:20+00:00",
    "updated": "2025-11-18T14:34:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14530v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14516v2",
    "title": "Full-Atom Peptide Design via Riemannian-Euclidean Bayesian Flow Networks",
    "authors": [
      "Hao Qian",
      "Shikui Tu",
      "Lei Xu"
    ],
    "abstract": "Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:13:23+00:00",
    "updated": "2025-11-19T03:15:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14516v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14510v1",
    "title": "CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design",
    "authors": [
      "Jiawei Yi",
      "Ping Gong",
      "Youhui Bai",
      "Jiaqi Ruan",
      "Shengnan Wang",
      "Pengcheng Wang",
      "Haibo Wang",
      "Weiguang Wang",
      "Xia Zhu",
      "Feng Wu",
      "Cheng Li"
    ],
    "abstract": "The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:03:21+00:00",
    "updated": "2025-11-18T14:03:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14510v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14501v1",
    "title": "Improved Convergence in Parameter-Agnostic Error Feedback through Momentum",
    "authors": [
      "Abdurakhmon Sadiev",
      "Yury Demidovich",
      "Igor Sokolov",
      "Grigory Malinovsky",
      "Sarit Khirirat",
      "Peter Richtárik"
    ],
    "abstract": "Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-18T13:47:08+00:00",
    "updated": "2025-11-18T13:47:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14501v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14488v1",
    "title": "Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching",
    "authors": [
      "Jintao Zhang",
      "Mingyue Cheng",
      "Zirui Liu",
      "Xianquan Wang",
      "Yitong Zhou",
      "Qi Liu"
    ],
    "abstract": "Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \\textbf{PAFM}, a \\textbf{P}erturbation-\\textbf{A}ware \\textbf{F}low \\textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:30:56+00:00",
    "updated": "2025-11-18T13:30:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14488v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14485v1",
    "title": "Notes on Kernel Methods in Machine Learning",
    "authors": [
      "Diego Armando Pérez-Rosero",
      "Danna Valentina Salazar-Dubois",
      "Juan Camilo Lugo-Rojas",
      "Andrés Marino Álvarez-Meza",
      "Germán Castellanos-Dominguez"
    ],
    "abstract": "These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:29:07+00:00",
    "updated": "2025-11-18T13:29:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14485v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14482v1",
    "title": "Gradient-Based Join Ordering",
    "authors": [
      "Tim Schwabe",
      "Maribel Acosta"
    ],
    "abstract": "Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "published": "2025-11-18T13:24:28+00:00",
    "updated": "2025-11-18T13:24:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14482v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14465v1",
    "title": "nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers",
    "authors": [
      "Clément Dumas"
    ],
    "abstract": "Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:05:02+00:00",
    "updated": "2025-11-18T13:05:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14465v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14455v1",
    "title": "Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks",
    "authors": [
      "Nicola Rares Franco",
      "Lorenzo Tedesco"
    ],
    "abstract": "We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\\varphi=\\varphi(x,u)$ such that $\\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:59:20+00:00",
    "updated": "2025-11-18T12:59:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14455v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14452v1",
    "title": "Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters",
    "authors": [
      "Emanuele Palumbo",
      "Sorawit Saengkyongam",
      "Maria R. Cervera",
      "Jens Behrmann",
      "Andrew C. Miller",
      "Guillermo Sapiro",
      "Christina Heinze-Deml",
      "Antoine Wehenkel"
    ],
    "abstract": "Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:56:20+00:00",
    "updated": "2025-11-18T12:56:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14452v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14445v1",
    "title": "Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning",
    "authors": [
      "Trishala Jayesh Ahalpara"
    ],
    "abstract": "We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T12:43:04+00:00",
    "updated": "2025-11-18T12:43:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14445v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14441v1",
    "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models",
    "authors": [
      "Daniel Klippert",
      "Alexander Marx"
    ],
    "abstract": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T12:40:41+00:00",
    "updated": "2025-11-18T12:40:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14441v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14427v1",
    "title": "Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning",
    "authors": [
      "Rickmer Krohn",
      "Vignesh Prasad",
      "Gabriele Tiboni",
      "Georgia Chalvatzaki"
    ],
    "abstract": "Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-18T12:32:23+00:00",
    "updated": "2025-11-18T12:32:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14427v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14426v1",
    "title": "MiAD: Mirage Atom Diffusion for De Novo Crystal Generation",
    "authors": [
      "Andrey Okhotin",
      "Maksim Nakhodnov",
      "Nikita Kazeev",
      "Andrey E Ustyuzhanin",
      "Dmitry Vetrov"
    ],
    "abstract": "In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \\href{https://github.com/andrey-okhotin/miad.git}{\\texttt{github.com/andrey-okhotin/miad}}.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:29:19+00:00",
    "updated": "2025-11-18T12:29:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14426v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14422v1",
    "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
    "authors": [
      "Zhengchunmin Dai",
      "Jiaxiong Tang",
      "Peng Sun",
      "Honglong Chen",
      "Liantao Wu"
    ],
    "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.   To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T12:27:43+00:00",
    "updated": "2025-11-18T12:27:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14422v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14419v1",
    "title": "FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis",
    "authors": [
      "Xiaowei Xu",
      "Justin Sonneck",
      "Hongxiao Wang",
      "Roman Burkard",
      "Hendrik Wohrle",
      "Anton Grabmasier",
      "Matthias Gunzer",
      "Jianxu Chen"
    ],
    "abstract": "Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:25:18+00:00",
    "updated": "2025-11-18T12:25:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14419v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14416v1",
    "title": "Toward Robust and Harmonious Adaptation for Cross-modal Retrieval",
    "authors": [
      "Haobin Li",
      "Mouxing Yang",
      "Xi Peng"
    ],
    "abstract": "Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:21:23+00:00",
    "updated": "2025-11-18T12:21:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14416v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14406v1",
    "title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation",
    "authors": [
      "Bastien Vuillod",
      "Pierre-Alain Moellic",
      "Jean-Max Dutertre"
    ],
    "abstract": "Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:13:59+00:00",
    "updated": "2025-11-18T12:13:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14406v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14368v1",
    "title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model",
    "authors": [
      "Rishi Gupta",
      "Mukilan Karuppasamy",
      "Shyam Marjit",
      "Aditay Tripathi",
      "Anirban Chakraborty"
    ],
    "abstract": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T11:18:08+00:00",
    "updated": "2025-11-18T11:18:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14368v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14348v1",
    "title": "Enforcing hidden physics in physics-informed neural networks",
    "authors": [
      "Nanxi Chen",
      "Sifan Wang",
      "Rujin Ma",
      "Airong Chen",
      "Chuanjie Cui"
    ],
    "abstract": "Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:52:37+00:00",
    "updated": "2025-11-18T10:52:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14348v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14334v2",
    "title": "When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling",
    "authors": [
      "Alessio Pellegrino",
      "Jacopo Mauro"
    ],
    "abstract": "One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T10:40:32+00:00",
    "updated": "2025-11-19T10:26:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14334v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14320v1",
    "title": "Learning with Statistical Equality Constraints",
    "authors": [
      "Aneesh Barthakur",
      "Luiz F. O. Chamon"
    ],
    "abstract": "As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:24:13+00:00",
    "updated": "2025-11-18T10:24:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14320v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14317v1",
    "title": "Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect",
    "authors": [
      "Yuwen Zhang",
      "Viet Tran",
      "Paul Weng"
    ],
    "abstract": "In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:21:07+00:00",
    "updated": "2025-11-18T10:21:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14317v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14312v1",
    "title": "H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata",
    "authors": [
      "Chenyang Xu",
      "Siming Li",
      "Hao Wang"
    ],
    "abstract": "Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:16:22+00:00",
    "updated": "2025-11-18T10:16:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14312v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14307v1",
    "title": "Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions",
    "authors": [
      "Marcel Gibier",
      "Nolwenn Celton",
      "Raphaël Duroselle",
      "Pierre Serrano",
      "Olivier Boeffard",
      "Jean-François Bonastre"
    ],
    "abstract": "In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T10:05:36+00:00",
    "updated": "2025-11-18T10:05:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14307v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14301v1",
    "title": "Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion",
    "authors": [
      "Eric Xue",
      "Ruiyi Zhang",
      "Zijun Zhang",
      "Pengtao Xie"
    ],
    "abstract": "Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T09:56:16+00:00",
    "updated": "2025-11-18T09:56:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14301v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14295v1",
    "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
    "authors": [
      "Mohammad Zbib",
      "Hasan Abed Al Kader Hammoud",
      "Sina Mukalled",
      "Nadine Rizk",
      "Fatima Karnib",
      "Issam Lakkis",
      "Ammar Mohanna",
      "Bernard Ghanem"
    ],
    "abstract": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T09:47:01+00:00",
    "updated": "2025-11-18T09:47:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14295v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14293v1",
    "title": "Segmentwise Pruning in Audio-Language Models",
    "authors": [
      "Marcel Gibier",
      "Raphaël Duroselle",
      "Pierre Serrano",
      "Olivier Boeffard",
      "Jean-François Bonastre"
    ],
    "abstract": "Recent audio-language models have shown impressive performance across a wide range of audio tasks and are increasingly capable of handling long audio inputs. However, the computing costs in these models heavily depend on sequence length, which can become very large given the nature of audio data. In the vision-language domain, token pruning methods have proven effective in reducing token counts while preserving strong performance on standard benchmarks. In this work, we investigate the relevance and effectiveness of such token selection strategies in the context of audio-language models. We also improve them by proposing a lightweight strategy that takes the time dimension into account. While retaining only a quarter of the initial tokens, our approach results in a relative maximum decrease of 2% in CIDEr on Clotho v2 and a relative maximum decrease of 4% in accuracy on MMAU.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T09:43:27+00:00",
    "updated": "2025-11-18T09:43:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14293v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14283v1",
    "title": "NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction",
    "authors": [
      "Zi-Chen Xi",
      "Jiahui Huang",
      "Hao-Xiang Chen",
      "Francis Williams",
      "Qun-Ce Xu",
      "Tai-Jiang Mu",
      "Shi-Min Hu"
    ],
    "abstract": "We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T09:20:15+00:00",
    "updated": "2025-11-18T09:20:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14283v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14282v1",
    "title": "Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning",
    "authors": [
      "Vincent-Daniel Yun",
      "Junhyuk Jo",
      "Sunwoo Lee"
    ],
    "abstract": "Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T09:18:26+00:00",
    "updated": "2025-11-18T09:18:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14282v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14276v1",
    "title": "Comparing Task-Agnostic Embedding Models for Tabular Data",
    "authors": [
      "Frederik Hoppe",
      "Lars Kleinemeier",
      "Astrid Franz",
      "Udo Göbel"
    ],
    "abstract": "Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T09:10:40+00:00",
    "updated": "2025-11-18T09:10:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14276v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14268v1",
    "title": "Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks",
    "authors": [
      "Zhenchuan Ma",
      "Qizhi Teng",
      "Pengcheng Yan",
      "Lindong Li",
      "Kirill M. Gerke",
      "Marina V. Karsanina",
      "Xiaohai He"
    ],
    "abstract": "Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.",
    "categories": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph",
    "published": "2025-11-18T09:02:09+00:00",
    "updated": "2025-11-18T09:02:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14268v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14265v1",
    "title": "Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention",
    "authors": [
      "Rui Zhang",
      "Chao Li",
      "Kezhong Liu",
      "Chen Wang",
      "Bolong Zheng",
      "Hongbo Jiang"
    ],
    "abstract": "Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:56:30+00:00",
    "updated": "2025-11-18T08:56:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14265v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14263v1",
    "title": "Algebraformer: A Neural Approach to Linear Systems",
    "authors": [
      "Pietro Sittoni",
      "Francesco Tudisco"
    ],
    "abstract": "Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:53:22+00:00",
    "updated": "2025-11-18T08:53:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14263v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14262v1",
    "title": "Object-Centric World Models for Causality-Aware Reinforcement Learning",
    "authors": [
      "Yosuke Nishimoto",
      "Takashi Matsubara"
    ],
    "abstract": "World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \\emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:53:09+00:00",
    "updated": "2025-11-18T08:53:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14262v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14250v1",
    "title": "Count The Notes: Histogram-Based Supervision for Automatic Music Transcription",
    "authors": [
      "Jonathan Yaffe",
      "Ben Maman",
      "Meinard Müller",
      "Amit H. Bermano"
    ],
    "abstract": "Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T08:40:05+00:00",
    "updated": "2025-11-18T08:40:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14250v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14238v1",
    "title": "Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization",
    "authors": [
      "Yan Huang",
      "Yongyi Su",
      "Xin Lin",
      "Le Zhang",
      "Xun Xu"
    ],
    "abstract": "The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T08:16:16+00:00",
    "updated": "2025-11-18T08:16:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14238v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14229v1",
    "title": "EBind: a practical approach to space binding",
    "authors": [
      "Jim Broadbent",
      "Felix Cohen",
      "Frederik Hvilshøj",
      "Eric Landau",
      "Eren Sasoglu"
    ],
    "abstract": "We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:03:30+00:00",
    "updated": "2025-11-18T08:03:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14229v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14227v1",
    "title": "DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home",
    "authors": [
      "Yuxiang Wang",
      "Siwen Wang",
      "Haowei Han",
      "Ao Wang",
      "Boya Liu",
      "Yong Zhao",
      "Chengbo Wu",
      "Bin Zhu",
      "Bin Qin",
      "Xiaokai Zhou",
      "Xiao Yan",
      "Jiawei Jiang",
      "Bo Du"
    ],
    "abstract": "Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T08:01:55+00:00",
    "updated": "2025-11-18T08:01:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14227v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14220v1",
    "title": "Parallelizing Tree Search with Twice Sequential Monte Carlo",
    "authors": [
      "Yaniv Oren",
      "Joery A. de Vries",
      "Pascal R. van der Vaart",
      "Matthijs T. J. Spaan",
      "Wendelin Böhmer"
    ],
    "abstract": "Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T07:54:29+00:00",
    "updated": "2025-11-18T07:54:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14220v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14218v1",
    "title": "Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts",
    "authors": [
      "Xinlei Xiong",
      "Wenbo Hu",
      "Shuxun Zhou",
      "Kaifeng Bi",
      "Lingxi Xie",
      "Ying Liu",
      "Richang Hong",
      "Qi Tian"
    ],
    "abstract": "Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T07:49:52+00:00",
    "updated": "2025-11-18T07:49:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14218v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14214v2",
    "title": "Do Large Language Models (LLMs) Understand Chronology?",
    "authors": [
      "Pattaraphon Kenny Wongchamcharoen",
      "Paul Glasserman"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T07:45:12+00:00",
    "updated": "2025-11-19T17:19:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14214v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14210v1",
    "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
    "authors": [
      "N Dinesh Reddy",
      "Sudeep Pillai"
    ],
    "abstract": "We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T07:41:02+00:00",
    "updated": "2025-11-18T07:41:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14210v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14206v1",
    "title": "Causal Discovery on Higher-Order Interactions",
    "authors": [
      "Alessio Zanga",
      "Marco Scutari",
      "Fabio Stella"
    ],
    "abstract": "Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T07:35:00+00:00",
    "updated": "2025-11-18T07:35:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14206v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14195v1",
    "title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator",
    "authors": [
      "Zheyu Lin",
      "Jirui Yang",
      "Hengqi Guo",
      "Yubing Bao",
      "Yao Guan"
    ],
    "abstract": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T07:03:58+00:00",
    "updated": "2025-11-18T07:03:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14195v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14168v1",
    "title": "Certified Signed Graph Unlearning",
    "authors": [
      "Junpeng Zhao",
      "Lin Li",
      "Kaixi Hu",
      "Kaize Shi",
      "Jingling Yuan"
    ],
    "abstract": "Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T06:09:17+00:00",
    "updated": "2025-11-18T06:09:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14168v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14153v1",
    "title": "A Comprehensive Study of Implicit and Explicit Biases in Large Language Models",
    "authors": [
      "Fatima Kazi",
      "Alex Young",
      "Yash Inani",
      "Setareh Rafatirad"
    ],
    "abstract": "Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.",
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T05:27:17+00:00",
    "updated": "2025-11-18T05:27:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14153v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14148v1",
    "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
    "authors": [
      "Yuhua Jiang",
      "Shuang Cheng",
      "Yan Ding",
      "Feifei Gao",
      "Biqing Qi"
    ],
    "abstract": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-18T05:21:11+00:00",
    "updated": "2025-11-18T05:21:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14148v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14147v1",
    "title": "Imaging with super-resolution in changing random media",
    "authors": [
      "Alexander Christie",
      "Matan Leibovich",
      "Miguel Moscoso",
      "Alexei Novikov",
      "George Papanicolaou",
      "Chrysoula Tsogka"
    ],
    "abstract": "We develop an imaging algorithm that exploits strong scattering to achieve super-resolution in changing random media. The method processes large and diverse array datasets using sparse dictionary learning, clustering, and multidimensional scaling. Starting from random initializations, the algorithm reliably extracts the unknown medium properties necessary for accurate imaging using back-propagation, $\\ell_2$ or $\\ell_1$ methods. Remarkably, scattering enhances resolution beyond homogeneous medium limits. When abundant data are available, the algorithm allows the realization of super-resolution in imaging.",
    "categories": [
      "physics.optics",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "published": "2025-11-18T05:18:00+00:00",
    "updated": "2025-11-18T05:18:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14147v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14146v1",
    "title": "SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation",
    "authors": [
      "Renjie Chen",
      "Viet Anh Nguyen",
      "Huifu Xu"
    ],
    "abstract": "We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T05:13:49+00:00",
    "updated": "2025-11-18T05:13:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14146v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14142v1",
    "title": "From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling",
    "authors": [
      "Omkar Mahesh Kashyap",
      "Padegal Amit",
      "Madhav Kashyap",
      "Ashwini M Joshi",
      "Shylaja SS"
    ],
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T05:01:25+00:00",
    "updated": "2025-11-18T05:01:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14142v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14135v1",
    "title": "Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation",
    "authors": [
      "Promise Ekpo",
      "Saesha Agarwal",
      "Felix Grimm",
      "Lekan Molu",
      "Angelique Taylor"
    ],
    "abstract": "Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\\ 0.33 JFI, $p < 0.01$) while maintaining 86\\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T04:48:50+00:00",
    "updated": "2025-11-18T04:48:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14135v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14133v1",
    "title": "Synthetic Survival Control: Extending Synthetic Controls for \"When-If\" Decision",
    "authors": [
      "Jessy Xinyi Han",
      "Devavrat Shah"
    ],
    "abstract": "Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such \"when-if\" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.",
    "categories": [
      "cs.LG",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T04:36:20+00:00",
    "updated": "2025-11-18T04:36:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14133v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14129v1",
    "title": "MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification",
    "authors": [
      "Xiang Luo",
      "Chang Liu",
      "Gang Xiong",
      "Chen Yang",
      "Gaopeng Gou",
      "Yaochen Ren",
      "Zhen Li"
    ],
    "abstract": "Fine-grained identification of IDS-flagged suspicious traffic is crucial in cybersecurity. In practice, cyber threats evolve continuously, making the discovery of novel malicious traffic a critical necessity as well as the identification of known classes. Recent studies have advanced this goal with deep models, but they often rely on task-specific architectures that limit transferability and require per-dataset tuning. In this paper we introduce MalRAG, the first LLM driven retrieval-augmented framework for open-set malicious traffic identification. MalRAG freezes the LLM and operates via comprehensive traffic knowledge construction, adaptive retrieval, and prompt engineering. Concretely, we construct a multi-view traffic database by mining prior malicious traffic from content, structural, and temporal perspectives. Furthermore, we introduce a Coverage-Enhanced Retrieval Algorithm that queries across these views to assemble the most probable candidates, thereby improving the inclusion of correct evidence. We then employ Traffic-Aware Adaptive Pruning to select a variable subset of these candidates based on traffic-aware similarity scores, suppressing incorrect matches and yielding reliable retrieved evidence. Moreover, we develop a suite of guidance prompts where task instruction, evidence referencing, and decision guidance are integrated with the retrieved evidence to improve LLM performance. Across diverse real-world datasets and settings, MalRAG delivers state-of-the-art results in both fine-grained identification of known classes and novel malicious traffic discovery. Ablation and deep-dive analyses further show that MalRAG effective leverages LLM capabilities yet achieves open-set malicious traffic identification without relying on a specific LLM.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T04:25:16+00:00",
    "updated": "2025-11-18T04:25:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14129v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14993v2",
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "authors": [
      "Vladimir Arkhipkin",
      "Vladimir Korviakov",
      "Nikolai Gerasimenko",
      "Denis Parkhomenko",
      "Viacheslav Vasilev",
      "Alexey Letunovskiy",
      "Nikolai Vaulin",
      "Maria Kovaleva",
      "Ivan Kirillov",
      "Lev Novitskiy",
      "Denis Koposov",
      "Nikita Kiselev",
      "Alexander Varlamov",
      "Dmitrii Mikhailov",
      "Vladimir Polovnikov",
      "Andrey Shutkin",
      "Julia Agafonova",
      "Ilya Vasiliev",
      "Anastasiia Kargapoltseva",
      "Anna Dmitrienko",
      "Anastasia Maltseva",
      "Anna Averchenkova",
      "Olga Kim",
      "Tatiana Nikulina",
      "Denis Dimitrov"
    ],
    "abstract": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-19T00:23:22+00:00",
    "updated": "2025-11-20T13:09:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14993v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14981v1",
    "title": "Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation",
    "authors": [
      "Nicholas Cooper",
      "Lijun Chen",
      "Sailesh Dwivedy",
      "Danna Gurari"
    ],
    "abstract": "Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T23:50:31+00:00",
    "updated": "2025-11-18T23:50:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14981v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14980v1",
    "title": "Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework",
    "authors": [
      "Ahmet Umur Özsoy"
    ],
    "abstract": "Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.",
    "categories": [
      "q-fin.MF",
      "cs.LG"
    ],
    "primary_category": "q-fin.MF",
    "published": "2025-11-18T23:47:49+00:00",
    "updated": "2025-11-18T23:47:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14980v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14969v1",
    "title": "Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion",
    "authors": [
      "Zanxu Wang",
      "Homayoon Beigi"
    ],
    "abstract": "This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "published": "2025-11-18T23:24:27+00:00",
    "updated": "2025-11-18T23:24:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14969v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14967v1",
    "title": "MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation",
    "authors": [
      "Basel Shbita",
      "Farhan Ahmed",
      "Chad DeLuca"
    ],
    "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "published": "2025-11-18T23:14:44+00:00",
    "updated": "2025-11-18T23:14:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14967v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14962v1",
    "title": "Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks",
    "authors": [
      "Haizhou Wen",
      "He Li",
      "Zhen Li"
    ],
    "abstract": "Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.",
    "categories": [
      "physics.comp-ph",
      "cs.LG",
      "eess.IV",
      "physics.bio-ph",
      "q-bio.QM"
    ],
    "primary_category": "physics.comp-ph",
    "published": "2025-11-18T23:04:13+00:00",
    "updated": "2025-11-18T23:04:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14962v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14961v1",
    "title": "Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference",
    "authors": [
      "Artur A. Oliveira",
      "Mateus Espadoto",
      "Roberto M. Cesar",
      "Roberto Hirata"
    ],
    "abstract": "We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T23:02:59+00:00",
    "updated": "2025-11-18T23:02:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14961v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14953v1",
    "title": "Compiling to recurrent neurons",
    "authors": [
      "Joey Velez-Ginorio",
      "Nada Amin",
      "Konrad Kording",
      "Steve Zdancewic"
    ],
    "abstract": "Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\\textsf{Cajal}\\scriptstyle(\\mathbb{\\multimap}, \\mathbb{2}, \\mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.",
    "categories": [
      "cs.PL",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "published": "2025-11-18T22:26:27+00:00",
    "updated": "2025-11-18T22:26:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14953v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14952v1",
    "title": "Artificial intelligence approaches for energy-efficient laser cutting machines",
    "authors": [
      "Mohamed Abdallah Salem",
      "Hamdy Ahmed Ashour",
      "Ahmed Elshenawy"
    ],
    "abstract": "This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T22:25:58+00:00",
    "updated": "2025-11-18T22:25:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14952v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14939v1",
    "title": "Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report",
    "authors": [
      "Daniel Oliveira de Brito",
      "Letícia Gabriella de Souza",
      "Marcelo Matheus Gauy",
      "Marcelo Finger",
      "Arnaldo Candido Junior"
    ],
    "abstract": "This technical report investigates the performance of pre-trained audio models on COVID-19 detection tasks using established benchmark datasets. We fine-tuned Audio-MAE and three PANN architectures (CNN6, CNN10, CNN14) on the Coswara and COUGHVID datasets, evaluating both intra-dataset and cross-dataset generalization. We implemented a strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). Our experiments demonstrate that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage - a confounding factor that inflate performance metrics. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These findings highlight fundamental challenges in developing generalizable audio-based COVID-19 detection systems and underscore the importance of rigorous demographic controls for clinically robust model evaluation.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T21:54:20+00:00",
    "updated": "2025-11-18T21:54:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14939v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14936v1",
    "title": "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding",
    "authors": [
      "Mathieu Dufour",
      "Andrew Duncan"
    ],
    "abstract": "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:51:04+00:00",
    "updated": "2025-11-18T21:51:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14936v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14922v1",
    "title": "Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis",
    "authors": [
      "Pranay Kumar Peddi",
      "Dhrubajyoti Ghosh"
    ],
    "abstract": "Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:22:24+00:00",
    "updated": "2025-11-18T21:22:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14922v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14920v1",
    "title": "Structured Contrastive Learning for Interpretable Latent Representations",
    "authors": [
      "Zhengyang Shen",
      "Hua Tu",
      "Mayue Shi"
    ],
    "abstract": "Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as \"laissez-faire\" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T21:18:20+00:00",
    "updated": "2025-11-18T21:18:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14920v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14908v1",
    "title": "On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs",
    "authors": [
      "Gefté Almeida",
      "Marcio Pohlmann",
      "Alex Severo",
      "Diego Kreutz",
      "Tiago Heinrich",
      "Lourenço Pereira"
    ],
    "abstract": "In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T20:56:49+00:00",
    "updated": "2025-11-18T20:56:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14908v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14903v1",
    "title": "It's LIT! Reliability-Optimized LLMs with Inspectable Tools",
    "authors": [
      "Ruixin Zhang",
      "Jon Donnelly",
      "Zhicheng Guo",
      "Ghazal Khalighinejad",
      "Haiyang Huang",
      "Alina Jade Barnett",
      "Cynthia Rudin"
    ],
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:41:58+00:00",
    "updated": "2025-11-18T20:41:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14903v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14897v1",
    "title": "HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation",
    "authors": [
      "Pranav Indrakanti",
      "Ivor Simpson"
    ],
    "abstract": "We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T20:36:24+00:00",
    "updated": "2025-11-18T20:36:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14897v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14889v1",
    "title": "Bringing Federated Learning to Space",
    "authors": [
      "Grace Kim",
      "Filip Svoboda",
      "Nicholas Lane"
    ],
    "abstract": "As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive \"space-ification\" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:16:07+00:00",
    "updated": "2025-11-18T20:16:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14889v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15743v1",
    "title": "Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models",
    "authors": [
      "Linnea M. Wolniewicz",
      "Halil S. Kelebek",
      "Simone Mestici",
      "Michael D. Vergalla",
      "Giacomo Acciarini",
      "Bala Poduval",
      "Olga Verkhoglyadova",
      "Madhulika Guhathakurta",
      "Thomas E. Berger",
      "Atılım Güneş Baydin",
      "Frank Soboczenski"
    ],
    "abstract": "Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.",
    "categories": [
      "cs.LG",
      "astro-ph.EP",
      "astro-ph.IM"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:13:25+00:00",
    "updated": "2025-11-18T20:13:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15743v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14887v1",
    "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone",
    "authors": [
      "Nathan M. Roberts",
      "Xiaosong Du"
    ],
    "abstract": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T20:11:54+00:00",
    "updated": "2025-11-18T20:11:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14887v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14882v1",
    "title": "Exact Learning of Weighted Graphs Using Composite Queries",
    "authors": [
      "Michael T. Goodrich",
      "Songyu Liu",
      "Ioannis Panageas"
    ],
    "abstract": "In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.",
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "published": "2025-11-18T20:02:02+00:00",
    "updated": "2025-11-18T20:02:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14882v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14876v1",
    "title": "Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard",
    "authors": [
      "Henry Wong",
      "Clement Fung",
      "Weiran Lin",
      "Karen Li",
      "Stanley Chen",
      "Lujo Bauer"
    ],
    "abstract": "To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.   To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.   We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T19:49:46+00:00",
    "updated": "2025-11-18T19:49:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14876v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14868v1",
    "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings",
    "authors": [
      "Xueying Ding",
      "Xingyue Huang",
      "Mingxuan Ju",
      "Liam Collins",
      "Yozen Liu",
      "Leman Akoglu",
      "Neil Shah",
      "Tong Zhao"
    ],
    "abstract": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T19:37:40+00:00",
    "updated": "2025-11-18T19:37:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14868v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14865v1",
    "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
    "authors": [
      "Dwipam Katariya",
      "Snehita Varma",
      "Akshat Shreemali",
      "Benjamin Wu",
      "Kalanand Mishra",
      "Pranab Mohanty"
    ],
    "abstract": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T19:32:38+00:00",
    "updated": "2025-11-18T19:32:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14865v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14846v1",
    "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization",
    "authors": [
      "Yifeng Ding",
      "Hung Le",
      "Songyang Han",
      "Kangrui Ruan",
      "Zhenghui Jin",
      "Varun Kumar",
      "Zijian Wang",
      "Anoop Deoras"
    ],
    "abstract": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T19:01:16+00:00",
    "updated": "2025-11-18T19:01:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14846v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14832v1",
    "title": "How to pick the best anomaly detector?",
    "authors": [
      "Marie Hein",
      "Gregor Kasieczka",
      "Michael Krämer",
      "Louis Moureaux",
      "Alexander Mück",
      "David Shih"
    ],
    "abstract": "Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.",
    "categories": [
      "hep-ph",
      "cs.LG",
      "hep-ex",
      "physics.data-an"
    ],
    "primary_category": "hep-ph",
    "published": "2025-11-18T19:00:01+00:00",
    "updated": "2025-11-18T19:00:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14832v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14761v1",
    "title": "ARC Is a Vision Problem!",
    "authors": [
      "Keya Hu",
      "Ali Cy",
      "Linlu Qiu",
      "Xiaoman Delores Ding",
      "Runqian Wang",
      "Yeyin Eva Zhu",
      "Jacob Andreas",
      "Kaiming He"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T18:59:49+00:00",
    "updated": "2025-11-18T18:59:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14761v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14759v2",
    "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
    "authors": [
      "Physical Intelligence",
      "Ali Amin",
      "Raichelle Aniceto",
      "Ashwin Balakrishna",
      "Kevin Black",
      "Ken Conley",
      "Grace Connors",
      "James Darpinian",
      "Karan Dhabalia",
      "Jared DiCarlo",
      "Danny Driess",
      "Michael Equi",
      "Adnan Esmail",
      "Yunhao Fang",
      "Chelsea Finn",
      "Catherine Glossop",
      "Thomas Godden",
      "Ivan Goryachev",
      "Lachy Groom",
      "Hunter Hancock",
      "Karol Hausman",
      "Gashon Hussein",
      "Brian Ichter",
      "Szymon Jakubczak",
      "Rowan Jen",
      "Tim Jones",
      "Ben Katz",
      "Liyiming Ke",
      "Chandra Kuchi",
      "Marinda Lamb",
      "Devin LeBlanc",
      "Sergey Levine",
      "Adrian Li-Bell",
      "Yao Lu",
      "Vishnu Mano",
      "Mohith Mothukuri",
      "Suraj Nair",
      "Karl Pertsch",
      "Allen Z. Ren",
      "Charvi Sharma",
      "Lucy Xiaoyang Shi",
      "Laura Smith",
      "Jost Tobias Springenberg",
      "Kyle Stachowicz",
      "Will Stoeckle",
      "Alex Swerdlow",
      "James Tanner",
      "Marcel Torne",
      "Quan Vuong",
      "Anna Walling",
      "Haohuan Wang",
      "Blake Williams",
      "Sukwon Yoo",
      "Lili Yu",
      "Ury Zhilinsky",
      "Zhiyuan Zhou"
    ],
    "abstract": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:58:55+00:00",
    "updated": "2025-11-19T04:34:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14759v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14755v1",
    "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis",
    "authors": [
      "Albert Lin",
      "Alessandro Pinto",
      "Somil Bansal"
    ],
    "abstract": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.",
    "categories": [
      "cs.RO",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-18T18:55:20+00:00",
    "updated": "2025-11-18T18:55:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14755v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14753v1",
    "title": "SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction",
    "authors": [
      "Junfeng Wu",
      "Hadjer Benmeziane",
      "Kaoutar El Maghraoui",
      "Liu Liu",
      "Yinan Wang"
    ],
    "abstract": "Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:53:37+00:00",
    "updated": "2025-11-18T18:53:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14753v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14827v1",
    "title": "Implicit Bias of the JKO Scheme",
    "authors": [
      "Peter Halmos",
      "Boris Hanin"
    ],
    "abstract": "Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \\emph{modified} energy \\[ J^η(ρ) = J(ρ) - \\fracη{4}\\int_M \\Big\\lVert \\nabla_g \\frac{δJ}{δρ} (ρ) \\Big\\rVert_{2}^{2} \\,ρ(dx), \\] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \\textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \\emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T18:48:37+00:00",
    "updated": "2025-11-18T18:48:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14827v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14745v1",
    "title": "Look-Ahead Reasoning on Learning Platforms",
    "authors": [
      "Haiqing Zhu",
      "Tijana Zrnic",
      "Celestine Mendler-Dünner"
    ],
    "abstract": "On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:45:32+00:00",
    "updated": "2025-11-18T18:45:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14745v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14744v1",
    "title": "Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge",
    "authors": [
      "Antonia Ebner",
      "Christoph Bartmann",
      "Sonja Topf",
      "Sohvi Luukkonen",
      "Johannes Schimunek",
      "Günter Klambauer"
    ],
    "abstract": "Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's \"ImageNet moment\" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:43:42+00:00",
    "updated": "2025-11-18T18:43:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14744v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14743v1",
    "title": "Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction",
    "authors": [
      "Christof Naumzik",
      "Abdurahman Maarouf",
      "Stefan Feuerriegel",
      "Markus Weinmann"
    ],
    "abstract": "Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:43:29+00:00",
    "updated": "2025-11-18T18:43:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14743v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14738v1",
    "title": "LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data",
    "authors": [
      "Tzu-Hsuan Chou",
      "Chun-Nan Chou"
    ],
    "abstract": "Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:31:00+00:00",
    "updated": "2025-11-18T18:31:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14738v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14721v1",
    "title": "AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training",
    "authors": [
      "Fu-Ming Guo",
      "Yingfang Fan"
    ],
    "abstract": "Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.   We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:08:20+00:00",
    "updated": "2025-11-18T18:08:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14721v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14715v2",
    "title": "FLARE: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning",
    "authors": [
      "Abolfazl Younesi",
      "Leon Kiss",
      "Zahra Najafabadi Samani",
      "Juan Aznar Poveda",
      "Thomas Fahringer"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T17:57:40+00:00",
    "updated": "2025-11-19T11:55:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14715v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14710v1",
    "title": "Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization",
    "authors": [
      "Zonghao Chen",
      "Atsushi Nitanda",
      "Arthur Gretton",
      "Taiji Suzuki"
    ],
    "abstract": "We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \\emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \\texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T17:51:17+00:00",
    "updated": "2025-11-18T17:51:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14710v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14698v1",
    "title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring",
    "authors": [
      "Sriram Srinivasan",
      "Srinivasan Aruchamy",
      "Siva Ram Krisha Vadali"
    ],
    "abstract": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T17:37:38+00:00",
    "updated": "2025-11-18T17:37:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14698v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14694v1",
    "title": "Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models",
    "authors": [
      "Rui Zhu",
      "Xiaopu Zhou",
      "Haixu Tang",
      "Stephen W. Scherer",
      "Lucila Ohno-Machado"
    ],
    "abstract": "Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental \"grammar\" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG",
      "q-bio.PE"
    ],
    "primary_category": "q-bio.GN",
    "published": "2025-11-18T17:29:39+00:00",
    "updated": "2025-11-18T17:29:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14694v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14682v1",
    "title": "Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk",
    "authors": [
      "Vaskar Chakma",
      "MD Jaheid Hasan Nerab",
      "Abdur Rouf",
      "Abu Sayed",
      "Hossem MD Saim",
      "Md. Nournabi Khan"
    ],
    "abstract": "Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.",
    "categories": [
      "cs.LG",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T17:21:32+00:00",
    "updated": "2025-11-18T17:21:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14682v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14651v1",
    "title": "Derivative of the truncated singular value and eigen decomposition",
    "authors": [
      "Jan Naumann"
    ],
    "abstract": "Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.",
    "categories": [
      "math.NA",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-18T16:45:46+00:00",
    "updated": "2025-11-18T16:45:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14651v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14640v1",
    "title": "Doppler Invariant CNN for Signal Classification",
    "authors": [
      "Avi Bagchi",
      "Dwight Hutchenson"
    ],
    "abstract": "Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "published": "2025-11-18T16:31:13+00:00",
    "updated": "2025-11-18T16:31:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14640v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14632v1",
    "title": "Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting",
    "authors": [
      "Yuchen Luo",
      "Xinyu Li",
      "Liuhua Peng",
      "Mingming Gong"
    ],
    "abstract": "In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \\textbf{channel-independent} (CI) or \\textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \\textbf{A}daptive \\textbf{C}hannel \\textbf{E}nhancer (\\textbf{ACE}) for enriching embedding processes and the \\textbf{A}daptive \\textbf{C}hannel \\textbf{F}orecaster (\\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:24:05+00:00",
    "updated": "2025-11-18T16:24:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14632v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14630v1",
    "title": "Failure to Mix: Large language models struggle to answer according to desired probability distributions",
    "authors": [
      "Ivy Yuqian Yang",
      "David Yu Zhang"
    ],
    "abstract": "Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of \"1\" 49% of the time produces an answer of \"0\" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:22:26+00:00",
    "updated": "2025-11-18T16:22:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14630v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14619v1",
    "title": "Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare",
    "authors": [
      "Marco Locatelli",
      "Arjen Hommersom",
      "Roberto Clemens Cerioli",
      "Daniela Besozzi",
      "Fabio Stella"
    ],
    "abstract": "Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T16:12:44+00:00",
    "updated": "2025-11-18T16:12:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14617v1",
    "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning",
    "authors": [
      "Ruoyu Qin",
      "Weiran He",
      "Weixiao Huang",
      "Yangkun Zhang",
      "Yikai Zhao",
      "Bo Pang",
      "Xinran Xu",
      "Yingdi Shan",
      "Yongwei Wu",
      "Mingxing Zhang"
    ],
    "abstract": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "published": "2025-11-18T16:12:21+00:00",
    "updated": "2025-11-18T16:12:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14617v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14823v1",
    "title": "Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence",
    "authors": [
      "Akbar Anbar Jafari",
      "Cagri Ozcinar",
      "Gholamreza Anbarjafari"
    ],
    "abstract": "Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:59:44+00:00",
    "updated": "2025-11-18T15:59:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14823v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14606v1",
    "title": "Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models",
    "authors": [
      "Shreya Adrita Banik",
      "Niaz Nafi Rahman",
      "Tahsina Moiukh",
      "Farig Sadeque"
    ],
    "abstract": "Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:58:04+00:00",
    "updated": "2025-11-18T15:58:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14606v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14603v1",
    "title": "A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease",
    "authors": [
      "Yilu Fang",
      "Jordan G. Nestor",
      "Casey N. Ta",
      "Jerard Z. Kneifati-Hayek",
      "Chunhua Weng"
    ],
    "abstract": "Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T15:53:31+00:00",
    "updated": "2025-11-18T15:53:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14603v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15741v1",
    "title": "Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer",
    "authors": [
      "Hyo-Jeong Jang"
    ],
    "abstract": "Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T15:26:42+00:00",
    "updated": "2025-11-18T15:26:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15741v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14584v1",
    "title": "ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents",
    "authors": [
      "Ankush Kadu",
      "Ashwanth Krishnan"
    ],
    "abstract": "Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:25:05+00:00",
    "updated": "2025-11-18T15:25:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14584v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14581v1",
    "title": "Online learning of subgrid-scale models for quasi-geostrophic turbulence in planetary interiors",
    "authors": [
      "Hugo Frezat",
      "Thomas Gastine",
      "Alexandre Fournier"
    ],
    "abstract": "The use of machine learning to represent subgrid-scale (SGS) dynamics is now well established in weather forecasting and climate modelling. Recent advances have demonstrated that SGS models trained via ``online'' end-to-end learning -- where the dynamical solver operating on the filtered equations participates in the training -- can outperform traditional physics-based approaches. Most studies, however, have focused on idealised periodic domains, neglecting the mechanical boundaries present e.g. in planetary interiors. To address this issue, we consider two-dimensional quasi-geostrophic turbulent flow in an axisymmetric bounded domain that we model using a pseudo-spectral differentiable solver, thereby enabling online learning. We examine three configurations, varying the geometry (between an exponential container and a spherical shell) and the rotation rate. Flow is driven by a prescribed analytical forcing, allowing for precise control over the energy injection scale and an exact estimate of the power input. We evaluate the accuracy of the online-trained SGS model against the reference direct numerical simulation using integral quantities and spectral diagnostics. In all configurations, we show that an SGS model trained on data spanning only one turnover time remains stable and accurate over integrations at least a hundred times longer than the training period. Moreover, we demonstrate the model's remarkable ability to reproduce slow processes occurring on time scales far exceeding the training duration, such as the inward drift of jets in the spherical shell. These results suggest a promising path towards developing SGS models for planetary and stellar interior dynamics, including dynamo processes.",
    "categories": [
      "physics.flu-dyn",
      "astro-ph.EP",
      "cs.LG"
    ],
    "primary_category": "physics.flu-dyn",
    "published": "2025-11-18T15:21:38+00:00",
    "updated": "2025-11-18T15:21:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14581v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14569v1",
    "title": "Task Addition and Weight Disentanglement in Closed-Vocabulary Models",
    "authors": [
      "Adam Hazimeh",
      "Alessandro Favero",
      "Pascal Frossard"
    ],
    "abstract": "Task arithmetic has recently emerged as a promising method for editing pre-trained \\textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \\textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \\textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T15:12:21+00:00",
    "updated": "2025-11-18T15:12:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14569v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14559v1",
    "title": "Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models",
    "authors": [
      "Xinzhe Zheng",
      "Shiyu Jiang",
      "Gustavo Seabra",
      "Chenglong Li",
      "Yanjun Li"
    ],
    "abstract": "Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM",
    "published": "2025-11-18T15:01:27+00:00",
    "updated": "2025-11-18T15:01:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14559v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14554v1",
    "title": "ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection",
    "authors": [
      "Mohammad Romani"
    ],
    "abstract": "Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T14:56:34+00:00",
    "updated": "2025-11-18T14:56:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14554v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14545v1",
    "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time",
    "authors": [
      "Haorui Ma",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ],
    "abstract": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T14:49:03+00:00",
    "updated": "2025-11-18T14:49:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14545v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14544v1",
    "title": "Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction",
    "authors": [
      "Jaume Ros",
      "Alessio Arleo",
      "Fernando Paulovich"
    ],
    "abstract": "Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:45:19+00:00",
    "updated": "2025-11-18T14:45:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14544v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14543v1",
    "title": "MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation",
    "authors": [
      "Youran Zhou",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "abstract": "Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.   We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.   Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:44:49+00:00",
    "updated": "2025-11-18T14:44:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14543v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14530v1",
    "title": "DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation",
    "authors": [
      "Xiangchen Yin",
      "Jiahui Yuan",
      "Zhangchi Hu",
      "Wenzhang Sun",
      "Jie Chen",
      "Xiaozhen Qiao",
      "Hao Li",
      "Xiaoyan Sun"
    ],
    "abstract": "Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T14:34:20+00:00",
    "updated": "2025-11-18T14:34:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14530v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14516v2",
    "title": "Full-Atom Peptide Design via Riemannian-Euclidean Bayesian Flow Networks",
    "authors": [
      "Hao Qian",
      "Shikui Tu",
      "Lei Xu"
    ],
    "abstract": "Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:13:23+00:00",
    "updated": "2025-11-19T03:15:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14516v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15738v1",
    "title": "Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn",
    "authors": [
      "Chao Yu",
      "Qixin Tan",
      "Jiaxuan Gao",
      "Shi Yu",
      "Hong Lu",
      "Xinting Yang",
      "Zelai Xu",
      "Yu Wang",
      "Yi Wu",
      "Eugene Vinitsky"
    ],
    "abstract": "Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:07:57+00:00",
    "updated": "2025-11-18T14:07:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15738v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14510v1",
    "title": "CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design",
    "authors": [
      "Jiawei Yi",
      "Ping Gong",
      "Youhui Bai",
      "Jiaqi Ruan",
      "Shengnan Wang",
      "Pengcheng Wang",
      "Haibo Wang",
      "Weiguang Wang",
      "Xia Zhu",
      "Feng Wu",
      "Cheng Li"
    ],
    "abstract": "The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T14:03:21+00:00",
    "updated": "2025-11-18T14:03:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14510v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14501v1",
    "title": "Improved Convergence in Parameter-Agnostic Error Feedback through Momentum",
    "authors": [
      "Abdurakhmon Sadiev",
      "Yury Demidovich",
      "Igor Sokolov",
      "Grigory Malinovsky",
      "Sarit Khirirat",
      "Peter Richtárik"
    ],
    "abstract": "Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-18T13:47:08+00:00",
    "updated": "2025-11-18T13:47:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14501v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14488v1",
    "title": "Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching",
    "authors": [
      "Jintao Zhang",
      "Mingyue Cheng",
      "Zirui Liu",
      "Xianquan Wang",
      "Yitong Zhou",
      "Qi Liu"
    ],
    "abstract": "Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \\textbf{PAFM}, a \\textbf{P}erturbation-\\textbf{A}ware \\textbf{F}low \\textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:30:56+00:00",
    "updated": "2025-11-18T13:30:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14488v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14485v1",
    "title": "Notes on Kernel Methods in Machine Learning",
    "authors": [
      "Diego Armando Pérez-Rosero",
      "Danna Valentina Salazar-Dubois",
      "Juan Camilo Lugo-Rojas",
      "Andrés Marino Álvarez-Meza",
      "Germán Castellanos-Dominguez"
    ],
    "abstract": "These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:29:07+00:00",
    "updated": "2025-11-18T13:29:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14485v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14482v1",
    "title": "Gradient-Based Join Ordering",
    "authors": [
      "Tim Schwabe",
      "Maribel Acosta"
    ],
    "abstract": "Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "published": "2025-11-18T13:24:28+00:00",
    "updated": "2025-11-18T13:24:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14482v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14465v1",
    "title": "nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers",
    "authors": [
      "Clément Dumas"
    ],
    "abstract": "Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T13:05:02+00:00",
    "updated": "2025-11-18T13:05:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14465v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14455v2",
    "title": "Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks",
    "authors": [
      "Nicola Rares Franco",
      "Lorenzo Tedesco"
    ],
    "abstract": "We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\\varphi=\\varphi(x,u)$ such that $\\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:59:20+00:00",
    "updated": "2025-11-20T18:06:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14455v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14452v1",
    "title": "Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters",
    "authors": [
      "Emanuele Palumbo",
      "Sorawit Saengkyongam",
      "Maria R. Cervera",
      "Jens Behrmann",
      "Andrew C. Miller",
      "Guillermo Sapiro",
      "Christina Heinze-Deml",
      "Antoine Wehenkel"
    ],
    "abstract": "Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:56:20+00:00",
    "updated": "2025-11-18T12:56:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14452v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14445v1",
    "title": "Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning",
    "authors": [
      "Trishala Jayesh Ahalpara"
    ],
    "abstract": "We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T12:43:04+00:00",
    "updated": "2025-11-18T12:43:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14445v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14441v1",
    "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models",
    "authors": [
      "Daniel Klippert",
      "Alexander Marx"
    ],
    "abstract": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T12:40:41+00:00",
    "updated": "2025-11-18T12:40:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14441v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14427v1",
    "title": "Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning",
    "authors": [
      "Rickmer Krohn",
      "Vignesh Prasad",
      "Gabriele Tiboni",
      "Georgia Chalvatzaki"
    ],
    "abstract": "Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "published": "2025-11-18T12:32:23+00:00",
    "updated": "2025-11-18T12:32:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14427v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14426v1",
    "title": "MiAD: Mirage Atom Diffusion for De Novo Crystal Generation",
    "authors": [
      "Andrey Okhotin",
      "Maksim Nakhodnov",
      "Nikita Kazeev",
      "Andrey E Ustyuzhanin",
      "Dmitry Vetrov"
    ],
    "abstract": "In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \\href{https://github.com/andrey-okhotin/miad.git}{\\texttt{github.com/andrey-okhotin/miad}}.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:29:19+00:00",
    "updated": "2025-11-18T12:29:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14426v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14422v1",
    "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
    "authors": [
      "Zhengchunmin Dai",
      "Jiaxiong Tang",
      "Peng Sun",
      "Honglong Chen",
      "Liantao Wu"
    ],
    "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.   To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T12:27:43+00:00",
    "updated": "2025-11-18T12:27:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14422v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14419v1",
    "title": "FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis",
    "authors": [
      "Xiaowei Xu",
      "Justin Sonneck",
      "Hongxiao Wang",
      "Roman Burkard",
      "Hendrik Wohrle",
      "Anton Grabmasier",
      "Matthias Gunzer",
      "Jianxu Chen"
    ],
    "abstract": "Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:25:18+00:00",
    "updated": "2025-11-18T12:25:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14419v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14416v1",
    "title": "Toward Robust and Harmonious Adaptation for Cross-modal Retrieval",
    "authors": [
      "Haobin Li",
      "Mouxing Yang",
      "Xi Peng"
    ],
    "abstract": "Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:21:23+00:00",
    "updated": "2025-11-18T12:21:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14416v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14406v1",
    "title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation",
    "authors": [
      "Bastien Vuillod",
      "Pierre-Alain Moellic",
      "Jean-Max Dutertre"
    ],
    "abstract": "Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T12:13:59+00:00",
    "updated": "2025-11-18T12:13:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14406v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14368v1",
    "title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model",
    "authors": [
      "Rishi Gupta",
      "Mukilan Karuppasamy",
      "Shyam Marjit",
      "Aditay Tripathi",
      "Anirban Chakraborty"
    ],
    "abstract": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T11:18:08+00:00",
    "updated": "2025-11-18T11:18:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14368v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14348v1",
    "title": "Enforcing hidden physics in physics-informed neural networks",
    "authors": [
      "Nanxi Chen",
      "Sifan Wang",
      "Rujin Ma",
      "Airong Chen",
      "Chuanjie Cui"
    ],
    "abstract": "Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:52:37+00:00",
    "updated": "2025-11-18T10:52:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14348v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14334v2",
    "title": "When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling",
    "authors": [
      "Alessio Pellegrino",
      "Jacopo Mauro"
    ],
    "abstract": "One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T10:40:32+00:00",
    "updated": "2025-11-19T10:26:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14334v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14320v1",
    "title": "Learning with Statistical Equality Constraints",
    "authors": [
      "Aneesh Barthakur",
      "Luiz F. O. Chamon"
    ],
    "abstract": "As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:24:13+00:00",
    "updated": "2025-11-18T10:24:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14320v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14317v1",
    "title": "Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect",
    "authors": [
      "Yuwen Zhang",
      "Viet Tran",
      "Paul Weng"
    ],
    "abstract": "In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:21:07+00:00",
    "updated": "2025-11-18T10:21:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14317v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14312v1",
    "title": "H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata",
    "authors": [
      "Chenyang Xu",
      "Siming Li",
      "Hao Wang"
    ],
    "abstract": "Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T10:16:22+00:00",
    "updated": "2025-11-18T10:16:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14312v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14307v1",
    "title": "Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions",
    "authors": [
      "Marcel Gibier",
      "Nolwenn Celton",
      "Raphaël Duroselle",
      "Pierre Serrano",
      "Olivier Boeffard",
      "Jean-François Bonastre"
    ],
    "abstract": "In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T10:05:36+00:00",
    "updated": "2025-11-18T10:05:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14307v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14301v1",
    "title": "Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion",
    "authors": [
      "Eric Xue",
      "Ruiyi Zhang",
      "Zijun Zhang",
      "Pengtao Xie"
    ],
    "abstract": "Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2025-11-18T09:56:16+00:00",
    "updated": "2025-11-18T09:56:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14301v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14295v1",
    "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
    "authors": [
      "Mohammad Zbib",
      "Hasan Abed Al Kader Hammoud",
      "Sina Mukalled",
      "Nadine Rizk",
      "Fatima Karnib",
      "Issam Lakkis",
      "Ammar Mohanna",
      "Bernard Ghanem"
    ],
    "abstract": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-18T09:47:01+00:00",
    "updated": "2025-11-18T09:47:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14295v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14293v1",
    "title": "Segmentwise Pruning in Audio-Language Models",
    "authors": [
      "Marcel Gibier",
      "Raphaël Duroselle",
      "Pierre Serrano",
      "Olivier Boeffard",
      "Jean-François Bonastre"
    ],
    "abstract": "Recent audio-language models have shown impressive performance across a wide range of audio tasks and are increasingly capable of handling long audio inputs. However, the computing costs in these models heavily depend on sequence length, which can become very large given the nature of audio data. In the vision-language domain, token pruning methods have proven effective in reducing token counts while preserving strong performance on standard benchmarks. In this work, we investigate the relevance and effectiveness of such token selection strategies in the context of audio-language models. We also improve them by proposing a lightweight strategy that takes the time dimension into account. While retaining only a quarter of the initial tokens, our approach results in a relative maximum decrease of 2% in CIDEr on Clotho v2 and a relative maximum decrease of 4% in accuracy on MMAU.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T09:43:27+00:00",
    "updated": "2025-11-18T09:43:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14293v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14283v1",
    "title": "NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction",
    "authors": [
      "Zi-Chen Xi",
      "Jiahui Huang",
      "Hao-Xiang Chen",
      "Francis Williams",
      "Qun-Ce Xu",
      "Tai-Jiang Mu",
      "Shi-Min Hu"
    ],
    "abstract": "We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T09:20:15+00:00",
    "updated": "2025-11-18T09:20:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14283v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14282v1",
    "title": "Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning",
    "authors": [
      "Vincent-Daniel Yun",
      "Junhyuk Jo",
      "Sunwoo Lee"
    ],
    "abstract": "Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T09:18:26+00:00",
    "updated": "2025-11-18T09:18:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14282v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14276v1",
    "title": "Comparing Task-Agnostic Embedding Models for Tabular Data",
    "authors": [
      "Frederik Hoppe",
      "Lars Kleinemeier",
      "Astrid Franz",
      "Udo Göbel"
    ],
    "abstract": "Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T09:10:40+00:00",
    "updated": "2025-11-18T09:10:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14276v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14268v1",
    "title": "Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks",
    "authors": [
      "Zhenchuan Ma",
      "Qizhi Teng",
      "Pengcheng Yan",
      "Lindong Li",
      "Kirill M. Gerke",
      "Marina V. Karsanina",
      "Xiaohai He"
    ],
    "abstract": "Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.",
    "categories": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph",
    "published": "2025-11-18T09:02:09+00:00",
    "updated": "2025-11-18T09:02:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14268v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14265v1",
    "title": "Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention",
    "authors": [
      "Rui Zhang",
      "Chao Li",
      "Kezhong Liu",
      "Chen Wang",
      "Bolong Zheng",
      "Hongbo Jiang"
    ],
    "abstract": "Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:56:30+00:00",
    "updated": "2025-11-18T08:56:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14265v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14263v1",
    "title": "Algebraformer: A Neural Approach to Linear Systems",
    "authors": [
      "Pietro Sittoni",
      "Francesco Tudisco"
    ],
    "abstract": "Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:53:22+00:00",
    "updated": "2025-11-18T08:53:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14263v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14262v1",
    "title": "Object-Centric World Models for Causality-Aware Reinforcement Learning",
    "authors": [
      "Yosuke Nishimoto",
      "Takashi Matsubara"
    ],
    "abstract": "World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \\emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:53:09+00:00",
    "updated": "2025-11-18T08:53:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14262v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14250v1",
    "title": "Count The Notes: Histogram-Based Supervision for Automatic Music Transcription",
    "authors": [
      "Jonathan Yaffe",
      "Ben Maman",
      "Meinard Müller",
      "Amit H. Bermano"
    ],
    "abstract": "Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "published": "2025-11-18T08:40:05+00:00",
    "updated": "2025-11-18T08:40:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14250v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14238v1",
    "title": "Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization",
    "authors": [
      "Yan Huang",
      "Yongyi Su",
      "Xin Lin",
      "Le Zhang",
      "Xun Xu"
    ],
    "abstract": "The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-18T08:16:16+00:00",
    "updated": "2025-11-18T08:16:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14238v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14229v1",
    "title": "EBind: a practical approach to space binding",
    "authors": [
      "Jim Broadbent",
      "Felix Cohen",
      "Frederik Hvilshøj",
      "Eric Landau",
      "Eren Sasoglu"
    ],
    "abstract": "We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T08:03:30+00:00",
    "updated": "2025-11-18T08:03:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14229v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14227v1",
    "title": "DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home",
    "authors": [
      "Yuxiang Wang",
      "Siwen Wang",
      "Haowei Han",
      "Ao Wang",
      "Boya Liu",
      "Yong Zhao",
      "Chengbo Wu",
      "Bin Zhu",
      "Bin Qin",
      "Xiaokai Zhou",
      "Xiao Yan",
      "Jiawei Jiang",
      "Bo Du"
    ],
    "abstract": "Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-18T08:01:55+00:00",
    "updated": "2025-11-18T08:01:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14227v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14220v1",
    "title": "Parallelizing Tree Search with Twice Sequential Monte Carlo",
    "authors": [
      "Yaniv Oren",
      "Joery A. de Vries",
      "Pascal R. van der Vaart",
      "Matthijs T. J. Spaan",
      "Wendelin Böhmer"
    ],
    "abstract": "Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T07:54:29+00:00",
    "updated": "2025-11-18T07:54:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14220v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14218v1",
    "title": "Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts",
    "authors": [
      "Xinlei Xiong",
      "Wenbo Hu",
      "Shuxun Zhou",
      "Kaifeng Bi",
      "Lingxi Xie",
      "Ying Liu",
      "Richang Hong",
      "Qi Tian"
    ],
    "abstract": "Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T07:49:52+00:00",
    "updated": "2025-11-18T07:49:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14218v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15679v1",
    "title": "Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion",
    "authors": [
      "Jianqiao Mao",
      "Max A. Little"
    ],
    "abstract": "Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\\left(\\boldsymbol{X}^{*},\\boldsymbol{Y}^{*},\\boldsymbol{M}^{*}\\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T18:26:55+00:00",
    "updated": "2025-11-19T18:26:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15679v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15634v1",
    "title": "Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities",
    "authors": [
      "Benjamin Dupuis",
      "Mert Gürbüzbalaban",
      "Umut Şimşekli",
      "Jian Wang",
      "Sinan Yildirim",
      "Lingjiong Zhu"
    ],
    "abstract": "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T17:18:54+00:00",
    "updated": "2025-11-19T17:18:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15634v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15619v1",
    "title": "CODE: A global approach to ODE dynamics learning",
    "authors": [
      "Nils Wildt",
      "Daniel M. Tartakovsky",
      "Sergey Oladyshkin",
      "Wolfgang Nowak"
    ],
    "abstract": "Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T17:04:24+00:00",
    "updated": "2025-11-19T17:04:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15615v1",
    "title": "Near-optimal delta-convex estimation of Lipschitz functions",
    "authors": [
      "Gábor Balázs"
    ],
    "abstract": "This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T17:02:30+00:00",
    "updated": "2025-11-19T17:02:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15615v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15543v1",
    "title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation",
    "authors": [
      "Georgios Venianakis",
      "Constantinos Theodoropoulos",
      "Michail Kavousanakis"
    ],
    "abstract": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T15:37:17+00:00",
    "updated": "2025-11-19T15:37:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15543v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15507v1",
    "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
    "authors": [
      "Nika Haghtalab",
      "Omar Montasser",
      "Mingda Qiao"
    ],
    "abstract": "We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\\widetilde O((d + k) / ε^2)$ within $\\widetilde O(\\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\\widetilde O(\\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-19T14:59:47+00:00",
    "updated": "2025-11-19T14:59:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15507v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15446v1",
    "title": "Gini Score under Ties and Case Weights",
    "authors": [
      "Alexej Brauer",
      "Mario V. Wüthrich"
    ],
    "abstract": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T14:01:12+00:00",
    "updated": "2025-11-19T14:01:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15446v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15332v1",
    "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss",
    "authors": [
      "The Tien Mai"
    ],
    "abstract": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.   Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T10:50:46+00:00",
    "updated": "2025-11-19T10:50:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15332v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15330v1",
    "title": "BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data",
    "authors": [
      "Marta S. Lemanczyk",
      "Lucas Kock",
      "Johanna Schlimme",
      "Nadja Klein",
      "Bernhard Y. Renard"
    ],
    "abstract": "Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.",
    "categories": [
      "stat.ME",
      "q-bio.GN",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-19T10:48:30+00:00",
    "updated": "2025-11-19T10:48:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15330v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15315v1",
    "title": "Robust Bayesian Optimisation with Unbounded Corruptions",
    "authors": [
      "Abdelhamid Ezzerg",
      "Ilija Bogunovic",
      "Jeremias Knoblauch"
    ],
    "abstract": "Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T10:28:56+00:00",
    "updated": "2025-11-19T10:28:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15315v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15196v1",
    "title": "Particle Monte Carlo methods for Lattice Field Theory",
    "authors": [
      "David Yallup"
    ],
    "abstract": "High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "hep-lat"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T07:31:46+00:00",
    "updated": "2025-11-19T07:31:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15196v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15146v1",
    "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings",
    "authors": [
      "Eugene Ndiaye"
    ],
    "abstract": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T05:59:01+00:00",
    "updated": "2025-11-19T05:59:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15146v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15120v1",
    "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit",
    "authors": [
      "Bohan Zhang",
      "Zihao Wang",
      "Hengyu Fu",
      "Jason D. Lee"
    ],
    "abstract": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T04:46:47+00:00",
    "updated": "2025-11-19T04:46:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15120v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.15010v1",
    "title": "Latent space analysis and generalization to out-of-distribution data",
    "authors": [
      "Katie Rainey",
      "Erin Hausmann",
      "Donald Waagen",
      "David Gray",
      "Donald Hulsey"
    ],
    "abstract": "Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \\textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-19T01:23:34+00:00",
    "updated": "2025-11-19T01:23:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.15010v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14827v1",
    "title": "Implicit Bias of the JKO Scheme",
    "authors": [
      "Peter Halmos",
      "Boris Hanin"
    ],
    "abstract": "Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \\emph{modified} energy \\[ J^η(ρ) = J(ρ) - \\fracη{4}\\int_M \\Big\\lVert \\nabla_g \\frac{δJ}{δρ} (ρ) \\Big\\rVert_{2}^{2} \\,ρ(dx), \\] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \\textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \\emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T18:48:37+00:00",
    "updated": "2025-11-18T18:48:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14827v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14745v1",
    "title": "Look-Ahead Reasoning on Learning Platforms",
    "authors": [
      "Haiqing Zhu",
      "Tijana Zrnic",
      "Celestine Mendler-Dünner"
    ],
    "abstract": "On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T18:45:32+00:00",
    "updated": "2025-11-18T18:45:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14745v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14710v1",
    "title": "Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization",
    "authors": [
      "Zonghao Chen",
      "Atsushi Nitanda",
      "Arthur Gretton",
      "Taiji Suzuki"
    ],
    "abstract": "We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \\emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \\texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T17:51:17+00:00",
    "updated": "2025-11-18T17:51:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14710v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14691v1",
    "title": "Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer",
    "authors": [
      "Kallol Mondal",
      "Ankush Kumar"
    ],
    "abstract": "Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\\% and 78.08\\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "stat.ML"
    ],
    "primary_category": "cs.NE",
    "published": "2025-11-18T17:28:29+00:00",
    "updated": "2025-11-18T17:28:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14691v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14545v1",
    "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time",
    "authors": [
      "Haorui Ma",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ],
    "abstract": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T14:49:03+00:00",
    "updated": "2025-11-18T14:49:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14545v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14441v1",
    "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models",
    "authors": [
      "Daniel Klippert",
      "Alexander Marx"
    ],
    "abstract": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T12:40:41+00:00",
    "updated": "2025-11-18T12:40:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14441v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14206v1",
    "title": "Causal Discovery on Higher-Order Interactions",
    "authors": [
      "Alessio Zanga",
      "Marco Scutari",
      "Fabio Stella"
    ],
    "abstract": "Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T07:35:00+00:00",
    "updated": "2025-11-18T07:35:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14206v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14146v1",
    "title": "SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation",
    "authors": [
      "Renjie Chen",
      "Viet Anh Nguyen",
      "Huifu Xu"
    ],
    "abstract": "We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T05:13:49+00:00",
    "updated": "2025-11-18T05:13:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14146v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14133v1",
    "title": "Synthetic Survival Control: Extending Synthetic Controls for \"When-If\" Decision",
    "authors": [
      "Jessy Xinyi Han",
      "Devavrat Shah"
    ],
    "abstract": "Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such \"when-if\" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.",
    "categories": [
      "cs.LG",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T04:36:20+00:00",
    "updated": "2025-11-18T04:36:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14133v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14056v1",
    "title": "Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds",
    "authors": [
      "Marios Papamichals",
      "Regina Ruane"
    ],
    "abstract": "Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.DG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T02:15:25+00:00",
    "updated": "2025-11-18T02:15:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14056v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14049v1",
    "title": "SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics",
    "authors": [
      "Semen Leontev"
    ],
    "abstract": "Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.   We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.   Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.   Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-18T02:00:55+00:00",
    "updated": "2025-11-18T02:00:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14049v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14042v1",
    "title": "Splat Regression Models",
    "authors": [
      "Mara Daniels",
      "Philippe Rigollet"
    ],
    "abstract": "We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-18T01:49:33+00:00",
    "updated": "2025-11-18T01:49:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14042v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13999v1",
    "title": "On the Gradient Complexity of Private Optimization with Private Oracles",
    "authors": [
      "Michael Menart",
      "Aleksandar Nikolov"
    ],
    "abstract": "We study the running time, in terms of first order oracle queries, of differentially private empirical/population risk minimization of Lipschitz convex losses. We first consider the setting where the loss is non-smooth and the optimizer interacts with a private proxy oracle, which sends only private messages about a minibatch of gradients. In this setting, we show that expected running time $Ω(\\min\\{\\frac{\\sqrt{d}}{α^2}, \\frac{d}{\\log(1/α)}\\})$ is necessary to achieve $α$ excess risk on problems of dimension $d$ when $d \\geq 1/α^2$. Upper bounds via DP-SGD show these results are tight when $d>\\tildeΩ(1/α^4)$. We further show our lower bound can be strengthened to $Ω(\\min\\{\\frac{d}{\\bar{m}α^2}, \\frac{d}{\\log(1/α)} \\})$ for algorithms which use minibatches of size at most $\\bar{m} < \\sqrt{d}$. We next consider smooth losses, where we relax the private oracle assumption and give lower bounds under only the condition that the optimizer is private. Here, we lower bound the expected number of first order oracle calls by $\\tildeΩ\\big(\\frac{\\sqrt{d}}α + \\min\\{\\frac{1}{α^2}, n\\}\\big)$, where $n$ is the size of the dataset. Modifications to existing algorithms show this bound is nearly tight. Compared to non-private lower bounds, our results show that differentially private optimizers pay a dimension dependent runtime penalty. Finally, as a natural extension of our proof technique, we show lower bounds in the non-smooth setting for optimizers interacting with information limited oracles. Specifically, if the proxy oracle transmits at most $Γ$-bits of information about the gradients in the minibatch, then $Ω\\big(\\min\\{\\frac{d}{α^2Γ}, \\frac{d}{\\log(1/α)}\\}\\big)$ oracle calls are needed. This result shows fundamental limitations of gradient quantization techniques in optimization.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T23:58:11+00:00",
    "updated": "2025-11-17T23:58:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13999v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13934v1",
    "title": "Empirical Likelihood for Random Forests and Ensembles",
    "authors": [
      "Harold D. Chiang",
      "Yukitoshi Matsushita",
      "Taisuke Otsu"
    ],
    "abstract": "We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-17T21:38:43+00:00",
    "updated": "2025-11-17T21:38:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13934v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13911v1",
    "title": "Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands",
    "authors": [
      "Vasiliki Tassopoulou",
      "Charis Stamouli",
      "Haochang Shou",
      "George J. Pappas",
      "Christos Davatzikos"
    ],
    "abstract": "Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-17T21:04:14+00:00",
    "updated": "2025-11-17T21:04:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13911v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13699v1",
    "title": "Efficient Calibration for Decision Making",
    "authors": [
      "Parikshit Gopalan",
      "Konstantinos Stavropoulos",
      "Kunal Talwar",
      "Pranay Tankala"
    ],
    "abstract": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.   We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T18:52:00+00:00",
    "updated": "2025-11-17T18:52:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13699v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13675v1",
    "title": "Scientific Data Compression and Super-Resolution Sampling",
    "authors": [
      "Minh Vu",
      "Andrey Lokhov"
    ],
    "abstract": "Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.",
    "categories": [
      "cs.LG",
      "physics.data-an",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T18:31:57+00:00",
    "updated": "2025-11-17T18:31:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13675v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13602v1",
    "title": "Nonparametric Estimation of Joint Entropy through Partitioned Sample-Spacing Method",
    "authors": [
      "Jungwoo Ho",
      "Sangun Park",
      "Soyeong Oh"
    ],
    "abstract": "We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacings (PSS). The method extends univariate spacing ideas to multivariate settings by partitioning the sample space into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms k-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions (d = 10 to 40) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical alternative to normalizing flow-based approaches, with broad potential in information-theoretic machine learning applications.",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-17T17:05:34+00:00",
    "updated": "2025-11-17T17:05:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13602v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13503v1",
    "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business",
    "authors": [
      "Ioannis Diamantis"
    ],
    "abstract": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-17T15:41:11+00:00",
    "updated": "2025-11-17T15:41:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13503v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13465v2",
    "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate",
    "authors": [
      "Meng Zhu",
      "Quan Xiao",
      "Weidong Min"
    ],
    "abstract": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T15:07:55+00:00",
    "updated": "2025-11-19T14:11:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13465v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13421v1",
    "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
    "authors": [
      "Tingkai Yan",
      "Haodong Wen",
      "Binghui Li",
      "Kairong Luo",
      "Wenguang Chen",
      "Kaifeng Lyu"
    ],
    "abstract": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T14:34:03+00:00",
    "updated": "2025-11-17T14:34:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13421v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13394v1",
    "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
    "authors": [
      "Vasilis Gkolemis",
      "Christos Diou",
      "Michael Gutmann"
    ],
    "abstract": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T14:07:36+00:00",
    "updated": "2025-11-17T14:07:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13394v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13237v1",
    "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification",
    "authors": [
      "Alan G. Paredes Cetina",
      "Kaouther Benguessoum",
      "Raoni Lourenço",
      "Sylvain Kubler"
    ],
    "abstract": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T11:00:43+00:00",
    "updated": "2025-11-17T11:00:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13237v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13229v1",
    "title": "Laplace Learning in Wasserstein Space",
    "authors": [
      "Mary Chriselda Antony Oliver",
      "Michael Roberts",
      "Carola-Bibiane Schönlieb",
      "Matthew Thorpe"
    ],
    "abstract": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning   methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical   notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to   an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator   on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through   numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T10:49:36+00:00",
    "updated": "2025-11-17T10:49:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13229v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13221v1",
    "title": "Likelihood-guided Regularization in Attention Based Models",
    "authors": [
      "Mohamed Salem",
      "Inyoung Kim"
    ],
    "abstract": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-17T10:38:09+00:00",
    "updated": "2025-11-17T10:38:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13221v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13199v1",
    "title": "Asymptotic confidence bands for centered purely random forests",
    "authors": [
      "Natalie Neumeyer",
      "Jan Rabe",
      "Mathias Trabs"
    ],
    "abstract": "In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-17T10:09:01+00:00",
    "updated": "2025-11-17T10:09:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13199v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13049v1",
    "title": "Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information",
    "authors": [
      "Antoine Ledent",
      "Mun Chong Soo",
      "Nong Minh Hieu"
    ],
    "abstract": "We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \\textit{share a common subspace}. We assume that a large amount $M$ of \\textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\\widetilde{O}\\left(\\sqrt{\\frac{nd}{M}}\\right)$ and $\\widetilde{O}\\left(\\sqrt{\\frac{dr}{N}}\\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T06:53:50+00:00",
    "updated": "2025-11-17T06:53:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13049v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13025v1",
    "title": "Reconstruction of Manifold Distances from Noisy Observations",
    "authors": [
      "Charles Fefferman",
      "Jonathan Marty",
      "Kevin Ren"
    ],
    "abstract": "We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\\varepsilon \\log \\varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \\asymp \\varepsilon^{-2d-2}\\log(1/\\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.DG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-17T06:24:31+00:00",
    "updated": "2025-11-17T06:24:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13025v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12881v1",
    "title": "On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples",
    "authors": [
      "Cheongjae Jang",
      "Jonghyun Won",
      "Soyeon Jun",
      "Chun Kee Chung",
      "Keehyoung Joo",
      "Yung-Kyun Noh"
    ],
    "abstract": "Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-17T02:16:20+00:00",
    "updated": "2025-11-17T02:16:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12881v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12840v1",
    "title": "Benign Overfitting in Linear Classifiers with a Bias Term",
    "authors": [
      "Yuta Kondo"
    ],
    "abstract": "Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of \"homogeneous\" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-16T23:59:36+00:00",
    "updated": "2025-11-16T23:59:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12840v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12836v1",
    "title": "DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks",
    "authors": [
      "Waheed U. Bajwa",
      "Mert Gurbuzbalaban",
      "Mustafa Ali Kutbay",
      "Lingjiong Zhu",
      "Muhammad Zulqarnain"
    ],
    "abstract": "Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\\sqrtη)$ neighborhood of the target distribution, where $η$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-16T23:42:44+00:00",
    "updated": "2025-11-16T23:42:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12836v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12803v1",
    "title": "Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability",
    "authors": [
      "Yu-Han Huang",
      "Venugopal V. Veeravalli"
    ],
    "abstract": "A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \\emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results.",
    "categories": [
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.IT",
    "published": "2025-11-16T22:16:33+00:00",
    "updated": "2025-11-16T22:16:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12803v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12783v1",
    "title": "Function-on-Function Bayesian Optimization",
    "authors": [
      "Jingru Huang",
      "Haijie Xu",
      "Manrui Jiang",
      "Chen Zhang"
    ],
    "abstract": "Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-16T21:24:57+00:00",
    "updated": "2025-11-16T21:24:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12783v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12760v1",
    "title": "Conformal Online Learning of Deep Koopman Linear Embeddings",
    "authors": [
      "Ben Gao",
      "Jordan Patracone",
      "Stéphane Chrétien",
      "Olivier Alata"
    ],
    "abstract": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-16T20:08:48+00:00",
    "updated": "2025-11-16T20:08:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12760v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12749v1",
    "title": "TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting",
    "authors": [
      "Zong-Han Bai",
      "Po-Yen Chu"
    ],
    "abstract": "Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.   We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.   On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-16T19:24:33+00:00",
    "updated": "2025-11-16T19:24:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12749v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12695v1",
    "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning",
    "authors": [
      "Minghui Chen",
      "Hrad Ghoukasian",
      "Ruinan Jin",
      "Zehua Wang",
      "Sai Praneeth Karimireddy",
      "Xiaoxiao Li"
    ],
    "abstract": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-16T17:19:23+00:00",
    "updated": "2025-11-16T17:19:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12695v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12688v1",
    "title": "Accelerated Distributional Temporal Difference Learning with Linear Function Approximation",
    "authors": [
      "Kaicheng Jin",
      "Yang Peng",
      "Jiansheng Yang",
      "Zhihua Zhang"
    ],
    "abstract": "In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-16T17:09:52+00:00",
    "updated": "2025-11-16T17:09:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12688v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12659v1",
    "title": "Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back",
    "authors": [
      "Alon Cohen",
      "Liad Erez",
      "Steve Hanneke",
      "Tomer Koren",
      "Yishay Mansour",
      "Shay Moran",
      "Qian Zhang"
    ],
    "abstract": "The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\\frac{DS^{1.5}}ε + \\frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-16T15:47:47+00:00",
    "updated": "2025-11-16T15:47:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12659v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12592v1",
    "title": "Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC",
    "authors": [
      "Pratik Jawahar",
      "Caterina Doglioni",
      "Maurizio Pierini"
    ],
    "abstract": "Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\\text{MHz}$ online latency constraint at the LHC. In this work, we propose \\texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.",
    "categories": [
      "hep-ex",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "hep-ex",
    "published": "2025-11-16T13:31:35+00:00",
    "updated": "2025-11-16T13:31:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12592v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12545v1",
    "title": "Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation",
    "authors": [
      "Robin van der Laag",
      "Hao Wang",
      "Thomas Bäck",
      "Yingjie Fan"
    ],
    "abstract": "Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-16T10:40:17+00:00",
    "updated": "2025-11-16T10:40:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12545v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12389v1",
    "title": "Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation",
    "authors": [
      "Divake Kumar",
      "Patrick Poggi",
      "Sina Tayebati",
      "Devashri Naik",
      "Nilesh Ahuja",
      "Amit Ranjan Trivedi"
    ],
    "abstract": "Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-15T23:47:30+00:00",
    "updated": "2025-11-15T23:47:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12389v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12309v1",
    "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models",
    "authors": [
      "Austin Feng",
      "Marius Alonso",
      "Ambroise Odonnat"
    ],
    "abstract": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-15T17:45:42+00:00",
    "updated": "2025-11-15T17:45:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12309v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12278v1",
    "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning",
    "authors": [
      "Mingqi Wu",
      "Qiang Sun",
      "Yi Yang"
    ],
    "abstract": "High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-15T16:11:39+00:00",
    "updated": "2025-11-15T16:11:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12278v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12261v1",
    "title": "Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection",
    "authors": [
      "Zongxin Shen",
      "Yanyong Huang",
      "Dongjie Wang",
      "Jinyuan Chang",
      "Fengmao Lv",
      "Tianrui Li",
      "Xiaoyi Jiang"
    ],
    "abstract": "Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-15T15:34:52+00:00",
    "updated": "2025-11-15T15:34:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12261v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12257v1",
    "title": "Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems",
    "authors": [
      "Elhadji Cisse Faye",
      "Mame Diarra Fall",
      "Nicolas Dobigeon",
      "Eric Barat"
    ],
    "abstract": "This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.",
    "categories": [
      "stat.CO",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "primary_category": "stat.CO",
    "published": "2025-11-15T15:27:31+00:00",
    "updated": "2025-11-15T15:27:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12257v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12234v1",
    "title": "A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment",
    "authors": [
      "Soham Sarkar",
      "Arnab Hazra"
    ],
    "abstract": "Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.",
    "categories": [
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "stat.AP",
    "published": "2025-11-15T14:22:56+00:00",
    "updated": "2025-11-15T14:22:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12234v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12180v1",
    "title": "Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering",
    "authors": [
      "Ge Cheng",
      "Shuo Wang",
      "Yun Zhang"
    ],
    "abstract": "Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-15T12:17:40+00:00",
    "updated": "2025-11-15T12:17:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12180v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12147v1",
    "title": "Finding Time Series Anomalies using Granular-ball Vector Data Description",
    "authors": [
      "Lifeng Shen",
      "Liang Peng",
      "Ruiwen Liu",
      "Shuyin Xia",
      "Yi Liu"
    ],
    "abstract": "Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-15T10:24:53+00:00",
    "updated": "2025-11-15T10:24:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12147v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12120v1",
    "title": "Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy",
    "authors": [
      "Hongyang Yang",
      "Xiao-Yang Liu",
      "Shan Zhong",
      "Anwar Walid"
    ],
    "abstract": "Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.",
    "categories": [
      "q-fin.TR",
      "q-fin.CP",
      "q-fin.PM",
      "stat.ML"
    ],
    "primary_category": "q-fin.TR",
    "published": "2025-11-15T09:15:10+00:00",
    "updated": "2025-11-15T09:15:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12120v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12065v1",
    "title": "Aggregating Conformal Prediction Sets via α-Allocation",
    "authors": [
      "Congbin Xu",
      "Yue Yu",
      "Haojie Ren",
      "Zhaojun Wang",
      "Changliang Zou"
    ],
    "abstract": "Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-15T07:11:21+00:00",
    "updated": "2025-11-15T07:11:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12065v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.12016v1",
    "title": "MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control",
    "authors": [
      "Youwu Lin",
      "Xiaoyu Qian",
      "Jinru Wu",
      "Qi Liu",
      "Pei Wang"
    ],
    "abstract": "We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-15T03:48:44+00:00",
    "updated": "2025-11-15T03:48:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.12016v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11983v1",
    "title": "Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence",
    "authors": [
      "Debashis Chatterjee"
    ],
    "abstract": "Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.   We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.   Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-15T01:42:49+00:00",
    "updated": "2025-11-15T01:42:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11983v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11966v1",
    "title": "On the Entropy Calibration of Language Models",
    "authors": [
      "Steven Cao",
      "Gregory Valiant",
      "Percy Liang"
    ],
    "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-15T00:33:03+00:00",
    "updated": "2025-11-15T00:33:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11966v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11927v1",
    "title": "PCA recovery thresholds in low-rank matrix inference with sparse noise",
    "authors": [
      "Urte Adomaityte",
      "Gabriele Sicuro",
      "Pierpaolo Vivo"
    ],
    "abstract": "We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T23:09:54+00:00",
    "updated": "2025-11-14T23:09:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11927v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11817v1",
    "title": "FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition",
    "authors": [
      "Zhongde An",
      "Jinhong You",
      "Jiyanglin Li",
      "Yiming Tang",
      "Wen Li",
      "Heming Du",
      "Shouguo Du"
    ],
    "abstract": "Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\\%.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T19:13:24+00:00",
    "updated": "2025-11-14T19:13:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11817v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11564v1",
    "title": "Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility",
    "authors": [
      "Albert Tan",
      "Mohsen Bayati",
      "James Nordlund",
      "Roman Istomin"
    ],
    "abstract": "We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-14T18:55:51+00:00",
    "updated": "2025-11-14T18:55:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11564v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11413v1",
    "title": "Multicalibration yields better matchings",
    "authors": [
      "Riccardo Colini Baldeschi",
      "Simone Di Gregorio",
      "Simone Fioravanti",
      "Federico Fusco",
      "Ido Guy",
      "Daniel Haimovich",
      "Stefano Leonardi",
      "Fridolin Linder",
      "Lorenzo Perini",
      "Matteo Russo",
      "Niek Tax"
    ],
    "abstract": "Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.   In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\\hat γ$, with the following property. Picking the best matching based on the output of $\\hat γ$ is competitive with the best decision rule in $\\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T15:45:07+00:00",
    "updated": "2025-11-14T15:45:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11413v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11355v1",
    "title": "Model Class Selection",
    "authors": [
      "Ryan Cecil",
      "Lucas Mentch"
    ],
    "abstract": "Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided.",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-14T14:43:26+00:00",
    "updated": "2025-11-14T14:43:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11355v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11781v1",
    "title": "Coordinate Descent for Network Linearization",
    "authors": [
      "Vlad Rakhlin",
      "Amir Jevnisek",
      "Shai Avidan"
    ],
    "abstract": "ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T14:03:58+00:00",
    "updated": "2025-11-14T14:03:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11781v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11318v1",
    "title": "Dual Riemannian Newton Method on Statistical Manifolds",
    "authors": [
      "Derun Zhou",
      "Keisuke Yano",
      "Mahito Sugiyama"
    ],
    "abstract": "In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference.",
    "categories": [
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.CO",
    "published": "2025-11-14T13:58:34+00:00",
    "updated": "2025-11-14T13:58:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11318v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11294v1",
    "title": "Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint",
    "authors": [
      "Bertille Tierny",
      "Arthur Charpentier",
      "François Hu"
    ],
    "abstract": "Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T13:27:54+00:00",
    "updated": "2025-11-14T13:27:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11294v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11211v1",
    "title": "A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates",
    "authors": [
      "Wei-Cheng Lee",
      "Francesco Orabona"
    ],
    "abstract": "In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T12:10:23+00:00",
    "updated": "2025-11-14T12:10:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11211v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.13763v1",
    "title": "Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants",
    "authors": [
      "Anthony Kiggundu",
      "Bin Han",
      "Hans D. Schotten"
    ],
    "abstract": "We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T11:59:19+00:00",
    "updated": "2025-11-14T11:59:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.13763v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11161v1",
    "title": "Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths",
    "authors": [
      "Yuzhen Zhao",
      "Yating Liu",
      "Marc Hoffmann"
    ],
    "abstract": "This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T10:56:52+00:00",
    "updated": "2025-11-14T10:56:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11161v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11003v1",
    "title": "Learning bounds for doubly-robust covariate shift adaptation",
    "authors": [
      "Jeonghwan Lee",
      "Cong Ma"
    ],
    "abstract": "Distribution shift between the training domain and the test domain poses a key challenge for modern machine learning. An extensively studied instance is the \\emph{covariate shift}, where the marginal distribution of covariates differs across domains, while the conditional distribution of outcome remains the same. The doubly-robust (DR) estimator, recently introduced by \\cite{kato2023double}, combines the density ratio estimation with a pilot regression model and demonstrates asymptotic normality and $\\sqrt{n}$-consistency, even when the pilot estimates converge slowly. However, the prior arts has focused exclusively on deriving asymptotic results and has left open the question of non-asymptotic guarantees for the DR estimator.   This paper establishes the first non-asymptotic learning bounds for the DR covariate shift adaptation. Our main contributions are two-fold: (\\romannumeral 1) We establish \\emph{structure-agnostic} high-probability upper bounds on the excess target risk of the DR estimator that depend only on the $L^2$-errors of the pilot estimates and the Rademacher complexity of the model class, without assuming specific procedures to obtain the pilot estimate, and (\\romannumeral 2) under \\emph{well-specified parameterized models}, we analyze the DR covariate shift adaptation based on modern techniques for non-asymptotic analysis of MLE, whose key terms governed by the Fisher information mismatch term between the source and target distributions. Together, these findings bridge asymptotic efficiency properties and a finite-sample out-of-distribution generalization bounds, providing a comprehensive theoretical underpinnings for the DR covariate shift adaptation.",
    "categories": [
      "math.ST",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-14T06:46:23+00:00",
    "updated": "2025-11-14T06:46:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11003v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10919v1",
    "title": "Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data",
    "authors": [
      "Jialei Liu",
      "Jun Liao",
      "Kuangnan Fang"
    ],
    "abstract": "Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-14T03:15:31+00:00",
    "updated": "2025-11-14T03:15:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10919v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10898v1",
    "title": "Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters",
    "authors": [
      "Chenghao Duan",
      "Chuanyi Ji"
    ],
    "abstract": "Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\\% - 15\\%$ in both the overall performance and class-wise accuracy.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-14T02:21:30+00:00",
    "updated": "2025-11-14T02:21:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10898v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10890v1",
    "title": "LLM enhanced graph inference for long-term disease progression modelling",
    "authors": [
      "Tiantian He",
      "An Zhao",
      "Elinor Thompson",
      "Anna Schroder",
      "Ahmed Abdulaal",
      "Frederik Barkhof",
      "Daniel C. Alexander"
    ],
    "abstract": "Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-14T02:03:10+00:00",
    "updated": "2025-11-14T02:03:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10890v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10859v1",
    "title": "Private Zeroth-Order Optimization with Public Data",
    "authors": [
      "Xuchen Gong",
      "Tian Li"
    ],
    "abstract": "One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\\times$ runtime speedup.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T23:51:24+00:00",
    "updated": "2025-11-13T23:51:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10859v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10824v1",
    "title": "Neural Local Wasserstein Regression",
    "authors": [
      "Inga Girshfeld",
      "Xiaohui Chen"
    ],
    "abstract": "We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \\emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T21:54:18+00:00",
    "updated": "2025-11-13T21:54:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10824v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10776v1",
    "title": "Potential Outcome Rankings for Counterfactual Decision Making",
    "authors": [
      "Yuta Kawakami",
      "Jin Tian"
    ],
    "abstract": "Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-13T19:58:19+00:00",
    "updated": "2025-11-13T19:58:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10776v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10619v1",
    "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem",
    "authors": [
      "Avrim Blum",
      "Marten Garicano",
      "Kavya Ravichandran",
      "Dravyansh Sharma"
    ],
    "abstract": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T18:46:56+00:00",
    "updated": "2025-11-13T18:46:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10446v2",
    "title": "Continuum Dropout for Neural Differential Equations",
    "authors": [
      "Jonghun Lee",
      "YongKyung Oh",
      "Sungil Kim",
      "Dong-Young Lim"
    ],
    "abstract": "Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T16:10:45+00:00",
    "updated": "2025-11-18T08:29:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10446v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10406v1",
    "title": "Diffusion annealed Langevin dynamics: a theoretical study",
    "authors": [
      "Patrick Cattiaux",
      "Paula Cordero-Encinar",
      "Arnaud Guillin"
    ],
    "abstract": "In this work we study the diffusion annealed Langevin dynamics, a score-based diffusion process recently introduced in the theory of generative models and which is an alternative to the classical overdamped Langevin diffusion. Our goal is to provide a rigorous construction and to study the theoretical efficiency of these models for general base distribution as well as target distribution. As a matter of fact these diffusion processes are a particular case of Nelson processes i.e. diffusion processes with a given flow of time marginals.   Providing existence and uniqueness of the solution to the annealed Langevin diffusion leads to proving a Poincaré inequality for the conditional distribution of $X$ knowing $X+Z=y$ uniformly in $y$, as recently observed by one of us and her coauthors. Part of this work is thus devoted to the study of such Poincaré inequalities. Additionally we show that strengthening the Poincaré inequality into a logarithmic Sobolev inequality improves the efficiency of the model.",
    "categories": [
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "math.PR",
    "published": "2025-11-13T15:26:42+00:00",
    "updated": "2025-11-13T15:26:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10406v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10383v1",
    "title": "Operator Models for Continuous-Time Offline Reinforcement Learning",
    "authors": [
      "Nicolas Hoischen",
      "Petar Bevanda",
      "Max Beier",
      "Stefan Sosnowski",
      "Boris Houska",
      "Sandra Hirche"
    ],
    "abstract": "Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T14:58:30+00:00",
    "updated": "2025-11-13T14:58:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10383v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10048v1",
    "title": "Masking criteria for selecting an imputation model",
    "authors": [
      "Yanjiao Yang",
      "Daniel Suen",
      "Yen-Chi Chen"
    ],
    "abstract": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-13T07:47:41+00:00",
    "updated": "2025-11-13T07:47:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10048v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09996v1",
    "title": "A Novel Data-Dependent Learning Paradigm for Large Hypothesis Classes",
    "authors": [
      "Alireza F. Pour",
      "Shai Ben-David"
    ],
    "abstract": "We address the general task of learning with a set of candidate models that is too large to have a uniform convergence of empirical estimates to true losses. While the common approach to such challenges is SRM (or regularization) based learning algorithms, we propose a novel learning paradigm that relies on stronger incorporation of empirical data and requires less algorithmic decisions to be based on prior assumptions. We analyze the generalization capabilities of our approach and demonstrate its merits in several common learning assumptions, including similarity of close points, clustering of the domain into highly label-homogeneous regions, Lipschitzness assumptions of the labeling rule, and contrastive learning assumptions. Our approach allows utilizing such assumptions without the need to know their true parameters a priori.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T06:02:55+00:00",
    "updated": "2025-11-13T06:02:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09996v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09925v2",
    "title": "Global Convergence of Four-Layer Matrix Factorization under Random Initialization",
    "authors": [
      "Minrui Luo",
      "Weihang Xu",
      "Xiang Gao",
      "Maryam Fazel",
      "Simon Shaolei Du"
    ],
    "abstract": "Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-13T03:40:10+00:00",
    "updated": "2025-11-19T15:50:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09925v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09902v1",
    "title": "Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling",
    "authors": [
      "Hossein Rouhvarzi",
      "Anastasis Kratsios"
    ],
    "abstract": "Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.   Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.",
    "categories": [
      "cs.LG",
      "math.CA",
      "math.DS",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T03:01:50+00:00",
    "updated": "2025-11-13T03:01:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09902v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09897v1",
    "title": "Theory and computation for structured variational inference",
    "authors": [
      "Shunan Sheng",
      "Bohan Wu",
      "Bennett Zhu",
      "Sinho Chewi",
      "Aram-Alexandre Pooladian"
    ],
    "abstract": "Structured variational inference constitutes a core methodology in modern statistical applications. Unlike mean-field variational inference, the approximate posterior is assumed to have interdependent structure. We consider the natural setting of star-structured variational inference, where a root variable impacts all the other ones. We prove the first results for existence, uniqueness, and self-consistency of the variational approximation. In turn, we derive quantitative approximation error bounds for the variational approximation to the posterior, extending prior work from the mean-field setting to the star-structured setting. We also develop a gradient-based algorithm with provable guarantees for computing the variational approximation using ideas from optimal transport theory. We explore the implications of our results for Gaussian measures and hierarchical Bayesian models, including generalized linear models with location family priors and spike-and-slab priors with one-dimensional debiasing. As a by-product of our analysis, we develop new stability results for star-separable transport maps which might be of independent interest.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T02:57:54+00:00",
    "updated": "2025-11-13T02:57:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09897v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09872v1",
    "title": "Randomized batch-sampling Kaczmarz methods for general linear systems",
    "authors": [
      "Dong-Yue Xie",
      "Xi Yang"
    ],
    "abstract": "To conduct a more in-depth investigation of randomized solvers for general linear systems, we adopt a unified randomized batch-sampling Kaczmarz framework with per-iteration costs as low as cyclic block methods, and develop a general analysis technique to establish its convergence guarantee. With concentration inequalities, we derive new expected linear convergence rate bounds. The analysis applies to any randomized non-extended block Kaczmarz methods with static stochastic samplings. In addition, the new rate bounds are scale-invariant which eliminate the dependence on the magnitude of the data matrix. In most experiments, the new bounds are significantly tighter than existing ones and better reflect the empirical convergence behavior of block methods. Within this new framework, the batch-sampling distribution, as a learnable parameter, provides the possibility for block methods to achieve efficient performance in specific application scenarios, which deserves further investigation.",
    "categories": [
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-13T02:10:35+00:00",
    "updated": "2025-11-13T02:10:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09872v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09845v2",
    "title": "Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints",
    "authors": [
      "Cac Phan",
      "Kai Wang"
    ],
    "abstract": "This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(δ, ε)$-Goldstein stationary points using $Θ(δ^{-1}ε^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.",
    "categories": [
      "math.OC",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-13T00:59:20+00:00",
    "updated": "2025-11-15T02:26:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09845v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09801v1",
    "title": "Generalized infinite dimensional Alpha-Procrustes based geometries",
    "authors": [
      "Salvish Goomanee",
      "Andi Han",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ],
    "abstract": "This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T23:05:14+00:00",
    "updated": "2025-11-12T23:05:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09801v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09759v1",
    "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport",
    "authors": [
      "Borna Bateni",
      "Yubai Yuan",
      "Qi Xu",
      "Annie Qu"
    ],
    "abstract": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-12T21:42:11+00:00",
    "updated": "2025-11-12T21:42:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09759v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14784v1",
    "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator",
    "authors": [
      "Sourav De",
      "Koustav Chowdhury",
      "Bibhabasu Mandal",
      "Sagar Ghosh",
      "Swagatam Das",
      "Debolina Paul",
      "Saptarshi Chakraborty"
    ],
    "abstract": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.CO",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T21:16:53+00:00",
    "updated": "2025-11-12T21:16:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14784v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09722v1",
    "title": "Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling",
    "authors": [
      "Sujay Nair",
      "Evan Coleman",
      "Sherrie Wang",
      "Elsa Olivetti"
    ],
    "abstract": "Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \\pm 0.01$ and recalls of $0.22 \\pm 0.02$ on test data at 1$\\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T20:28:40+00:00",
    "updated": "2025-11-12T20:28:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09722v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10859v1",
    "title": "Private Zeroth-Order Optimization with Public Data",
    "authors": [
      "Xuchen Gong",
      "Tian Li"
    ],
    "abstract": "One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\\times$ runtime speedup.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T23:51:24+00:00",
    "updated": "2025-11-13T23:51:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10859v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10824v1",
    "title": "Neural Local Wasserstein Regression",
    "authors": [
      "Inga Girshfeld",
      "Xiaohui Chen"
    ],
    "abstract": "We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \\emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T21:54:18+00:00",
    "updated": "2025-11-13T21:54:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10824v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10776v1",
    "title": "Potential Outcome Rankings for Counterfactual Decision Making",
    "authors": [
      "Yuta Kawakami",
      "Jin Tian"
    ],
    "abstract": "Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-13T19:58:19+00:00",
    "updated": "2025-11-13T19:58:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10776v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10619v1",
    "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem",
    "authors": [
      "Avrim Blum",
      "Marten Garicano",
      "Kavya Ravichandran",
      "Dravyansh Sharma"
    ],
    "abstract": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T18:46:56+00:00",
    "updated": "2025-11-13T18:46:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10619v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10446v2",
    "title": "Continuum Dropout for Neural Differential Equations",
    "authors": [
      "Jonghun Lee",
      "YongKyung Oh",
      "Sungil Kim",
      "Dong-Young Lim"
    ],
    "abstract": "Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T16:10:45+00:00",
    "updated": "2025-11-18T08:29:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10446v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10406v1",
    "title": "Diffusion annealed Langevin dynamics: a theoretical study",
    "authors": [
      "Patrick Cattiaux",
      "Paula Cordero-Encinar",
      "Arnaud Guillin"
    ],
    "abstract": "In this work we study the diffusion annealed Langevin dynamics, a score-based diffusion process recently introduced in the theory of generative models and which is an alternative to the classical overdamped Langevin diffusion. Our goal is to provide a rigorous construction and to study the theoretical efficiency of these models for general base distribution as well as target distribution. As a matter of fact these diffusion processes are a particular case of Nelson processes i.e. diffusion processes with a given flow of time marginals.   Providing existence and uniqueness of the solution to the annealed Langevin diffusion leads to proving a Poincaré inequality for the conditional distribution of $X$ knowing $X+Z=y$ uniformly in $y$, as recently observed by one of us and her coauthors. Part of this work is thus devoted to the study of such Poincaré inequalities. Additionally we show that strengthening the Poincaré inequality into a logarithmic Sobolev inequality improves the efficiency of the model.",
    "categories": [
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "math.PR",
    "published": "2025-11-13T15:26:42+00:00",
    "updated": "2025-11-13T15:26:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10406v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10383v1",
    "title": "Operator Models for Continuous-Time Offline Reinforcement Learning",
    "authors": [
      "Nicolas Hoischen",
      "Petar Bevanda",
      "Max Beier",
      "Stefan Sosnowski",
      "Boris Houska",
      "Sandra Hirche"
    ],
    "abstract": "Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T14:58:30+00:00",
    "updated": "2025-11-13T14:58:30+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10383v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10048v1",
    "title": "Masking criteria for selecting an imputation model",
    "authors": [
      "Yanjiao Yang",
      "Daniel Suen",
      "Yen-Chi Chen"
    ],
    "abstract": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-13T07:47:41+00:00",
    "updated": "2025-11-13T07:47:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10048v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09996v1",
    "title": "A Novel Data-Dependent Learning Paradigm for Large Hypothesis Classes",
    "authors": [
      "Alireza F. Pour",
      "Shai Ben-David"
    ],
    "abstract": "We address the general task of learning with a set of candidate models that is too large to have a uniform convergence of empirical estimates to true losses. While the common approach to such challenges is SRM (or regularization) based learning algorithms, we propose a novel learning paradigm that relies on stronger incorporation of empirical data and requires less algorithmic decisions to be based on prior assumptions. We analyze the generalization capabilities of our approach and demonstrate its merits in several common learning assumptions, including similarity of close points, clustering of the domain into highly label-homogeneous regions, Lipschitzness assumptions of the labeling rule, and contrastive learning assumptions. Our approach allows utilizing such assumptions without the need to know their true parameters a priori.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T06:02:55+00:00",
    "updated": "2025-11-13T06:02:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09996v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09925v2",
    "title": "Global Convergence of Four-Layer Matrix Factorization under Random Initialization",
    "authors": [
      "Minrui Luo",
      "Weihang Xu",
      "Xiang Gao",
      "Maryam Fazel",
      "Simon Shaolei Du"
    ],
    "abstract": "Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-13T03:40:10+00:00",
    "updated": "2025-11-19T15:50:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09925v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09902v1",
    "title": "Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling",
    "authors": [
      "Hossein Rouhvarzi",
      "Anastasis Kratsios"
    ],
    "abstract": "Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.   Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.",
    "categories": [
      "cs.LG",
      "math.CA",
      "math.DS",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-13T03:01:50+00:00",
    "updated": "2025-11-13T03:01:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09902v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09897v1",
    "title": "Theory and computation for structured variational inference",
    "authors": [
      "Shunan Sheng",
      "Bohan Wu",
      "Bennett Zhu",
      "Sinho Chewi",
      "Aram-Alexandre Pooladian"
    ],
    "abstract": "Structured variational inference constitutes a core methodology in modern statistical applications. Unlike mean-field variational inference, the approximate posterior is assumed to have interdependent structure. We consider the natural setting of star-structured variational inference, where a root variable impacts all the other ones. We prove the first results for existence, uniqueness, and self-consistency of the variational approximation. In turn, we derive quantitative approximation error bounds for the variational approximation to the posterior, extending prior work from the mean-field setting to the star-structured setting. We also develop a gradient-based algorithm with provable guarantees for computing the variational approximation using ideas from optimal transport theory. We explore the implications of our results for Gaussian measures and hierarchical Bayesian models, including generalized linear models with location family priors and spike-and-slab priors with one-dimensional debiasing. As a by-product of our analysis, we develop new stability results for star-separable transport maps which might be of independent interest.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-13T02:57:54+00:00",
    "updated": "2025-11-13T02:57:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09897v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09872v1",
    "title": "Randomized batch-sampling Kaczmarz methods for general linear systems",
    "authors": [
      "Dong-Yue Xie",
      "Xi Yang"
    ],
    "abstract": "To conduct a more in-depth investigation of randomized solvers for general linear systems, we adopt a unified randomized batch-sampling Kaczmarz framework with per-iteration costs as low as cyclic block methods, and develop a general analysis technique to establish its convergence guarantee. With concentration inequalities, we derive new expected linear convergence rate bounds. The analysis applies to any randomized non-extended block Kaczmarz methods with static stochastic samplings. In addition, the new rate bounds are scale-invariant which eliminate the dependence on the magnitude of the data matrix. In most experiments, the new bounds are significantly tighter than existing ones and better reflect the empirical convergence behavior of block methods. Within this new framework, the batch-sampling distribution, as a learnable parameter, provides the possibility for block methods to achieve efficient performance in specific application scenarios, which deserves further investigation.",
    "categories": [
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-13T02:10:35+00:00",
    "updated": "2025-11-13T02:10:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09872v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09845v2",
    "title": "Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints",
    "authors": [
      "Cac Phan",
      "Kai Wang"
    ],
    "abstract": "This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(δ, ε)$-Goldstein stationary points using $Θ(δ^{-1}ε^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.",
    "categories": [
      "math.OC",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-13T00:59:20+00:00",
    "updated": "2025-11-15T02:26:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09845v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09801v1",
    "title": "Generalized infinite dimensional Alpha-Procrustes based geometries",
    "authors": [
      "Salvish Goomanee",
      "Andi Han",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ],
    "abstract": "This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T23:05:14+00:00",
    "updated": "2025-11-12T23:05:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09801v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09759v1",
    "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport",
    "authors": [
      "Borna Bateni",
      "Yubai Yuan",
      "Qi Xu",
      "Annie Qu"
    ],
    "abstract": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-12T21:42:11+00:00",
    "updated": "2025-11-12T21:42:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09759v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.14784v1",
    "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator",
    "authors": [
      "Sourav De",
      "Koustav Chowdhury",
      "Bibhabasu Mandal",
      "Sagar Ghosh",
      "Swagatam Das",
      "Debolina Paul",
      "Saptarshi Chakraborty"
    ],
    "abstract": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.CO",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T21:16:53+00:00",
    "updated": "2025-11-12T21:16:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.14784v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09722v1",
    "title": "Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling",
    "authors": [
      "Sujay Nair",
      "Evan Coleman",
      "Sherrie Wang",
      "Elsa Olivetti"
    ],
    "abstract": "Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \\pm 0.01$ and recalls of $0.22 \\pm 0.02$ on test data at 1$\\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T20:28:40+00:00",
    "updated": "2025-11-12T20:28:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09722v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09677v1",
    "title": "Boosted GFlowNets: Improving Exploration via Sequential Learning",
    "authors": [
      "Pedro Dall'Antonia",
      "Tiago da Silva",
      "Daniel Augusto de Souza",
      "César Lincoln C. Mattos",
      "Diego Mesquita"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) are powerful samplers for compositional objects that, by design, sample proportionally to a given non-negative reward. Nonetheless, in practice, they often struggle to explore the reward landscape evenly: trajectories toward easy-to-reach regions dominate training, while hard-to-reach modes receive vanishing or uninformative gradients, leading to poor coverage of high-reward areas. We address this imbalance with Boosted GFlowNets, a method that sequentially trains an ensemble of GFlowNets, each optimizing a residual reward that compensates for the mass already captured by previous models. This residual principle reactivates learning signals in underexplored regions and, under mild assumptions, ensures a monotone non-degradation property: adding boosters cannot worsen the learned distribution and typically improves it. Empirically, Boosted GFlowNets achieve substantially better exploration and sample diversity on multimodal synthetic benchmarks and peptide design tasks, while preserving the stability and simplicity of standard trajectory-balance training.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-12T19:30:11+00:00",
    "updated": "2025-11-12T19:30:11+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09677v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09500v1",
    "title": "Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions",
    "authors": [
      "Tengyuan Liang"
    ],
    "abstract": "We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + σZ$, where $σ\\in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(σ^4)$ and $O(σ^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Ampère equation with higher-order accuracy, and can be implemented efficiently via score matching.   Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \\[ \\mathbf{T}^*(y) = y + σ^2 \\nabla \\log q(y), \\] with denoisers exhibiting less aggressive distributional shrinkage, \\[ \\mathbf{T}_1(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y), \\] \\[ \\mathbf{T}_2(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y) - \\frac{σ^4}{8} \\nabla \\left( \\frac{1}{2} \\| \\nabla \\log q(y) \\|^2 + \\nabla \\cdot \\nabla \\log q(y) \\right) . \\]",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T17:20:42+00:00",
    "updated": "2025-11-12T17:20:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09500v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09486v1",
    "title": "A general framework for adaptive nonparametric dimensionality reduction",
    "authors": [
      "Antonio Di Noia",
      "Federico Ravenda",
      "Antonietta Mira"
    ],
    "abstract": "Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T16:59:22+00:00",
    "updated": "2025-11-12T16:59:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09486v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09465v1",
    "title": "Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions",
    "authors": [
      "Hedwig Nora Nordlinder",
      "Lukas Billera",
      "Jack Collier Ryder",
      "Anton Oresten",
      "Aron Stålmarck",
      "Theodor Mosetti Björk",
      "Ben Murrell"
    ],
    "abstract": "Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori.   Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T16:31:12+00:00",
    "updated": "2025-11-12T16:31:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09465v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09425v1",
    "title": "Several Supporting Evidences for the Adaptive Feature Program",
    "authors": [
      "Yicheng Li",
      "Qian Lin"
    ],
    "abstract": "Theoretically exploring the advantages of neural networks might be one of the most challenging problems in the AI era. An adaptive feature program has recently been proposed to analyze the feature learning characteristic property of neural networks in a more abstract way. Motivated by the celebrated Le Cam equivalence, we advocate the over-parametrized sequence models to further simplify the analysis of the training dynamics of adaptive feature program and present several supporting evidences for the adaptive feature program. More precisely, after having introduced the feature error measure (FEM) to characterize the quality of the learned feature, we show that the FEM is decreasing during the training process of several concrete adaptive feature models including linear regression, single/multiple index models, etc. We believe that this hints at the potential successes of the adaptive feature program.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-12T15:39:28+00:00",
    "updated": "2025-11-12T15:39:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09425v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09216v1",
    "title": "Controllable protein design through Feynman-Kac steering",
    "authors": [
      "Erik Hartman",
      "Jonas Wallin",
      "Johan Malmström",
      "Jimmy Olsson"
    ],
    "abstract": "Diffusion-based models have recently enabled the generation of realistic and diverse protein structures, yet they remain limited in their ability to steer outcomes toward specific functional or biochemical objectives, such as binding affinity or sequence composition. Here we extend the Feynman-Kac (FK) steering framework, an inference-time control approach, to diffusion-based protein design. By coupling FK steering with structure generation, the method guides sampling toward desirable structural or energetic features while maintaining the diversity of the underlying diffusion process. To enable simultaneous generation of both sequence and structure properties, rewards are computed on models refined through ProteinMPNN and all-atom relaxation. Applied to binder design, FK steering consistently improves predicted interface energetics across diverse targets with minimal computational overhead. More broadly, this work demonstrates that inference-time FK control generalizes diffusion-based protein design to arbitrary, non-differentiable, and reward-agnostic objectives, providing a unified and model-independent framework for guided molecular generation.",
    "categories": [
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-12T11:23:44+00:00",
    "updated": "2025-11-12T11:23:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09216v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09118v1",
    "title": "Learning to Validate Generative Models: a Goodness-of-Fit Approach",
    "authors": [
      "Pietro Cappelli",
      "Gaia Grosso",
      "Marco Letizia",
      "Humberto Reyes-González",
      "Marco Zanetti"
    ],
    "abstract": "Generative models are increasingly central to scientific workflows, yet their systematic use and interpretation require a proper understanding of their limitations through rigorous validation. Classic approaches struggle with scalability, statistical power, or interpretability when applied to high-dimensional data, making it difficult to certify the reliability of these models in realistic, high-dimensional scientific settings. Here, we propose the use of the New Physics Learning Machine (NPLM), a learning based approach to goodness-of-fit testing inspired by the Neyman-Pearson construction, to test generative networks trained on high-dimensional scientific data. We demonstrate the performance of NPLM for validation in two benchmark cases: generative models trained on mixtures of Gaussian models with increasing dimensionality, and a public end-to-end generator for the Large Hadron Collider called FlashSim, trained on jet data, typical in the field of high-energy physics. We demonstrate that the NPLM can serve as a powerful validation method while also providing a means to diagnose sub-optimally modeled regions of the data.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "hep-ex",
      "hep-ph"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T08:47:08+00:00",
    "updated": "2025-11-12T08:47:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09118v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09577v1",
    "title": "Siegel Neural Networks",
    "authors": [
      "Xuan Son Nguyen",
      "Aymeric Histace",
      "Nistor Grozavu"
    ],
    "abstract": "Riemannian symmetric spaces (RSS) such as hyperbolic spaces and symmetric positive definite (SPD) manifolds have become popular spaces for representation learning. In this paper, we propose a novel approach for building discriminative neural networks on Siegel spaces, a family of RSS that is largely unexplored in machine learning tasks. For classification applications, one focus of recent works is the construction of multiclass logistic regression (MLR) and fully-connected (FC) layers for hyperbolic and SPD neural networks. Here we show how to build such layers for Siegel neural networks. Our approach relies on the quotient structure of those spaces and the notation of vector-valued distance on RSS. We demonstrate the relevance of our approach on two applications, i.e., radar clutter classification and node classification. Our results successfully demonstrate state-of-the-art performance across all datasets.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T07:47:46+00:00",
    "updated": "2025-11-12T07:47:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09577v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11682v1",
    "title": "Generalized Inequality-based Approach for Probabilistic WCET Estimation",
    "authors": [
      "Hayate Toba",
      "Atsushi Yano",
      "Takuya Azumi"
    ],
    "abstract": "Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T06:19:31+00:00",
    "updated": "2025-11-12T06:19:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11682v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09002v2",
    "title": "Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation",
    "authors": [
      "Hongru Zhao",
      "Jinwen Fu",
      "Tuan Pham"
    ],
    "abstract": "Self-consuming generative models have received significant attention over the last few years. In this paper, we study a self-consuming generative model with heterogeneous preferences that is a generalization of the model in Ferbach et al. (2024). The model is retrained round by round using real data and its previous-round synthetic outputs. The asymptotic behavior of the retraining dynamics is investigated across four regimes using different techniques including the nonlinear Perron--Frobenius theory. Our analyses improve upon that of Ferbach et al. (2024) and provide convergence results in settings where the well-known Banach contraction mapping arguments do not apply. Stability and non-stability results regarding the retraining dynamics are also given.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T05:46:17+00:00",
    "updated": "2025-11-13T04:47:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09002v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08991v1",
    "title": "Robust Sampling for Active Statistical Inference",
    "authors": [
      "Puheng Li",
      "Tijana Zrnic",
      "Emmanuel Candès"
    ],
    "abstract": "Active statistical inference is a new method for inference with AI-assisted data collection. Given a budget on the number of labeled data points that can be collected and assuming access to an AI predictive model, the basic idea is to improve estimation accuracy by prioritizing the collection of labels where the model is most uncertain. The drawback, however, is that inaccurate uncertainty estimates can make active sampling produce highly noisy results, potentially worse than those from naive uniform sampling. In this work, we present robust sampling strategies for active statistical inference. Robust sampling ensures that the resulting estimator is never worse than the estimator using uniform sampling. Furthermore, with reliable uncertainty estimates, the estimator usually outperforms standard active inference. This is achieved by optimally interpolating between uniform and active sampling, depending on the quality of the uncertainty scores, and by using ideas from robust optimization. We demonstrate the utility of the method on a series of real datasets from computational social science and survey research.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-12T05:18:36+00:00",
    "updated": "2025-11-12T05:18:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08991v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08878v1",
    "title": "Covariance Scattering Transforms",
    "authors": [
      "Andrea Cavallo",
      "Ayushman Raghuvanshi",
      "Sundeep Prabhakar Chepuri",
      "Elvin Isufi"
    ],
    "abstract": "Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-12T01:29:31+00:00",
    "updated": "2025-11-12T01:29:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08878v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08855v1",
    "title": "Path Signatures Enable Model-Free Mapping of RNA Modifications",
    "authors": [
      "Maud Lemercier",
      "Paola Arrubarrena",
      "Salvatore Di Giorgio",
      "Julia Brettschneider",
      "Thomas Cass",
      "Isabel S. Naarmann-de Vries",
      "Anastasia Papavasiliou",
      "Alessia Ruggieri",
      "Irem Tellioglu",
      "Chia Ching Wu",
      "F. Nina Papavasiliou",
      "Terry Lyons"
    ],
    "abstract": "Detecting chemical modifications on RNA molecules remains a key challenge in epitranscriptomics. Traditional reverse transcription-based sequencing methods introduce enzyme- and sequence-dependent biases and fragment RNA molecules, confounding the accurate mapping of modifications across the transcriptome. Nanopore direct RNA sequencing offers a powerful alternative by preserving native RNA molecules, enabling the detection of modifications at single-molecule resolution. However, current computational tools can identify only a limited subset of modification types within well-characterized sequence contexts for which ample training data exists. Here, we introduce a model-free computational method that reframes modification detection as an anomaly detection problem, requiring only canonical (unmodified) RNA reads without any other annotated data. For each nanopore read, our approach extracts robust, modification-sensitive features from the raw ionic current signal at a site using the signature transform, then computes an anomaly score by comparing the resulting feature vector to its nearest neighbors in an unmodified reference dataset. We convert anomaly scores into statistical p-values to enable anomaly detection at both individual read and site levels. Validation on densely-modified \\textit{E. coli} rRNA demonstrates that our approach detects known sites harboring diverse modification types, without prior training on these modifications. We further applyied this framework to dengue virus (DENV) transcripts and mammalian mRNAs. For DENV sfRNA, it led to revealing a novel 2'-O-methylated site, which we validate orthogonally by qRT-PCR assays. These results demonstrate that our model-free approach operates robustly across different types of RNAs and datasets generated with different nanopore sequencing chemistries.",
    "categories": [
      "q-bio.GN",
      "stat.ML"
    ],
    "primary_category": "q-bio.GN",
    "published": "2025-11-12T00:22:56+00:00",
    "updated": "2025-11-12T00:22:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08855v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08846v1",
    "title": "On topological descriptors for graph products",
    "authors": [
      "Mattie Ji",
      "Amauri H. Souza",
      "Vikas Garg"
    ],
    "abstract": "Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at https://github.com/Aalto-QuML/tda_graph_product.",
    "categories": [
      "cs.LG",
      "math.AT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-12T00:00:34+00:00",
    "updated": "2025-11-12T00:00:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08846v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08808v1",
    "title": "Effects of label noise on the classification of outlier observations",
    "authors": [
      "Matheus Vinícius Barreto de Farias",
      "Mario de Castro"
    ],
    "abstract": "This study investigates the impact of adding noise to the training set classes in classification tasks using the BCOPS algorithm (Balanced and Conformal Optimized Prediction Sets), proposed by Guan & Tibshirani (2022). The BCOPS algorithm is an application of conformal prediction combined with a machine learning method to construct prediction sets such that the probability of the true class being included in the prediction set for a test observation meets a specified coverage guarantee. An observation is considered an outlier if its true class is not present in the training set. The study employs both synthetic and real datasets and conducts experiments to evaluate the prediction abstention rate for outlier observations and the model's robustness in this previously untested scenario. The results indicate that the addition of noise, even in small amounts, can have a significant effect on model performance.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T22:22:54+00:00",
    "updated": "2025-11-11T22:22:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08808v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08791v1",
    "title": "The Probably Approximately Correct Learning Model in Computational Learning Theory",
    "authors": [
      "Rocco A. Servedio"
    ],
    "abstract": "This survey paper gives an overview of various known results on learning classes of Boolean functions in Valiant's Probably Approximately Correct (PAC) learning model and its commonly studied variants.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T21:23:39+00:00",
    "updated": "2025-11-11T21:23:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08791v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08789v1",
    "title": "A Generalized Bias-Variance Decomposition for Bregman Divergences",
    "authors": [
      "David Pfau"
    ],
    "abstract": "The bias-variance decomposition is a central result in statistics and machine learning, but is typically presented only for the squared error. We present a generalization of the bias-variance decomposition where the prediction error is a Bregman divergence, which is relevant to maximum likelihood estimation with exponential families. While the result is already known, there was not previously a clear, standalone derivation, so we provide one for pedagogical purposes. A version of this note previously appeared on the author's personal website without context. Here we provide additional discussion and references to the relevant prior literature.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T21:22:01+00:00",
    "updated": "2025-11-11T21:22:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08789v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.09573v1",
    "title": "Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost",
    "authors": [
      "Valentino F. Foit",
      "David W. Hogg",
      "Soledad Villar"
    ],
    "abstract": "Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T21:10:09+00:00",
    "updated": "2025-11-11T21:10:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.09573v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08719v1",
    "title": "Practical considerations when designing an online learning algorithm for an app-based mHealth intervention",
    "authors": [
      "Rachel T Gonzalez",
      "Madeline R Abbott",
      "Brahmajee Nallamothu",
      "Scott Hummel",
      "Michael Dorsch",
      "Walter Dempsey"
    ],
    "abstract": "The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data.",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-11T19:27:35+00:00",
    "updated": "2025-11-11T19:27:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08719v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08717v2",
    "title": "Optimal control of the future via prospective learning with control",
    "authors": [
      "Yuxin Bai",
      "Aranyak Acharyya",
      "Ashwin De Silva",
      "Zeyu Shen",
      "James Hassett",
      "Joshua T. Vogelstein"
    ],
    "abstract": "Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in either reinforcement learning (RL). While powerful, this learning framework is mathematically distinct from supervised learning, which has been the main workhorse for the recent achievements in AI. Moreover, RL typically operates in a stationary environment with episodic resets, limiting its utility to more realistic settings. Here, we extend supervised learning to address learning to control in non-stationary, reset-free environments. Using this framework, called ''Prospective Learning with Control (PL+C)'', we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective learning with control, foraging -- which is a canonical task for any mobile agent -- be it natural or artificial. We illustrate that modern RL algorithms fail to learn in these non-stationary reset-free environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T19:27:14+00:00",
    "updated": "2025-11-19T17:25:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08717v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08667v1",
    "title": "TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models",
    "authors": [
      "Léo Grinsztajn",
      "Klemens Flöge",
      "Oscar Key",
      "Felix Birkel",
      "Philipp Jund",
      "Brendan Roof",
      "Benjamin Jäger",
      "Dominik Safaric",
      "Simone Alessi",
      "Adrian Hayler",
      "Mihir Manium",
      "Rosen Yu",
      "Felix Jablonski",
      "Shi Bin Hoo",
      "Anurag Garg",
      "Jake Robertson",
      "Magnus Bühler",
      "Vladyslav Moroshan",
      "Lennart Purucker",
      "Clara Cornu",
      "Lilly Charlotte Wehrhahn",
      "Alessandro Bonetto",
      "Bernhard Schölkopf",
      "Sauraj Gambhir",
      "Noah Hollmann",
      "Frank Hutter"
    ],
    "abstract": "The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (<=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T18:57:15+00:00",
    "updated": "2025-11-11T18:57:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08667v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08544v3",
    "title": "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "abstract": "Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\\href{https://github.com/rbalestr-lab/lejepa}{GitHub repo}).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T18:21:55+00:00",
    "updated": "2025-11-14T08:38:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08544v3",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08401v1",
    "title": "Source-Optimal Training is Transfer-Suboptimal",
    "authors": [
      "C. Evans Hedges"
    ],
    "abstract": "We prove a fundamental misalignment in transfer learning: the source regularization that minimizes source risk almost never coincides with the regularization maximizing transfer benefit. Through sharp phase boundaries for L2-SP ridge regression, we characterize the transfer-optimal source penalty $τ_0^*$ and show it diverges predictably from task-optimal values, requiring stronger regularization in high-SNR regimes and weaker regularization in low-SNR regimes. Additionally, in isotropic settings the decision to transfer is remarkably independent of target sample size and noise, depending only on task alignment and source characteristics. CIFAR-10 and MNIST experiments confirm this counterintuitive pattern persists in non-linear networks.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T16:16:10+00:00",
    "updated": "2025-11-11T16:16:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08401v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08307v1",
    "title": "Concentration bounds on response-based vector embeddings of black-box generative models",
    "authors": [
      "Aranyak Acharyya",
      "Joshua Agterberg",
      "Youngser Park",
      "Carey E. Priebe"
    ],
    "abstract": "Generative models, such as large language models or text-to-image diffusion models, can generate relevant responses to user-given queries. Response-based vector embeddings of generative models facilitate statistical analysis and inference on a given collection of black-box generative models. The Data Kernel Perspective Space embedding is one particular method of obtaining response-based vector embeddings for a given set of generative models, already discussed in the literature. In this paper, under appropriate regularity conditions, we establish high probability concentration bounds on the sample vector embeddings for a given set of generative models, obtained through the method of Data Kernel Perspective Space embedding. Our results tell us the required number of sample responses needed in order to approximate the population-level vector embeddings with a desired level of accuracy. The algebraic tools used to establish our results can be used further for establishing concentration bounds on Classical Multidimensional Scaling embeddings in general, when the dissimilarities are observed with noise.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T14:37:05+00:00",
    "updated": "2025-11-11T14:37:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08307v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08303v1",
    "title": "Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression",
    "authors": [
      "Masahiro Kato"
    ],
    "abstract": "This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T14:35:18+00:00",
    "updated": "2025-11-11T14:35:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08303v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.11666v2",
    "title": "Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks",
    "authors": [
      "Rajit Rajpal",
      "Benedict Leimkuhler",
      "Yuanhao Jiang"
    ],
    "abstract": "Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T13:15:17+00:00",
    "updated": "2025-11-18T10:17:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.11666v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08180v1",
    "title": "Simulation-Based Fitting of Intractable Models via Sequential Sampling and Local Smoothing",
    "authors": [
      "Guido Masarotto"
    ],
    "abstract": "This paper presents a comprehensive algorithm for fitting generative models whose likelihood, moments, and other quantities typically used for inference are not analytically or numerically tractable. The proposed method aims to provide a general solution that requires only limited prior information on the model parameters. The algorithm combines a global search phase, aimed at identifying the region of the solution, with a local search phase that mimics a trust region version of the Fisher scoring algorithm for computing a quasi-likelihood estimator. Comparisons with alternative methods demonstrate the strong performance of the proposed approach. An R package implementing the algorithm is available on CRAN.",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-11T12:43:18+00:00",
    "updated": "2025-11-11T12:43:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08180v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08136v2",
    "title": "SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories",
    "authors": [
      "Returaj Burnwal",
      "Nirav Pravinbhai Bhatt",
      "Balaraman Ravindran"
    ],
    "abstract": "In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via Multiple Instance Learning. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T11:44:20+00:00",
    "updated": "2025-11-14T11:35:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08136v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08097v1",
    "title": "Model Predictive Control is almost Optimal for Heterogeneous Restless Multi-armed Bandits",
    "authors": [
      "Dheeraj Narasimha",
      "Nicolas Gast"
    ],
    "abstract": "We consider a general infinite horizon Heterogeneous Restless multi-armed Bandit (RMAB). Heterogeneity is a fundamental problem for many real-world systems largely because it resists many concentration arguments. In this paper, we assume that each of the $N$ arms can have different model parameters. We show that, under a mild assumption of uniform ergodicity, a natural finite-horizon LP-update policy with randomized rounding, that was originally proposed for the homogeneous case, achieves an $O(\\log N\\sqrt{1/N})$ optimality gap in infinite time average reward problems for fully heterogeneous RMABs. In doing so, we show results that provide strong theoretical guarantees on a well-known algorithm that works very well in practice. The LP-update policy is a model predictive approach that computes a decision at time $t$ by planing over a time-horizon $\\{t\\dots t+τ\\}$. Our simulation section demonstrates that our algorithm works extremely well even when $τ$ is very small and set to $5$, which makes it computationally efficient. Our theoretical results draw on techniques from the model predictive control literature by invoking the concept of \\emph{dissipativity} and generalize quite easily to the more general weakly coupled heterogeneous Markov Decision Process setting. In addition, we draw a parallel between our own policy and the LP-index policy by showing that the LP-index policy corresponds to $τ=1$. We describe where the latter's shortcomings arise from and how under our mild assumption we are able to address these shortcomings. The proof of our main theorem answers an open problem posed by (Brown et al 2020), paving the way for several new questions on the LP-update policies.",
    "categories": [
      "math.OC",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-11T10:53:49+00:00",
    "updated": "2025-11-11T10:53:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08097v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.08073v1",
    "title": "Online Linear Regression with Paid Stochastic Features",
    "authors": [
      "Nadav Merlis",
      "Kyoungseok Jang",
      "Nicolò Cesa-Bianchi"
    ],
    "abstract": "We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T10:19:24+00:00",
    "updated": "2025-11-11T10:19:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.08073v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07997v1",
    "title": "PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure",
    "authors": [
      "Ke Jia",
      "Yuheng Ma",
      "Yang Li",
      "Feifei Wang"
    ],
    "abstract": "We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.",
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T09:00:51+00:00",
    "updated": "2025-11-11T09:00:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07997v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07831v1",
    "title": "Distributionally Robust Online Markov Game with Linear Function Approximation",
    "authors": [
      "Zewu Zheng",
      "Yuanyuan Lin"
    ],
    "abstract": "The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \\episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve ε-approximate CCE with a regret bound of O{dHmin{H,1/min{σ_i}}\\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \\simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-11T04:56:39+00:00",
    "updated": "2025-11-11T04:56:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07831v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10680v1",
    "title": "LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices",
    "authors": [
      "Jean-Philippe Lignier"
    ],
    "abstract": "Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-11T03:55:06+00:00",
    "updated": "2025-11-11T03:55:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10680v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07786v2",
    "title": "A Closed-Form Framework for Schrödinger Bridges Between Arbitrary Densities",
    "authors": [
      "Hanwen Huang"
    ],
    "abstract": "Score-based generative models have recently attracted significant attention for their ability to generate high-fidelity data by learning maps from simple Gaussian priors to complex data distributions. A natural generalization of this idea to transformations between arbitrary probability distributions leads to the Schrödinger Bridge (SB) problem. However, SB solutions rarely admit closed-form expressios and are commonly obtained through iterative stochastic simulation procedures, which are computationally intensive and can be unstable. In this work, we introduce a unified closed-form framework for representing the stochastic dynamics of SB systems. Our formulation subsumes previously known analytical solutions including the Schrödinger Föllmer process and the Gaussian SB as specific instances. Notably, the classical Gaussian SB solution, previously derived using substantially more sophisticated tools such as Riemannian geometry and generator theory, follows directly from our formulation as an immediate corollary. Leveraging this framework, we develop a simulation-free algorithm that infers SB dynamics directly from samples of the source and target distributions. We demonstrate the versatility of our approach in two settings: (i) modeling developmental trajectories in single-cell genomics and (ii) solving image restoration tasks such as inpainting and deblurring. This work opens a new direction for efficient and scalable nonlinear diffusion modeling across scientific and machine learning applications.",
    "categories": [
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.CO",
    "published": "2025-11-11T03:08:26+00:00",
    "updated": "2025-11-17T21:08:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07786v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07671v1",
    "title": "Robust Experimental Design via Generalised Bayesian Inference",
    "authors": [
      "Yasir Zubayr Barlas",
      "Sabina J. Sloman",
      "Samuel Kaski"
    ],
    "abstract": "Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T22:25:01+00:00",
    "updated": "2025-11-10T22:25:01+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07671v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07604v1",
    "title": "Infinite-Dimensional Operator/Block Kaczmarz Algorithms: Regret Bounds and $λ$-Effectiveness",
    "authors": [
      "Halyun Jeong",
      "Palle E. T. Jorgensen",
      "Hyun-Kyoung Kwon",
      "Myung-Sin Song"
    ],
    "abstract": "We present a variety of projection-based linear regression algorithms with a focus on modern machine-learning models and their algorithmic performance. We study the role of the relaxation parameter in generalized Kaczmarz algorithms and establish a priori regret bounds with explicit $λ$-dependence to quantify how much an algorithm's performance deviates from its optimal performance. A detailed analysis of relaxation parameter is also provided. Applications include: explicit regret bounds for the framework of Kaczmarz algorithm models, non-orthogonal Fourier expansions, and the use of regret estimates in modern machine learning models, including for noisy data, i.e., regret bounds for the noisy Kaczmarz algorithms. Motivated by machine-learning practice, our wider framework treats bounded operators (on infinite-dimensional Hilbert spaces), with updates realized as (block) Kaczmarz algorithms, leading to new and versatile results.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T20:21:32+00:00",
    "updated": "2025-11-10T20:21:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07604v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07585v1",
    "title": "LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows",
    "authors": [
      "Raffi Khatchadourian",
      "Rolando Franco"
    ],
    "abstract": "Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.   Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.   We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T19:54:00+00:00",
    "updated": "2025-11-10T19:54:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07585v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07417v1",
    "title": "Language Generation with Infinite Contamination",
    "authors": [
      "Anay Mehrotra",
      "Grigoris Velegkas",
      "Xifan Yu",
      "Felix Zhou"
    ],
    "abstract": "We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.   Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).   We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.   Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T18:59:39+00:00",
    "updated": "2025-11-10T18:59:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07417v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07414v1",
    "title": "Wasserstein-Cramér-Rao Theory of Unbiased Estimation",
    "authors": [
      "Nicolás García Trillos",
      "Adam Quinn Jaffe",
      "Bodhisattva Sen"
    ],
    "abstract": "The quantity of interest in the classical Cramér-Rao theory of unbiased estimation (e.g., the Cramér-Rao lower bound, its exact attainment for exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the \"sensitivity\" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cramér-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and some concrete results that show that the Wasserstein projection estimator achieves the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing entirely new estimators.",
    "categories": [
      "math.ST",
      "math.OC",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-10T18:58:18+00:00",
    "updated": "2025-11-10T18:58:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07414v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07398v1",
    "title": "Solving bilevel optimization via sequential minimax optimization",
    "authors": [
      "Zhaosong Lu",
      "Sanyou Mei"
    ],
    "abstract": "In this paper we propose a sequential minimax optimization (SMO) method for solving a class of constrained bilevel optimization problems in which the lower-level part is a possibly nonsmooth convex optimization problem, while the upper-level part is a possibly nonconvex optimization problem. Specifically, SMO applies a first-order method to solve a sequence of minimax subproblems, which are obtained by employing a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems. Under suitable assumptions, we establish an operation complexity of $O(\\varepsilon^{-7}\\log\\varepsilon^{-1})$ and $O(\\varepsilon^{-6}\\log\\varepsilon^{-1})$, measured in terms of fundamental operations, for SMO in finding an $\\varepsilon$-KKT solution of the bilevel optimization problems with merely convex and strongly convex lower-level objective functions, respectively. The latter result improves the previous best-known operation complexity by a factor of $\\varepsilon^{-1}$. Preliminary numerical results demonstrate significantly superior computational performance compared to the recently developed first-order penalty method.",
    "categories": [
      "math.OC",
      "cs.LG",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-10T18:51:05+00:00",
    "updated": "2025-11-10T18:51:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07398v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07378v1",
    "title": "Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization",
    "authors": [
      "Yu Huang",
      "Zixin Wen",
      "Aarti Singh",
      "Yuejie Chi",
      "Yuxin Chen"
    ],
    "abstract": "The ability to reason lies at the core of artificial intelligence (AI), and challenging problems usually call for deeper and longer reasoning to tackle. A crucial question about AI reasoning is whether models can extrapolate learned reasoning patterns to solve harder tasks with longer chain-of-thought (CoT). In this work, we present a theoretical analysis of transformers learning on synthetic state-tracking tasks with gradient descent. We mathematically prove how the algebraic structure of state-tracking problems governs the degree of extrapolation of the learned CoT. Specifically, our theory characterizes the length generalization of transformers through the mechanism of attention concentration, linking the retrieval robustness of the attention layer to the state-tracking task structure of long-context reasoning. Moreover, for transformers with limited reasoning length, we prove that a recursive self-training scheme can progressively extend the range of solvable problem lengths. To our knowledge, we provide the first optimization guarantee that constant-depth transformers provably learn $\\mathsf{NC}^1$-complete problems with CoT, significantly going beyond prior art confined in $\\mathsf{TC}^0$, unless the widely held conjecture $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$ fails. Finally, we present a broad set of experiments supporting our theoretical results, confirming the length generalization behaviors and the mechanism of attention concentration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T18:40:24+00:00",
    "updated": "2025-11-10T18:40:24+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07378v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07365v1",
    "title": "Private Sketches for Linear Regression",
    "authors": [
      "Shrutimoy Das",
      "Debanuj Nayak",
      "Anirban Dasgupta"
    ],
    "abstract": "Linear regression is frequently applied in a variety of domains. In order to improve the efficiency of these methods, various methods have been developed that compute summaries or \\emph{sketches} of the datasets. Certain domains, however, contain sensitive data which necessitates that the application of these statistical methods does not reveal private information. Differentially private (DP) linear regression methods have been developed for mitigating this problem. These techniques typically involve estimating a noisy version of the parameter vector. Instead, we propose releasing private sketches of the datasets. We present differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T18:22:40+00:00",
    "updated": "2025-11-10T18:22:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07365v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07504v1",
    "title": "Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids",
    "authors": [
      "Raymond Zhang",
      "Hédi Hadiji",
      "Richard Combes"
    ],
    "abstract": "We consider the maximization of $x^\\top θ$ over $(x,θ) \\in \\mathcal{X} \\times Θ$, with $\\mathcal{X} \\subset \\mathbb{R}^d$ convex and $Θ\\subset \\mathbb{R}^d$ an ellipsoid. This problem is fundamental in linear bandits, as the learner must solve it at every time step using optimistic algorithms. We first show that for some sets $\\mathcal{X}$ e.g. $\\ell_p$ balls with $p>2$, no efficient algorithms exist unless $\\mathcal{P} = \\mathcal{NP}$. We then provide two novel algorithms solving this problem efficiently when $\\mathcal{X}$ is a centered ellipsoid. Our findings provide the first known method to implement optimistic algorithms for linear bandits in high dimensions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T17:22:34+00:00",
    "updated": "2025-11-10T17:22:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07504v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07272v1",
    "title": "Understanding the role of depth in the neural tangent kernel for overparameterized neural networks",
    "authors": [
      "William St-Arnaud",
      "Margarida Carvalho",
      "Golnoosh Farnadi"
    ],
    "abstract": "Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T16:18:04+00:00",
    "updated": "2025-11-10T16:18:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07272v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07270v1",
    "title": "High-Dimensional Asymptotics of Differentially Private PCA",
    "authors": [
      "Youngjoo Yun",
      "Rishabh Dudeja"
    ],
    "abstract": "In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\\rightarrow\\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.",
    "categories": [
      "math.ST",
      "cs.IT",
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-10T16:17:16+00:00",
    "updated": "2025-11-10T16:17:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07270v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07261v1",
    "title": "High-dimensional Bayesian filtering through deep density approximation",
    "authors": [
      "Kasper Bågmark",
      "Filip Rydin"
    ],
    "abstract": "In this work, we benchmark two recently developed deep density methods for nonlinear filtering. Starting from the Fokker--Planck equation with Bayes updates, we model the filtering density of a discretely observed SDE. The two filters: the deep splitting filter and the deep BSDE filter, are both based on Feynman--Kac formulas, Euler--Maruyama discretizations and neural networks. The two methods are extended to logarithmic formulations providing sound and robust implementations in increasing state dimension. Comparing to the classical particle filters and ensemble Kalman filters, we benchmark the methods on numerous examples. In the low-dimensional examples the particle filters work well, but when we scale up to a partially observed 100-dimensional Lorenz-96 model the particle-based methods fail and the logarithmic deep density method prevails. In terms of computational efficiency, the deep density methods reduce inference time by roughly two to five orders of magnitude relative to the particle-based filters.",
    "categories": [
      "math.NA",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-10T16:06:31+00:00",
    "updated": "2025-11-10T16:06:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07261v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07197v1",
    "title": "Simulation-based Methods for Optimal Sampling Design in Systems Biology",
    "authors": [
      "Tuan Minh Ha",
      "Binh Thanh Nguyen",
      "Lam Si Tung Ho"
    ],
    "abstract": "In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T15:26:51+00:00",
    "updated": "2025-11-10T15:26:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07197v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07496v1",
    "title": "Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models",
    "authors": [
      "Barath Chandran. C",
      "Srinivas Anumasa",
      "Dianbo Liu"
    ],
    "abstract": "Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high-dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-10T14:16:31+00:00",
    "updated": "2025-11-10T14:16:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07496v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07109v1",
    "title": "A Provably-Correct and Robust Convex Model for Smooth Separable NMF",
    "authors": [
      "Junjun Pan",
      "Valentin Leplat",
      "Michael Ng",
      "Nicolas Gillis"
    ],
    "abstract": "Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets.",
    "categories": [
      "math.NA",
      "cs.LG",
      "eess.SP",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "math.NA",
    "published": "2025-11-10T13:54:27+00:00",
    "updated": "2025-11-10T13:54:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07109v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07032v1",
    "title": "Fair Bayesian Data Selection via Generalized Discrepancy Measures",
    "authors": [
      "Yixuan Zhang",
      "Jiabin Luo",
      "Zhenggang Wang",
      "Feng Zhou",
      "Quyu Kong"
    ],
    "abstract": "Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T12:28:04+00:00",
    "updated": "2025-11-10T12:28:04+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07032v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06967v1",
    "title": "Approximate Bayesian inference for cumulative probit regression models",
    "authors": [
      "Emanuele Aliverti"
    ],
    "abstract": "Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network.",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-10T11:15:42+00:00",
    "updated": "2025-11-10T11:15:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06967v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06895v1",
    "title": "On The Presence of Double-Descent in Deep Reinforcement Learning",
    "authors": [
      "Viktor Veselý",
      "Aleksandar Todorov",
      "Matthia Sabatelli"
    ],
    "abstract": "The double descent (DD) paradox, where over-parameterized models see generalization improve past the interpolation point, remains largely unexplored in the non-stationary domain of Deep Reinforcement Learning (DRL). We present preliminary evidence that DD exists in model-free DRL, investigating it systematically across varying model capacity using the Actor-Critic framework. We rely on an information-theoretic metric, Policy Entropy, to measure policy uncertainty throughout training. Preliminary results show a clear epoch-wise DD curve; the policy's entrance into the second descent region correlates with a sustained, significant reduction in Policy Entropy. This entropic decay suggests that over-parameterization acts as an implicit regularizer, guiding the policy towards robust, flatter minima in the loss landscape. These findings establish DD as a factor in DRL and provide an information-based mechanism for designing agents that are more general, transferable, and robust.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T09:45:03+00:00",
    "updated": "2025-11-10T09:45:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06895v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06854v2",
    "title": "Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning",
    "authors": [
      "Jiexi Liu",
      "Meng Cao",
      "Songcan Chen"
    ],
    "abstract": "Irregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T08:53:10+00:00",
    "updated": "2025-11-15T13:23:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06854v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06794v1",
    "title": "Beyond Uniform Deletion: A Data Value-Weighted Framework for Certified Machine Unlearning",
    "authors": [
      "Lisong He",
      "Yi Yang",
      "Xiangyu Chang"
    ],
    "abstract": "As the right to be forgotten becomes legislated worldwide, machine unlearning mechanisms have emerged to efficiently update models for data deletion and enhance user privacy protection. However, existing machine unlearning algorithms frequently neglect the fact that different data points may contribute unequally to model performance (i.e., heterogeneous data values). Treat them equally in machine unlearning procedure can potentially degrading the performance of updated models. To address this limitation, we propose Data Value-Weighted Unlearning (DVWU), a general unlearning framework that accounts for data value heterogeneity into the unlearning process. Specifically, we design a weighting strategy based on data values, which are then integrated into the unlearning procedure to enable differentiated unlearning for data points with varying utility to the model. The DVWU framework can be broadly adapted to various existing machine unlearning methods. We use the one-step Newton update as an example for implementation, developing both output and objective perturbation algorithms to achieve certified unlearning. Experiments on both synthetic and real-world datasets demonstrate that our methods achieve superior predictive performance and robustness compared to conventional unlearning approaches. We further show the extensibility of our framework on gradient ascent method by incorporating the proposed weighting strategy into the gradient terms, highlighting the adaptability of DVWU for broader gradient-based deep unlearning methods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T07:32:29+00:00",
    "updated": "2025-11-10T07:32:29+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06794v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06790v1",
    "title": "Robust Causal Discovery under Imperfect Structural Constraints",
    "authors": [
      "Zidong Wang",
      "Xi Lin",
      "Chuchao He",
      "Xiaoguang Gao"
    ],
    "abstract": "Robust causal discovery from observational data under imperfect prior knowledge remains a significant and largely unresolved challenge. Existing methods typically presuppose perfect priors or can only handle specific, pre-identified error types. And their performance degrades substantially when confronted with flawed constraints of unknown location and type. This decline arises because most of them rely on inflexible and biased thresholding strategies that may conflict with the data distribution. To overcome these limitations, we propose to harmonizes knowledge and data through prior alignment and conflict resolution. First, we assess the credibility of imperfect structural constraints through a surrogate model, which then guides a sparse penalization term measuring the loss between the learned and constrained adjacency matrices. We theoretically prove that, under ideal assumption, the knowledge-driven objective aligns with the data-driven objective. Furthermore, to resolve conflicts when this assumption is violated, we introduce a multi-task learning framework optimized via multi-gradient descent, jointly minimizing both objectives. Our proposed method is robust to both linear and nonlinear settings. Extensive experiments, conducted under diverse noise conditions and structural equation model types, demonstrate the effectiveness and efficiency of our method under imperfect structural constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T07:27:08+00:00",
    "updated": "2025-11-10T07:27:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06790v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07486v1",
    "title": "Provably Efficient Sample Complexity for Robust CMDP",
    "authors": [
      "Sourav Ganguly",
      "Arnob Ghosh"
    ],
    "abstract": "We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\\mathcal{\\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T04:40:37+00:00",
    "updated": "2025-11-10T04:40:37+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07486v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06698v2",
    "title": "Lassoed Forests: Random Forests with Adaptive Lasso Post-selection",
    "authors": [
      "Jing Shang",
      "James Bannon",
      "Benjamin Haibe-Kains",
      "Robert Tibshirani"
    ],
    "abstract": "Random forests are a statistical learning technique that use bootstrap aggregation to average high-variance and low-bias trees. Improvements to random forests, such as applying Lasso regression to the tree predictions, have been proposed in order to reduce model bias. However, these changes can sometimes degrade performance (e.g., an increase in mean squared error). In this paper, we show in theory that the relative performance of these two methods, standard and Lasso-weighted random forests, depends on the signal-to-noise ratio. We further propose a unified framework to combine random forests and Lasso selection by applying adaptive weighting and show mathematically that it can strictly outperform the other two methods. We compare the three methods through simulation, including bias-variance decomposition, error estimates evaluation, and variable importance analysis. We also show the versatility of our method by applications to a variety of real-world datasets.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T04:39:09+00:00",
    "updated": "2025-11-11T20:58:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06698v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06674v1",
    "title": "Modeling and Topology Estimation of Low Rank Dynamical Networks",
    "authors": [
      "Wenqi Cao",
      "Aming Li"
    ],
    "abstract": "Conventional topology learning methods for dynamical networks become inapplicable to processes exhibiting low-rank characteristics. To address this, we propose the low rank dynamical network model which ensures identifiability. By employing causal Wiener filtering, we establish a necessary and sufficient condition that links the sparsity pattern of the filter to conditional Granger causality. Building on this theoretical result, we develop a consistent method for estimating all network edges. Simulation results demonstrate the parsimony of the proposed framework and consistency of the topology estimation approach.",
    "categories": [
      "cs.GR",
      "stat.ML"
    ],
    "primary_category": "cs.GR",
    "published": "2025-11-10T03:42:35+00:00",
    "updated": "2025-11-10T03:42:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06674v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06645v1",
    "title": "Adaptive Testing for Segmenting Watermarked Texts From Language Models",
    "authors": [
      "Xingchi Li",
      "Xiaochi Liu",
      "Guanxun Li"
    ],
    "abstract": "The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.",
    "categories": [
      "stat.ML",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-10T02:50:19+00:00",
    "updated": "2025-11-10T02:50:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06645v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06641v1",
    "title": "Neyman-Pearson Classification under Both Null and Alternative Distributions Shift",
    "authors": [
      "Mohammadreza M. Kalan",
      "Yuyang Deng",
      "Eitan J. Neugut",
      "Samory Kpotufe"
    ],
    "abstract": "We consider the problem of transfer learning in Neyman-Pearson classification, where the objective is to minimize the error w.r.t. a distribution $μ_1$, subject to the constraint that the error w.r.t. a distribution $μ_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman-Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $μ_1$, whereas in many practical scenarios shifts may occur in both $μ_0$ and $μ_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-10T02:38:27+00:00",
    "updated": "2025-11-10T02:38:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06641v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06639v1",
    "title": "Bernstein-von Mises for Adaptively Collected Data",
    "authors": [
      "Kevin Du",
      "Yash Nair",
      "Lucas Janson"
    ],
    "abstract": "Uncertainty quantification (UQ) for adaptively collected data, such as that coming from adaptive experiments, bandits, or reinforcement learning, is necessary for critical elements of data collection such as ensuring safety and conducting after-study inference. The data's adaptivity creates significant challenges for frequentist UQ, yet Bayesian UQ remains the same as if the data were independent and identically distributed (i.i.d.), making it an appealing and commonly used approach. Bayesian UQ requires the (correct) specification of a prior distribution while frequentist UQ does not, but for i.i.d. data the celebrated Bernstein-von Mises theorem shows that as the sample size grows, the prior 'washes out' and Bayesian UQ becomes frequentist-valid, implying that the choice of prior need not be a major impediment to Bayesian UQ as it makes no difference asymptotically. This paper for the first time extends the Bernstein-von Mises theorem to adaptively collected data, proving asymptotic equivalence between Bayesian UQ and Wald-type frequentist UQ in this challenging setting. Our result showing this asymptotic agreement does not require the standard stability condition required by works studying validity of Wald-type frequentist UQ; in cases where stability is satisfied, our results combined with these prior studies of frequentist UQ imply frequentist validity of Bayesian UQ. Counterintuitively however, they also provide a negative result that Bayesian UQ is not asymptotically frequentist valid when stability fails, despite the fact that the prior washes out and Bayesian UQ asymptotically matches standard Wald-type frequentist UQ. We empirically validate our theory (positive and negative) via a range of simulations.",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-10T02:36:13+00:00",
    "updated": "2025-11-10T02:36:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06639v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06585v1",
    "title": "Learning Biomolecular Motion: The Physics-Informed Machine Learning Paradigm",
    "authors": [
      "Aaryesh Deshpande"
    ],
    "abstract": "The convergence of statistical learning and molecular physics is transforming our approach to modeling biomolecular systems. Physics-informed machine learning (PIML) offers a systematic framework that integrates data-driven inference with physical constraints, resulting in models that are accurate, mechanistic, generalizable, and able to extrapolate beyond observed domains. This review surveys recent advances in physics-informed neural networks and operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, with emphasis on long-timescale kinetics, rare events, and free-energy estimation. We frame these approaches as solutions to the \"biomolecular closure problem\", recovering unresolved interactions beyond classical force fields while preserving thermodynamic consistency and mechanistic interpretability. We examine theoretical foundations, tools and frameworks, computational trade-offs, and unresolved issues, including model expressiveness and stability. We outline prospective research avenues at the intersection of machine learning, statistical physics, and computational chemistry, contending that future advancements will depend on mechanistic inductive biases, and integrated differentiable physical learning frameworks for biomolecular simulation and discovery.",
    "categories": [
      "q-bio.BM",
      "cs.LG",
      "physics.comp-ph",
      "stat.ML"
    ],
    "primary_category": "q-bio.BM",
    "published": "2025-11-10T00:24:06+00:00",
    "updated": "2025-11-10T00:24:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06585v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06568v2",
    "title": "Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity",
    "authors": [
      "João Mattos",
      "Debolina Halder Lina",
      "Arlei Silva"
    ],
    "abstract": "Link prediction is a fundamental task in graph machine learning with applications, ranging from social recommendation to knowledge graph completion. Fairness in this setting is critical, as biased predictions can exacerbate societal inequalities. Prior work adopts a dyadic definition of fairness, enforcing fairness through demographic parity between intra-group and inter-group link predictions. However, we show that this dyadic framing can obscure underlying disparities across subgroups, allowing systemic biases to go undetected. Moreover, we argue that demographic parity does not meet desired properties for fairness assessment in ranking-based tasks such as link prediction. We formalize the limitations of existing fairness evaluations and propose a framework that enables a more expressive assessment. Additionally, we propose a lightweight post-processing method combined with decoupled link predictors that effectively mitigates bias and achieves state-of-the-art fairness-utility trade-offs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T22:58:29+00:00",
    "updated": "2025-11-16T19:26:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06568v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.07485v1",
    "title": "When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift",
    "authors": [
      "Sushant Mehta"
    ],
    "abstract": "Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $α$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \\approx (1+α)/(1-α)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T20:48:09+00:00",
    "updated": "2025-11-09T20:48:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.07485v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06495v1",
    "title": "Probably Approximately Global Robustness Certification",
    "authors": [
      "Peter Blohm",
      "Patrick Indri",
      "Thomas Gärtner",
      "Sagar Malhotra"
    ],
    "abstract": "We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $ε$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T18:46:22+00:00",
    "updated": "2025-11-09T18:46:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06495v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06479v1",
    "title": "Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains",
    "authors": [
      "Muhammad Shahnawaz",
      "Adeel Safder"
    ],
    "abstract": "Supply chain disruptions and volatile demand pose significant challenges to the UK automotive industry, which relies heavily on Just-In-Time (JIT) manufacturing. While qualitative studies highlight the potential of integrating Artificial Intelligence (AI) with traditional optimization, a formal, quantitative demonstration of this synergy is lacking. This paper introduces a novel stochastic learning-optimization framework that integrates Bayesian inference with inventory optimization for supply chain management (SCM). We model a two-echelon inventory system subject to stochastic demand and supply disruptions, comparing a traditional static optimization policy against an adaptive policy where Bayesian learning continuously updates parameter estimates to inform stochastic optimization. Our simulations over 365 periods across three operational scenarios demonstrate that the integrated approach achieves 7.4\\% cost reduction in stable environments and 5.7\\% improvement during supply disruptions, while revealing important limitations during sudden demand shocks due to the inherent conservatism of Bayesian updating. This work provides mathematical validation for practitioner observations and establishes a formal framework for understanding AI-driven supply chain resilience, while identifying critical boundary conditions for successful implementation.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-09T17:51:51+00:00",
    "updated": "2025-11-09T17:51:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06479v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06425v1",
    "title": "Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings",
    "authors": [
      "Brian B. Avants",
      "Nicholas J. Tustison",
      "James R Stone"
    ],
    "abstract": "Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains.",
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-09T15:43:43+00:00",
    "updated": "2025-11-09T15:43:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06425v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06424v1",
    "title": "Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression",
    "authors": [
      "Amit Vaisman",
      "Guy Ohayon",
      "Hila Manor",
      "Michael Elad",
      "Tomer Michaeli"
    ],
    "abstract": "While zero-shot diffusion-based compression methods have seen significant progress in recent years, they remain notoriously slow and computationally demanding. This paper presents an efficient zero-shot diffusion-based compression method that runs substantially faster than existing methods, while maintaining performance that is on par with the state-of-the-art techniques. Our method builds upon the recently proposed Denoising Diffusion Codebook Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by sequentially choosing the diffusion noise vectors from reproducible random codebooks, guiding the denoiser's output to reconstruct the target image. We modify this framework with Turbo-DDCM, which efficiently combines a large number of noise vectors at each denoising step, thereby significantly reducing the number of required denoising operations. This modification is also coupled with an improved encoding protocol. Furthermore, we introduce two flexible variants of Turbo-DDCM, a priority-aware variant that prioritizes user-specified regions and a distortion-controlled variant that compresses an image based on a target PSNR rather than a target BPP. Comprehensive experiments position Turbo-DDCM as a compelling, practical, and flexible image compression scheme.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "eess.IV",
    "published": "2025-11-09T15:41:27+00:00",
    "updated": "2025-11-09T15:41:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06424v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06407v1",
    "title": "Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models",
    "authors": [
      "Takashi Hayakawa",
      "Satoshi Asai"
    ],
    "abstract": "Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-09T14:44:13+00:00",
    "updated": "2025-11-09T14:44:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06407v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06374v1",
    "title": "Adaptive Regularization for Large-Scale Sparse Feature Embedding Models",
    "authors": [
      "Mang Li",
      "Wei Lyu"
    ],
    "abstract": "The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T13:24:14+00:00",
    "updated": "2025-11-09T13:24:14+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06374v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06304v2",
    "title": "Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation",
    "authors": [
      "Kevin Bönisch",
      "Leandro Losaria"
    ],
    "abstract": "Since 2010, Kaggle has been a platform where data scientists from around the world come together to compete, collaborate, and push the boundaries of Data Science. Over these 15 years, it has grown from a purely competition-focused site into a broader ecosystem with forums, notebooks, models, datasets, and more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now have a unique opportunity to explore these competitions, technologies, and real-world applications of Machine Learning and AI. And so in this study, we take a closer look at 15 years of data science on Kaggle - through metadata, shared code, community discussions, and the competitions themselves. We explore Kaggle's growth, its impact on the data science community, uncover hidden technological trends, analyze competition winners, how Kagglers approach problems in general, and more. We do this by analyzing millions of kernels and discussion threads to perform both longitudinal trend analysis and standard exploratory data analysis. Our findings show that Kaggle is a steadily growing platform with increasingly diverse use cases, and that Kagglers are quick to adapt to new trends and apply them to real-world challenges, while producing - on average - models with solid generalization capabilities. We also offer a snapshot of the platform as a whole, highlighting its history and technological evolution. Finally, this study is accompanied by a video (https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up (https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi) for your convenience.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T10:01:39+00:00",
    "updated": "2025-11-20T12:47:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06304v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06239v1",
    "title": "Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces",
    "authors": [
      "Byoungwoo Park",
      "Juho Lee",
      "Guan-Horng Liu"
    ],
    "abstract": "Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-09T05:51:03+00:00",
    "updated": "2025-11-09T05:51:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06239v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06235v1",
    "title": "Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework",
    "authors": [
      "Zhitao Li",
      "Yiqiu Dong",
      "Xueying Zeng"
    ],
    "abstract": "This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-09T05:27:41+00:00",
    "updated": "2025-11-09T05:27:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06235v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06211v1",
    "title": "Sparse Linear Regression is Easy on Random Supports",
    "authors": [
      "Gautam Chandrasekaran",
      "Raghu Meka",
      "Konstantinos Stavropoulos"
    ],
    "abstract": "Sparse linear regression is one of the most basic questions in machine learning and statistics. Here, we are given as input a design matrix $X \\in \\mathbb{R}^{N \\times d}$ and measurements or labels ${y} \\in \\mathbb{R}^N$ where ${y} = {X} {w}^* + ξ$, and $ξ$ is the noise in the measurements. Importantly, we have the additional constraint that the unknown signal vector ${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than the ambient dimension. Our goal is to output a prediction vector $\\widehat{w}$ that has small prediction error: $\\frac{1}{N}\\cdot \\|{X} {w}^* - {X} \\widehat{w}\\|^2_2$.   Information-theoretically, we know what is best possible in terms of measurements: under most natural noise distributions, we can get prediction error at most $ε$ with roughly $N = O(k \\log d/ε)$ samples. Computationally, this currently needs $d^{Ω(k)}$ run-time. Alternately, with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap (in the dependence on $d$) between the two and we do not know if it is possible to get $d^{o(k)}$ run-time and $o(d)$ samples.   We give the first generic positive result for worst-case design matrices ${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at random, we can get prediction error $ε$ with $N = \\text{poly}(k, \\log d, 1/ε)$ samples and run-time $\\text{poly}(d,N)$. This run-time holds for any design matrix ${X}$ with condition number up to $2^{\\text{poly}(d)}$.   Previously, such results were known for worst-case ${w}^*$, but only for random design matrices from well-behaved families, matrices that have a very low condition number ($\\text{poly}(\\log d)$; e.g., as studied in compressed sensing), or those with special structural properties.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-09T03:48:21+00:00",
    "updated": "2025-11-09T03:48:21+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06211v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06189v1",
    "title": "Counterfactual Forecasting For Panel Data",
    "authors": [
      "Navonil Deb",
      "Raaz Dwivedi",
      "Sumanta Basu"
    ],
    "abstract": "We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-09T02:25:49+00:00",
    "updated": "2025-11-09T02:25:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06189v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06054v1",
    "title": "Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection",
    "authors": [
      "Alessio Arcudi",
      "Alessandro Ferreri",
      "Francesco Borsatti",
      "Gian Antonio Susto"
    ],
    "abstract": "Anomaly Detection (AD) is evolving through algorithms capable of identifying outliers in complex datasets. The Isolation Forest (IF), a pivotal AD technique, exhibits adaptability limitations and biases. This paper introduces the Function-based Isolation Forest (FuBIF), a generalization of IF that enables the use of real-valued functions for dataset branching, significantly enhancing the flexibility of evaluation tree construction. Complementing this, the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability in IF-based approaches by providing feature importance scores across possible FuBIF models. This paper details the operational framework of FuBIF, evaluates its performance against established methods, and explores its theoretical contributions. An open-source implementation is provided to encourage further research and ensure reproducibility.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-08T15:52:35+00:00",
    "updated": "2025-11-08T15:52:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06054v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.06040v2",
    "title": "The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model",
    "authors": [
      "Zhangsong Li"
    ],
    "abstract": "We study the computational task of detecting and estimating correlated signals in a pair of spiked Wigner matrices. Our model consists of observations   $$   X = \\tfracλ{\\sqrt{n}} xx^{\\top} + W \\,, \\quad Y = \\tfracμ{\\sqrt{n}} yy^{\\top} + Z \\,.   $$   where $x,y \\in \\mathbb R^n$ are signal vectors with norm $\\|x\\|,\\|y\\| \\approx\\sqrt{n}$ and correlation $\\langle x,y \\rangle \\approx ρ\\|x\\|\\|y\\|$, while $W,Z$ are independent Gaussian Wigner matrices. We propose an efficient algorithm that succeeds whenever $F(λ,μ,ρ)>1$, where   $$   F(λ,μ,ρ)=\\max\\Big\\{ λ,μ, \\frac{ λ^2 ρ^2 }{ 1-λ^2+λ^2 ρ^2 } + \\frac{ μ^2 ρ^2 }{ 1-μ^2+μ^2 ρ^2 } \\Big\\} \\,.   $$   Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be computationally infeasible.   We complement our algorithmic result with evidence for a matching computational lower bound. In particular, we prove that when $F(λ,μ,ρ)<1$, all algorithms based on {\\em low-degree polynomials} fails to distinguish $(X,Y)$ with two independent Wigner matrices. This low-degree analysis strongly suggests that $F(λ,μ,ρ)=1$ is the precise computation threshold for this problem.",
    "categories": [
      "math.ST",
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-08T15:23:44+00:00",
    "updated": "2025-11-13T10:13:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.06040v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05983v1",
    "title": "Benchmarking of Clustering Validity Measures Revisited",
    "authors": [
      "Connor Simpson",
      "Ricardo J. G. B. Campello",
      "Elizabeth Stojanovski"
    ],
    "abstract": "Validation plays a crucial role in the clustering process. Many different internal validity indexes exist for the purpose of determining the best clustering solution(s) from a given collection of candidates, e.g., as produced by different algorithms or different algorithm hyper-parameters. In this study, we present a comprehensive benchmark study of 26 internal validity indexes, which includes highly popular classic indexes as well as more recently developed ones. We adopted an enhanced revision of the methodology presented in Vendramin et al. (2010), developed here to address several shortcomings of this previous work. This overall new approach consists of three complementary custom-tailored evaluation sub-methodologies, each of which has been designed to assess specific aspects of an index's behaviour while preventing potential biases of the other sub-methodologies. Each sub-methodology features two complementary measures of performance, alongside mechanisms that allow for an in-depth investigation of more complex behaviours of the internal validity indexes under study. Additionally, a new collection of 16177 datasets has been produced, paired with eight widely-used clustering algorithms, for a wider applicability scope and representation of more diverse clustering scenarios.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-08T12:07:23+00:00",
    "updated": "2025-11-08T12:07:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05983v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05826v1",
    "title": "CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering",
    "authors": [
      "Taixi Chen",
      "Yiu-ming Cheung",
      "Yiqun Zhang"
    ],
    "abstract": "An appropriate distance metric is crucial for categorical data clustering, as the distance between categorical data cannot be directly calculated. However, the distances between attribute values usually vary in different clusters induced by their different distributions, which has not been taken into account, thus leading to unreasonable distance measurement. Therefore, we propose a cluster-customized distance metric for categorical data clustering, which can competitively update distances based on different distributions of attributes in each cluster. In addition, we extend the proposed distance metric to the mixed data that contains both numerical and categorical attributes. Experiments demonstrate the efficacy of the proposed method, i.e., achieving an average ranking of around first in fourteen datasets. The source code is available at https://anonymous.4open.science/r/CADM-47D8",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-08T03:24:22+00:00",
    "updated": "2025-11-08T03:24:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05826v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05804v1",
    "title": "Catching Contamination Before Generation: Spectral Kill Switches for Agents",
    "authors": [
      "Valentin Noël"
    ],
    "abstract": "Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.",
    "categories": [
      "cs.LG",
      "eess.SP",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-08T02:24:05+00:00",
    "updated": "2025-11-08T02:24:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05804v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05480v1",
    "title": "On Flow Matching KL Divergence",
    "authors": [
      "Maojiang Su",
      "Jerry Yao-Chieh Hu",
      "Sophia Pi",
      "Han Liu"
    ],
    "abstract": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $ε^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 ε+ A_2 ε^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T18:47:46+00:00",
    "updated": "2025-11-07T18:47:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05480v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05460v1",
    "title": "Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models",
    "authors": [
      "Sarkar Snigdha Sarathi Das",
      "Palash Goyal",
      "Mihir Parmar",
      "Yiwen Song",
      "Long T. Le",
      "Lesly Miculicich",
      "Jinsung Yoon",
      "Rui Zhang",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "abstract": "Pre-trained Time Series Foundational Models (TSFMs) represent a significant advance, capable of forecasting diverse time series with complex characteristics, including varied seasonalities, trends, and long-range dependencies. Despite their primary goal of universal time series forecasting, their efficacy is far from uniform; divergent training protocols and data sources cause individual TSFMs to exhibit highly variable performance across different forecasting tasks, domains, and horizons. Leveraging this complementary expertise by arbitrating existing TSFM outputs presents a compelling strategy, yet this remains a largely unexplored area of research. In this paper, we conduct a thorough examination of how different TSFMs exhibit specialized performance profiles across various forecasting settings, and how we can effectively leverage this behavior in arbitration between different time series models. We specifically analyze how factors such as model selection and forecast horizon distribution can influence the efficacy of arbitration strategies. Based on this analysis, we propose Synapse, a novel arbitration framework for TSFMs. Synapse is designed to dynamically leverage a pool of TSFMs, assign and adjust predictive weights based on their relative, context-dependent performance, and construct a robust forecast distribution by adaptively sampling from the output quantiles of constituent models. Experimental results demonstrate that Synapse consistently outperforms other popular ensembling techniques as well as individual TSFMs, demonstrating Synapse's efficacy in time series forecasting.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T18:01:51+00:00",
    "updated": "2025-11-07T18:01:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05460v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05452v2",
    "title": "Self-adaptive weighting and sampling for physics-informed neural networks",
    "authors": [
      "Wenqian Chen",
      "Amanda Howard",
      "Panos Stinis"
    ],
    "abstract": "Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-07T17:48:11+00:00",
    "updated": "2025-11-11T22:52:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05452v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05640v1",
    "title": "Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games",
    "authors": [
      "Hamza Virk",
      "Sandro Amaglobeli",
      "Zuhayr Syed"
    ],
    "abstract": "Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $τ$) is known a priori. When $τ$ is unknown, a fundamental scale ambiguity emerges that couples $τ$ with the reward parameters ($θ$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $θ$ and $τ$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T16:27:59+00:00",
    "updated": "2025-11-07T16:27:59+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05640v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05396v1",
    "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
    "authors": [
      "Yiting He",
      "Zhishuai Liu",
      "Weixin Wang",
      "Pan Xu"
    ],
    "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T16:24:22+00:00",
    "updated": "2025-11-07T16:24:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05396v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05368v1",
    "title": "Near-Efficient and Non-Asymptotic Multiway Inference",
    "authors": [
      "Oscar López",
      "Arvind Prasadan",
      "Carlos Llosa-Vite",
      "Richard B. Lehoucq",
      "Daniel M. Dunlavy"
    ],
    "abstract": "We establish non-asymptotic efficiency guarantees for tensor decomposition-based inference in count data models. Under a Poisson framework, we consider two related goals: (i) parametric inference, the estimation of the full distributional parameter tensor, and (ii) multiway analysis, the recovery of its canonical polyadic (CP) decomposition factors. Our main result shows that in the rank-one setting, a rank-constrained maximum-likelihood estimator achieves multiway analysis with variance matching the Cramér-Rao Lower Bound (CRLB) up to absolute constants and logarithmic factors. This provides a general framework for studying \"near-efficient\" multiway estimators in finite-sample settings. For higher ranks, we illustrate that our multiway estimator may not attain the CRLB; nevertheless, CP-based parametric inference remains nearly minimax optimal, with error bounds that improve on prior work by offering more favorable dependence on the CP rank. Numerical experiments corroborate near-efficiency in the rank-one case and highlight the efficiency gap in higher-rank scenarios.",
    "categories": [
      "math.ST",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-07T15:54:31+00:00",
    "updated": "2025-11-07T15:54:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05368v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05352v1",
    "title": "A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor Model: Maximum Likelihood Estimation and Fisher Information",
    "authors": [
      "Carlos Llosa-Vite",
      "Daniel M. Dunlavy",
      "Richard B. Lehoucq",
      "Oscar López",
      "Arvind Prasadan"
    ],
    "abstract": "We establish parameter inference for the Poisson canonical polyadic (PCP) tensor model through a latent-variable formulation. Our approach exploits the observation that any random PCP tensor can be derived by marginalizing an unobservable random tensor of one dimension larger. The loglikelihood of this larger dimensional tensor, referred to as the \"complete\" loglikelihood, is comprised of multiple rank one PCP loglikelihoods. Using this methodology, we first derive non-iterative maximum likelihood estimators for the PCP model and demonstrate that several existing algorithms for fitting non-negative matrix and tensor factorizations are Expectation-Maximization algorithms. Next, we derive the observed and expected Fisher information matrices for the PCP model. The Fisher information provides us crucial insights into the well-posedness of the tensor model, such as the role that tensor rank plays in identifiability and indeterminacy. For the special case of rank one PCP models, we demonstrate that these results are greatly simplified.",
    "categories": [
      "math.ST",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-07T15:45:13+00:00",
    "updated": "2025-11-07T15:45:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05352v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05187v1",
    "title": "Linear Gradient Prediction with Control Variates",
    "authors": [
      "Kamil Ciosek",
      "Nicolò Felicioni",
      "Juan Elenter Litwin"
    ],
    "abstract": "We propose a new way of training neural networks, with the goal of reducing training cost. Our method uses approximate predicted gradients instead of the full gradients that require an expensive backward pass. We derive a control-variate-based technique that ensures our updates are unbiased estimates of the true gradient. Moreover, we propose a novel way to derive a predictor for the gradient inspired by the theory of the Neural Tangent Kernel. We empirically show the efficacy of the technique on a vision transformer classification task.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T12:09:48+00:00",
    "updated": "2025-11-07T12:09:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05187v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05159v1",
    "title": "A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights",
    "authors": [
      "Shubhayan Pan",
      "Saptarshi Chakraborty",
      "Debolina Paul",
      "Kushal Bose",
      "Swagatam Das"
    ],
    "abstract": "Convex clustering is a well-regarded clustering method, resembling the similar centroid-based approach of Lloyd's $k$-means, without requiring a predefined cluster count. It starts with each data point as its centroid and iteratively merges them. Despite its advantages, this method can fail when dealing with data exhibiting linearly non-separable or non-convex structures. To mitigate the limitations, we propose a kernelized extension of the convex clustering method. This approach projects the data points into a Reproducing Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in this transformed space. This kernelization not only allows for better handling of complex data distributions but also produces an embedding in a finite-dimensional vector space. We provide a comprehensive theoretical underpinnings for our kernelized approach, proving algorithmic convergence and establishing finite sample bounds for our estimates. The effectiveness of our method is demonstrated through extensive experiments on both synthetic and real-world datasets, showing superior performance compared to state-of-the-art clustering techniques. This work marks a significant advancement in the field, offering an effective solution for clustering in non-linear and non-convex data scenarios.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-07T11:24:22+00:00",
    "updated": "2025-11-07T11:24:22+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05159v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05050v1",
    "title": "Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning",
    "authors": [
      "Masahiro Tanaka"
    ],
    "abstract": "In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-07T07:44:06+00:00",
    "updated": "2025-11-07T07:44:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05050v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04979v1",
    "title": "Scaling Up ROC-Optimizing Support Vector Machines",
    "authors": [
      "Gimun Bae",
      "Seung Jun Shin"
    ],
    "abstract": "The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.",
    "categories": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T04:38:25+00:00",
    "updated": "2025-11-07T04:38:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04979v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04957v1",
    "title": "Training and Testing with Multiple Splits: A Central Limit Theorem for Split-Sample Estimators",
    "authors": [
      "Bruno Fava"
    ],
    "abstract": "As predictive algorithms grow in popularity, using the same dataset to both train and test a new model has become routine across research, policy, and industry. Sample-splitting attains valid inference on model properties by using separate subsamples to estimate the model and to evaluate it. However, this approach has two drawbacks, since each task uses only part of the data, and different splits can lead to widely different estimates. Averaging across multiple splits, I develop an inference approach that uses more data for training, uses the entire sample for testing, and improves reproducibility. I address the statistical dependence from reusing observations across splits by proving a new central limit theorem for a large class of split-sample estimators under arguably mild and general conditions. Importantly, I make no restrictions on model complexity or convergence rates. I show that confidence intervals based on the normal approximation are valid for many applications, but may undercover in important cases of interest, such as comparing the performance between two models. I develop a new inference approach for such cases, explicitly accounting for the dependence across splits. Moreover, I provide a measure of reproducibility for p-values obtained from split-sample estimators. Finally, I apply my results to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. I show that my inference approach with repeated cross-fitting achieves better power than previous alternatives, often enough to find statistical significance that would otherwise be missed.",
    "categories": [
      "econ.EM",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "econ.EM",
    "published": "2025-11-07T03:48:15+00:00",
    "updated": "2025-11-07T03:48:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04957v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04907v1",
    "title": "Efficient Swap Multicalibration of Elicitable Properties",
    "authors": [
      "Lunjia Hu",
      "Haipeng Luo",
      "Spandan Senapati",
      "Vatsal Sharan"
    ],
    "abstract": "Multicalibration [HJKRR18] is an algorithmic fairness perspective that demands that the predictions of a predictor are correct conditional on themselves and membership in a collection of potentially overlapping subgroups of a population. The work of [NR23] established a surprising connection between multicalibration for an arbitrary property $Γ$ (e.g., mean or median) and property elicitation: a property $Γ$ can be multicalibrated if and only if it is elicitable, where elicitability is the notion that the true property value of a distribution can be obtained by solving a regression problem over the distribution. In the online setting, [NR23] proposed an inefficient algorithm that achieves $\\sqrt T$ $\\ell_2$-multicalibration error for a hypothesis class of group membership functions and an elicitable property $Γ$, after $T$ rounds of interaction between a forecaster and adversary.   In this paper, we generalize multicalibration for an elicitable property $Γ$ from group membership functions to arbitrary bounded hypothesis classes and introduce a stronger notion -- swap multicalibration, following [GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when given access to an online agnostic learner, achieves $T^{1/(r+1)}$ $\\ell_r$-swap multicalibration error with high probability (for $r\\ge2$) for a hypothesis class with bounded sequential Rademacher complexity and an elicitable property $Γ$. For the special case of $r=2$, this implies an oracle-efficient algorithm that achieves $T^{1/3}$ $\\ell_2$-swap multicalibration error, which significantly improves on the previously established bounds for the problem [NR23, GMS25, LSS25a], and completely resolves an open question raised in [GJRR24] on the possibility of an oracle-efficient algorithm that achieves $\\sqrt{T}$ $\\ell_2$-mean multicalibration error by answering it in a strongly affirmative sense.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-07T01:14:39+00:00",
    "updated": "2025-11-07T01:14:39+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04907v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04873v1",
    "title": "Prototype Selection Using Topological Data Analysis",
    "authors": [
      "Jordan Eckert",
      "Elvan Ceyhan",
      "Henry Schenck"
    ],
    "abstract": "Recently, there has been an explosion in statistical learning literature to represent data using topological principles to capture structure and relationships. We propose a topological data analysis (TDA)-based framework, named Topological Prototype Selector (TPS), for selecting representative subsets (prototypes) from large datasets. We demonstrate the effectiveness of TPS on simulated data under different data intrinsic characteristics, and compare TPS against other currently used prototype selection methods in real data settings. In all simulated and real data settings, TPS significantly preserves or improves classification performance while substantially reducing data size. These contributions advance both algorithmic and geometric aspects of prototype learning and offer practical tools for parallelized, interpretable, and efficient classification.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T23:21:43+00:00",
    "updated": "2025-11-06T23:21:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04873v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04869v1",
    "title": "Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs",
    "authors": [
      "Preetum Nakkiran",
      "Arwen Bradley",
      "Adam Goliński",
      "Eugene Ndiaye",
      "Michael Kirchhof",
      "Sinead Williamson"
    ],
    "abstract": "Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of \"B-calibration,\" which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-06T23:14:45+00:00",
    "updated": "2025-11-06T23:14:45+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04869v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05623v1",
    "title": "Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties",
    "authors": [
      "Mariafrancesca Patalano",
      "Giovanna Capizzi",
      "Kamran Paynabar"
    ],
    "abstract": "Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "published": "2025-11-06T23:13:03+00:00",
    "updated": "2025-11-06T23:13:03+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05623v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.05620v1",
    "title": "Fooling Algorithms in Non-Stationary Bandits using Belief Inertia",
    "authors": [
      "Gal Mendelson",
      "Eyal Tadmor"
    ],
    "abstract": "We study the problem of worst case regret in piecewise stationary multi armed bandits. While the minimax theory for stationary bandits is well established, understanding analogous limits in time-varying settings is challenging. Existing lower bounds rely on what we refer to as infrequent sampling arguments, where long intervals without exploration allow adversarial reward changes that induce large regret.   In this paper, we introduce a fundamentally different approach based on a belief inertia argument. Our analysis captures how an algorithm's empirical beliefs, encoded through historical reward averages, create momentum that resists new evidence after a change. We show how this inertia can be exploited to construct adversarial instances that mislead classical algorithms such as Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret that grows linearly with T and with a substantial constant factor, regardless of how their parameters are tuned, even with a single change point.   We extend the analysis to algorithms that periodically restart to handle non stationarity and prove that, even then, the worst case regret remains linear in T. Our results indicate that utilizing belief inertia can be a powerful method for deriving sharp lower bounds in non stationary bandits.",
    "categories": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T22:16:56+00:00",
    "updated": "2025-11-06T22:16:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.05620v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04790v1",
    "title": "Causal Structure and Representation Learning with Biomedical Applications",
    "authors": [
      "Caroline Uhler",
      "Jiaqi Zhang"
    ],
    "abstract": "Massive data collection holds the promise of a better understanding of complex phenomena and, ultimately, better decisions. Representation learning has become a key driver of deep learning applications, as it allows learning latent spaces that capture important properties of the data without requiring any supervised annotations. Although representation learning has been hugely successful in predictive tasks, it can fail miserably in causal tasks including predicting the effect of a perturbation/intervention. This calls for a marriage between representation learning and causal inference. An exciting opportunity in this regard stems from the growing availability of multi-modal data (observational and perturbational, imaging-based and sequencing-based, at the single-cell level, tissue-level, and organism-level). We outline a statistical and computational framework for causal structure and representation learning motivated by fundamental biomedical questions: how to effectively use observational and perturbational data to perform causal discovery on observed causal variables; how to use multi-modal views of the system to learn causal variables; and how to design optimal perturbations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T20:17:58+00:00",
    "updated": "2025-11-06T20:17:58+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04790v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04666v1",
    "title": "Forgetting is Everywhere",
    "authors": [
      "Ben Sanati",
      "Thomas L. Lee",
      "Trevor McInroe",
      "Aidan Scannell",
      "Nikolay Malkin",
      "David Abel",
      "Amos Storkey"
    ],
    "abstract": "A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T18:52:57+00:00",
    "updated": "2025-11-06T18:52:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04666v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04599v4",
    "title": "From Global to Local Correlation: Geometric Decomposition of Statistical Inference",
    "authors": [
      "Pawel Gajer",
      "Jacques Ravel"
    ],
    "abstract": "Understanding feature-outcome associations in high-dimensional data remains   challenging when relationships vary across subpopulations, yet standard   methods assuming global associations miss context-dependent patterns, reducing   statistical power and interpretability. We develop a geometric decomposition   framework offering two strategies for partitioning inference problems into   regional analyses on data-derived Riemannian graphs. Gradient flow   decomposition uses path-monotonicity-validated discrete Morse theory to   partition samples into gradient flow cells where outcomes exhibit monotonic   behavior. Co-monotonicity decomposition utilizes vertex-level coefficients   that provide context-dependent versions of the classical Pearson correlation:   these coefficients measure edge-based directional concordance between outcome   and features, or between feature pairs, defining embeddings of samples into   association space. These embeddings induce Riemannian k-NN graphs on which   biclustering identifies co-monotonicity cells (coherent regions) and feature   modules. This extends naturally to multi-modal integration across multiple   feature sets. Both strategies apply independently or jointly, with Bayesian   posterior sampling providing credible intervals.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-06T17:51:32+00:00",
    "updated": "2025-11-19T09:55:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04599v4",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04576v2",
    "title": "Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis",
    "authors": [
      "Zhuo Zhang",
      "Xiong Xiong",
      "Sen Zhang",
      "Yuan Zhao",
      "Xi Yang"
    ],
    "abstract": "PDEs arise ubiquitously in science and engineering, where solutions depend on parameters (physical properties, boundary conditions, geometry). Traditional numerical methods require re-solving the PDE for each parameter, making parameter space exploration prohibitively expensive. Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. We critically analyze two main paradigms: (1) PINNs, which embed physical laws as soft constraints and excel at inverse problems with sparse data, and (2) neural operators (e.g., DeepONet, Fourier Neural Operator), which learn mappings between infinite-dimensional function spaces and achieve unprecedented generalization. Through comparisons across fluid dynamics, solid mechanics, heat transfer, and electromagnetics, we show neural operators can achieve computational speedups of $10^3$ to $10^5$ times faster than traditional solvers for multi-query scenarios, while maintaining comparable accuracy. We provide practical guidance for method selection, discuss theoretical foundations (universal approximation, convergence), and identify critical open challenges: high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive, incrementally updated resource for this rapidly evolving field",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T17:31:59+00:00",
    "updated": "2025-11-07T06:01:34+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04576v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04568v1",
    "title": "Riesz Regression As Direct Density Ratio Estimation",
    "authors": [
      "Masahiro Kato"
    ],
    "abstract": "Riesz regression has garnered attention as a tool in debiased machine learning for causal and structural parameter estimation (Chernozhukov et al., 2021). This study shows that Riesz regression is closely related to direct density-ratio estimation (DRE) in important cases, including average treat- ment effect (ATE) estimation. Specifically, the idea and objective in Riesz regression coincide with the one in least-squares importance fitting (LSIF, Kanamori et al., 2009) in direct density-ratio estimation. While Riesz regression is general in the sense that it can be applied to Riesz representer estimation in a wide class of problems, the equivalence with DRE allows us to directly import exist- ing results in specific cases, including convergence-rate analyses, the selection of loss functions via Bregman-divergence minimization, and regularization techniques for flexible models, such as neural networks. Conversely, insights about the Riesz representer in debiased machine learning broaden the applications of direct density-ratio estimation methods. This paper consolidates our prior results in Kato (2025a) and Kato (2025b).",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T17:25:05+00:00",
    "updated": "2025-11-06T17:25:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04568v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04552v1",
    "title": "Generative Bayesian Filtering and Parameter Learning",
    "authors": [
      "Edoardo Marcelli",
      "Sean O'Hagan",
      "Veronika Rockova"
    ],
    "abstract": "Generative Bayesian Filtering (GBF) provides a powerful and flexible framework for performing posterior inference in complex nonlinear and non-Gaussian state-space models. Our approach extends Generative Bayesian Computation (GBC) to dynamic settings, enabling recursive posterior inference using simulation-based methods powered by deep neural networks. GBF does not require explicit density evaluations, making it particularly effective when observation or transition distributions are analytically intractable. To address parameter learning, we introduce the Generative-Gibbs sampler, which bypasses explicit density evaluation by iteratively sampling each variable from its implicit full conditional distribution. Such technique is broadly applicable and enables inference in hierarchical Bayesian models with intractable densities, including state-space models. We assess the performance of the proposed methodologies through both simulated and empirical studies, including the estimation of $α$-stable stochastic volatility models. Our findings indicate that GBF significantly outperforms existing likelihood-free approaches in accuracy and robustness when dealing with intractable state-space models.",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-06T17:04:48+00:00",
    "updated": "2025-11-06T17:04:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04552v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04518v1",
    "title": "Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity",
    "authors": [
      "Obed Amo",
      "Samit Ghosh",
      "Markus Lange-Hegermann",
      "Bogdan Raiţă",
      "Michael Pokojovy"
    ],
    "abstract": "We present a new benchmarking study comparing a boundary-constrained Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical finite element method combined with Crank--Nicolson time stepping (CN-FEM) for solving the two-dimensional wave equation with homogeneous Dirichlet boundary conditions. The B-EPGP construction leverages exponential-polynomial bases derived from the characteristic variety to enforce the PDE and boundary conditions exactly and employs penalized least squares to estimate the coefficients. To ensure fairness across paradigms, we introduce a degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP consistently attains lower space-time $L^2$-error and maximum-in-time $L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of magnitude.",
    "categories": [
      "cs.LG",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T16:33:17+00:00",
    "updated": "2025-11-06T16:33:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04518v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04445v1",
    "title": "ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting",
    "authors": [
      "Syeda Sitara Wishal Fatima",
      "Afshin Rahimi"
    ],
    "abstract": "Time series forecasting is essential across domains from finance to supply chain management. This paper introduces ForecastGAN, a novel decomposition based adversarial framework addressing limitations in existing approaches for multi-horizon predictions. Although transformer models excel in long-term forecasting, they often underperform in short-term scenarios and typically ignore categorical features. ForecastGAN operates through three integrated modules: a Decomposition Module that extracts seasonality and trend components; a Model Selection Module that identifies optimal neural network configurations based on forecasting horizon; and an Adversarial Training Module that enhances prediction robustness through Conditional Generative Adversarial Network training. Unlike conventional approaches, ForecastGAN effectively integrates both numerical and categorical features. We validate our framework on eleven benchmark multivariate time series datasets that span various forecasting horizons. The results show that ForecastGAN consistently outperforms state-of-the-art transformer models for short-term forecasting while remaining competitive for long-term horizons. This research establishes a more generalizable approach to time series forecasting that adapts to specific contexts while maintaining strong performance across diverse data characteristics without extensive hyperparameter tuning.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T15:19:23+00:00",
    "updated": "2025-11-06T15:19:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04445v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04403v1",
    "title": "Online Bayesian Experimental Design for Partially Observed Dynamical Systems",
    "authors": [
      "Sara Pérez-Vieites",
      "Sahel Iqbal",
      "Simo Särkkä",
      "Dominik Baumann"
    ],
    "abstract": "Bayesian experimental design (BED) provides a principled framework for optimizing data collection, but existing approaches do not apply to crucial real-world settings such as dynamical systems with partial observability, where only noisy and incomplete observations are available. These systems are naturally modeled as state-space models (SSMs), where latent states mediate the link between parameters and data, making the likelihood -- and thus information-theoretic objectives like the expected information gain (EIG) -- intractable. In addition, the dynamical nature of the system requires online algorithms that update posterior distributions and select designs sequentially in a computationally efficient manner. We address these challenges by deriving new estimators of the EIG and its gradient that explicitly marginalize latent states, enabling scalable stochastic optimization in nonlinear SSMs. Our approach leverages nested particle filters (NPFs) for efficient online inference with convergence guarantees. Applications to realistic models, such as the susceptible-infected-recovered (SIR) and a moving source location task, show that our framework successfully handles both partial observability and online computation.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T14:29:05+00:00",
    "updated": "2025-11-06T14:29:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04403v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04301v1",
    "title": "Simultaneous Optimization of Geodesics and Fréchet Means",
    "authors": [
      "Frederik Möbius Rygaard",
      "Søren Hauberg",
      "Steen Markvorsen"
    ],
    "abstract": "A central part of geometric statistics is to compute the Fréchet mean. This is a well-known intrinsic mean on a Riemannian manifold that minimizes the sum of squared Riemannian distances from the mean point to all other data points. The Fréchet mean is simple to define and generalizes the Euclidean mean, but for most manifolds even minimizing the Riemannian distance involves solving an optimization problem. Therefore, numerical computations of the Fréchet mean require solving an embedded optimization problem in each iteration. We introduce the GEORCE-FM algorithm to simultaneously compute the Fréchet mean and Riemannian distances in each iteration in a local chart, making it faster than previous methods. We extend the algorithm to Finsler manifolds and introduce an adaptive extension such that GEORCE-FM scales to a large number of data points. Theoretically, we show that GEORCE-FM has global convergence and local quadratic convergence and prove that the adaptive extension converges in expectation to the Fréchet mean. We further empirically demonstrate that GEORCE-FM outperforms existing baseline methods to estimate the Fréchet mean in terms of both accuracy and runtime.",
    "categories": [
      "stat.ML",
      "math.DG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T12:08:15+00:00",
    "updated": "2025-11-06T12:08:15+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04301v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04291v1",
    "title": "Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition",
    "authors": [
      "Giovanni Barbarino",
      "Nicolas Gillis",
      "Subhayan Saha"
    ],
    "abstract": "Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used successfully in many applications, such as hyperspectral imaging, chemical kinetics, spectroscopy, topic modeling, and audio source separation. However, its robustness to noise has been a long-standing open problem. In this paper, we prove that min-vol NMF identifies the groundtruth factors in the presence of noise under a condition referred to as the expanded sufficiently scattered condition which requires the data points to be sufficiently well scattered in the latent simplex generated by the basis vectors.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SP",
      "math.NA"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T11:36:32+00:00",
    "updated": "2025-11-06T11:36:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04291v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04275v1",
    "title": "Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift",
    "authors": [
      "Jungbin Jun",
      "Ilsang Ohn"
    ],
    "abstract": "Conformal prediction has emerged as a powerful framework for constructing distribution-free prediction sets with guaranteed coverage assuming only the exchangeability assumption. However, this assumption is often violated in online environments where data distributions evolve over time. Several recent approaches have been proposed to address this limitation, but, typically, they slowly adapt to distribution shifts because they update predictions only in a forward manner, that is, they generate a prediction for a newly observed data point while previously computed predictions are not updated. In this paper, we propose a novel online conformal inference method with retrospective adjustment, which is designed to achieve faster adaptation to distributional shifts. Our method leverages regression approaches with efficient leave-one-out update formulas to retroactively adjust past predictions when new data arrive, thereby aligning the entire set of predictions with the most recent data distribution. Through extensive numerical studies performed on both synthetic and real-world data sets, we show that the proposed approach achieves faster coverage recalibration and improved statistical efficiency compared to existing online conformal prediction methods.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T11:11:51+00:00",
    "updated": "2025-11-06T11:11:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04275v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04160v2",
    "title": "On Joint Regularization and Calibration in Deep Ensembles",
    "authors": [
      "Laurits Fredsgaard",
      "Mikkel N. Schmidt"
    ],
    "abstract": "Deep ensembles are a powerful tool in machine learning, improving both model performance and uncertainty calibration. While ensembles are typically formed by training and tuning models individually, evidence suggests that jointly tuning the ensemble can lead to better performance. This paper investigates the impact of jointly tuning weight decay, temperature scaling, and early stopping on both predictive performance and uncertainty quantification. Additionally, we propose a partially overlapping holdout strategy as a practical compromise between enabling joint evaluation and maximizing the use of data for training. Our results demonstrate that jointly tuning the ensemble generally matches or improves performance, with significant variation in effect size across different tasks and metrics. We highlight the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering an attractive practical solution. We believe our findings provide valuable insights and guidance for practitioners looking to optimize deep ensemble models. Code is available at: https://github.com/lauritsf/ensemble-optimality-gap",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T08:04:19+00:00",
    "updated": "2025-11-07T02:19:40+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04160v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.04000v1",
    "title": "Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations",
    "authors": [
      "Kyaw Hpone Myint",
      "Zhe Wu",
      "Alexandre G. R. Day",
      "Giri Iyengar"
    ],
    "abstract": "Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T02:50:23+00:00",
    "updated": "2025-11-06T02:50:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.04000v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03972v2",
    "title": "Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models",
    "authors": [
      "Semih Cayci"
    ],
    "abstract": "An important question in deep learning is how higher-order optimization methods affect generalization. In this work, we analyze a stochastic Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in a regression setting. Our theoretical contributions are twofold. First, we establish finite-time convergence bounds via a variable-metric analysis in parameter space, with explicit dependencies on the batch size, network width and depth. Second, we derive non-asymptotic generalization bounds for SGN using uniform stability in the overparameterized regime, characterizing the impact of curvature, batch size, and overparameterization on generalization performance. Our theoretical results identify a favorable generalization regime for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along the optimization path yields tighter stability bounds.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T01:50:45+00:00",
    "updated": "2025-11-12T06:47:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03972v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03963v1",
    "title": "Robust inference using density-powered Stein operators",
    "authors": [
      "Shinto Eguchi"
    ],
    "abstract": "We introduce a density-power weighted variant for the Stein operator, called the $γ$-Stein operator. This is a novel class of operators derived from the $γ$-divergence, designed to build robust inference methods for unnormalized probability models. The operator's construction (weighting by the model density raised to a positive power $γ$ inherently down-weights the influence of outliers, providing a principled mechanism for robustness. Applying this operator yields a robust generalization of score matching that retains the crucial property of being independent of the model's normalizing constant. We extend this framework to develop two key applications: the $γ$-kernelized Stein discrepancy for robust goodness-of-fit testing, and $γ$-Stein variational gradient descent for robust Bayesian posterior approximation. Empirical results on contaminated Gaussian and quartic potential models show our methods significantly outperform standard baselines in both robustness and statistical efficiency.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T01:32:17+00:00",
    "updated": "2025-11-06T01:32:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03963v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03953v1",
    "title": "Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels",
    "authors": [
      "Wuxia Chen",
      "Taposh Banerjee",
      "Vahid Tarokh"
    ],
    "abstract": "We address the problem of quickest change detection in Markov processes with unknown transition kernels. The key idea is to learn the conditional score $\\nabla_{\\mathbf{y}} \\log p(\\mathbf{y}|\\mathbf{x})$ directly from sample pairs $( \\mathbf{x},\\mathbf{y})$, where both $\\mathbf{x}$ and $\\mathbf{y}$ are high-dimensional data generated by the same transition kernel. In this way, we avoid explicit likelihood evaluation and provide a practical way to learn the transition dynamics. Based on this estimation, we develop a score-based CUSUM procedure that uses conditional Hyvarinen score differences to detect changes in the kernel. To ensure bounded increments, we propose a truncated version of the statistic. With Hoeffding's inequality for uniformly ergodic Markov processes, we prove exponential lower bounds on the mean time to false alarm. We also prove asymptotic upper bounds on detection delay. These results give both theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models.",
    "categories": [
      "cs.LG",
      "eess.SP",
      "math.ST",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-06T01:07:36+00:00",
    "updated": "2025-11-06T01:07:36+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03953v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03952v1",
    "title": "High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes",
    "authors": [
      "Aukosh Jagannath",
      "Taj Jones-McCormick",
      "Varnan Sarangian"
    ],
    "abstract": "We develop a high-dimensional scaling limit for Stochastic Gradient Descent with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework to rigourously compare online SGD with some of its popular variants. We show that the scaling limits of SGD-M coincide with those of online SGD after an appropriate time rescaling and a specific choice of step-size. However, if the step-size is kept the same between the two algorithms, SGD-M will amplify high-dimensional effects, potentially degrading performance relative to online SGD. We demonstrate our framework on two popular learning problems: Spiked Tensor PCA and Single Index Models. In both cases, we also examine online SGD with an adaptive step-size based on normalized gradients. In the high-dimensional regime, this algorithm yields multiple benefits: its dynamics admit fixed points closer to the population minimum and widens the range of admissible step-sizes for which the iterates converge to such solutions. These examples provide a rigorous account, aligning with empirical motivation, of how early preconditioners can stabilize and improve dynamics in settings where online SGD fails.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-06T01:05:18+00:00",
    "updated": "2025-11-06T01:05:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03952v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03892v1",
    "title": "A general technique for approximating high-dimensional empirical kernel matrices",
    "authors": [
      "Chiraag Kaushik",
      "Justin Romberg",
      "Vidya Muthukumar"
    ],
    "abstract": "We present simple, user-friendly bounds for the expected operator norm of a random kernel matrix under general conditions on the kernel function $k(\\cdot,\\cdot)$. Our approach uses decoupling results for U-statistics and the non-commutative Khintchine inequality to obtain upper and lower bounds depending only on scalar statistics of the kernel function and a ``correlation kernel'' matrix corresponding to $k(\\cdot,\\cdot)$. We then apply our method to provide new, tighter approximations for inner-product kernel matrices on general high-dimensional data, where the sample size and data dimension are polynomially related. Our method obtains simplified proofs of existing results that rely on the moment method and combinatorial arguments while also providing novel approximation results for the case of anisotropic Gaussian data. Finally, using similar techniques to our approximation result, we show a tighter lower bound on the bias of kernel regression with anisotropic Gaussian data.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T22:36:52+00:00",
    "updated": "2025-11-05T22:36:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03892v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03831v1",
    "title": "Higher-Order Causal Structure Learning with Additive Models",
    "authors": [
      "James Enouen",
      "Yujia Zheng",
      "Ignavier Ng",
      "Yan Liu",
      "Kun Zhang"
    ],
    "abstract": "Causal structure learning has long been the central task of inferring causal insights from data. Despite the abundance of real-world processes exhibiting higher-order mechanisms, however, an explicit treatment of interactions in causal discovery has received little attention. In this work, we focus on extending the causal additive model (CAM) to additive models with higher-order interactions. This second level of modularity we introduce to the structure learning problem is most easily represented by a directed acyclic hypergraph which extends the DAG. We introduce the necessary definitions and theoretical tools to handle the novel structure we introduce and then provide identifiability results for the hyper DAG, extending the typical Markov equivalence classes. We next provide insights into why learning the more complex hypergraph structure may actually lead to better empirical results. In particular, more restrictive assumptions like CAM correspond to easier-to-learn hyper DAGs and better finite sample complexity. We finally develop an extension of the greedy CAM algorithm which can handle the more complex hyper DAG search space and demonstrate its empirical usefulness in synthetic experiments.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-05T19:55:55+00:00",
    "updated": "2025-11-05T19:55:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03831v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03797v1",
    "title": "Learning Paths for Dynamic Measure Transport: A Control Perspective",
    "authors": [
      "Aimee Maurais",
      "Bamdad Hosseini",
      "Youssef Marzouk"
    ],
    "abstract": "We bring a control perspective to the problem of identifying paths of measures for sampling via dynamic measure transport (DMT). We highlight the fact that commonly used paths may be poor choices for DMT and connect existing methods for learning alternate paths to mean-field games. Based on these connections we pose a flexible family of optimization problems for identifying tilted paths of measures for DMT and advocate for the use of objective terms which encourage smoothness of the corresponding velocities. We present a numerical algorithm for solving these problems based on recent Gaussian process methods for solution of partial differential equations and demonstrate the ability of our method to recover more efficient and smooth transport models compared to those which use an untilted reference path.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T19:02:23+00:00",
    "updated": "2025-11-05T19:02:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03797v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03708v2",
    "title": "The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp Characterization of the Price of Unknown Margin",
    "authors": [
      "Rong Jiang",
      "Cong Ma"
    ],
    "abstract": "We study batched nonparametric contextual bandits under a margin condition when the margin parameter $α$ is unknown. To capture the statistical cost of this ignorance, we introduce the regret inflation criterion, defined as the ratio between the regret of an adaptive algorithm and that of an oracle knowing $α$. We show that the optimal regret inflation grows polynomially with the horizon $T$, with exponent given by the value of a convex optimization problem that depends on the dimension, smoothness, and number of batches $M$. Moreover, the minimizer of this optimization problem directly prescribes the batch allocation and exploration strategy of a rate-optimal algorithm. Building on this principle, we develop RoBIN (RObust batched algorithm with adaptive BINning), which achieves the optimal regret inflation up to polylogarithmic factors. These results reveal a new adaptivity barrier: under batching, adaptation to an unknown margin parameter inevitably incurs a polynomial penalty, sharply characterized by a variational problem. Remarkably, this barrier vanishes once the number of batches exceeds order $\\log \\log T$; with only a doubly logarithmic number of updates, one can recover the oracle regret rate up to polylogarithmic factors.",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-05T18:42:47+00:00",
    "updated": "2025-11-10T22:01:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03708v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03693v1",
    "title": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning",
    "authors": [
      "Md Ahasanul Arafath",
      "Abhijit Kumar Ghosh",
      "Md Rony Ahmed",
      "Sabrin Afroz",
      "Minhazul Hosen",
      "Md Hasan Moon",
      "Md Tanzim Reza",
      "Md Ashad Alam"
    ],
    "abstract": "Colorectal cancer (CRC) grading is a critical prognostic factor but remains hampered by inter-observer variability and the privacy constraints of multi-institutional data sharing. While deep learning offers a path to automation, centralized training models conflict with data governance regulations and neglect the diagnostic importance of multi-scale analysis. In this work, we propose a scalable, privacy-preserving federated learning (FL) framework for CRC histopathological grading that integrates multi-scale feature learning within a distributed training paradigm. Our approach employs a dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear detail and broader tissue-level context. This architecture is integrated into a robust FL system stabilized using FedProx to mitigate client drift across heterogeneous data distributions from multiple hospitals. Extensive evaluation on the CRC-HGD dataset demonstrates that our framework achieves an overall accuracy of 83.5%, outperforming a comparable centralized model (81.6%). Crucially, the system excels in identifying the most aggressive Grade III tumors with a high recall of 87.5%, a key clinical priority to prevent dangerous false negatives. Performance further improves with higher magnification, reaching 88.0% accuracy at 40x. These results validate that our federated multi-scale approach not only preserves patient privacy but also enhances model performance and generalization. The proposed modular pipeline, with built-in preprocessing, checkpointing, and error handling, establishes a foundational step toward deployable, privacy-aware clinical AI for digital pathology.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T18:18:09+00:00",
    "updated": "2025-11-05T18:18:09+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03693v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03667v1",
    "title": "Addressing prior dependence in hierarchical Bayesian modeling for PTA data analysis I: Methodology and implementation",
    "authors": [
      "Luigi D'amico",
      "Eleonora Villa",
      "Fatima Modica Bittordo",
      "Aldo Barca",
      "Francesco Alì",
      "Massimo Meneghetti",
      "Luca Naso"
    ],
    "abstract": "Complex inference tasks, such as those encountered in Pulsar Timing Array (PTA) data analysis, rely on Bayesian frameworks. The high-dimensional parameter space and the strong interdependencies among astrophysical, pulsar noise, and nuisance parameters introduce significant challenges for efficient learning and robust inference. These challenges are emblematic of broader issues in decision science, where model over-parameterization and prior sensitivity can compromise both computational tractability and the reliability of the results. We address these issues in the framework of hierarchical Bayesian modeling by introducing a reparameterization strategy. Our approach employs Normalizing Flows (NFs) to decorrelate the parameters governing hierarchical priors from those of astrophysical interest. The use of NF-based mappings provides both the flexibility to realize the reparametrization and the tractability to preserve proper probability densities. We further adopt i-nessai, a flow-guided nested sampler, to accelerate exploration of complex posteriors. This unified use of NFs improves statistical robustness and computational efficiency, providing a principled methodology for addressing hierarchical Bayesian inference in PTA analysis.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO",
      "astro-ph.HE",
      "stat.ML"
    ],
    "primary_category": "astro-ph.IM",
    "published": "2025-11-05T17:33:44+00:00",
    "updated": "2025-11-05T17:33:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03667v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03618v1",
    "title": "Towards Formalizing Reinforcement Learning Theory",
    "authors": [
      "Shangtong Zhang"
    ],
    "abstract": "In this paper, we formalize the almost sure convergence of $Q$-learning and linear temporal difference (TD) learning with Markovian samples using the Lean 4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are among the earliest and most influential reinforcement learning (RL) algorithms. The investigation of their convergence properties is not only a major research topic during the early development of the RL field but also receives increasing attention nowadays. This paper formally verifies their almost sure convergence in a unified framework based on the Robbins-Siegmund theorem. The framework developed in this work can be easily extended to convergence rates and other modes of convergence. This work thus makes an important step towards fully formalizing convergent RL results. The code is available at https://github.com/ShangtongZhang/rl-theory-in-lean.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-05T16:35:47+00:00",
    "updated": "2025-11-05T16:35:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03618v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03606v1",
    "title": "Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity",
    "authors": [
      "Diego Martinez-Taboada",
      "Tomas Gonzalez",
      "Aaditya Ramdas"
    ],
    "abstract": "The study of self-normalized processes plays a crucial role in a wide range of applications, from sequential decision-making to econometrics. While the behavior of self-normalized concentration has been widely investigated for scalar-valued processes, vector-valued processes remain comparatively underexplored, especially outside of the sub-Gaussian framework. In this contribution, we provide concentration bounds for self-normalized processes with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds). We illustrate the relevance of our results in the context of online linear regression, with applications in (kernelized) linear bandits.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T16:27:02+00:00",
    "updated": "2025-11-05T16:27:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03606v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03492v1",
    "title": "Why Less is More (Sometimes): A Theory of Data Curation",
    "authors": [
      "Elvis Dohmatob",
      "Mohammad Pezeshki",
      "Reyhane Askari-Hemmat"
    ],
    "abstract": "This paper introduces a theoretical framework to resolve a central paradox in modern machine learning: When is it better to use less data? This question has become critical as classical scaling laws suggesting ``more is more'' (Sun et al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et al., 2025; Muenighoff et al., 2025), which achieve superior performance with small, aggressively curated datasets. Here, we study data curation strategies where an imperfect oracle selects the training examples according to their difficulty and correctness. Our results provide exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing when and why keeping only a subset of data can improve generalization. In contrast to classical scaling laws, we show that under certain conditions, small curated datasets can outperform full datasets, and we provide analytical conditions for this by deriving precise phase transition curves tied to data size and quality. We validate these theoretical claims with empirical results on ImageNet, confirming our predictions about when curation improves accuracy and can even mitigate model collapse. Furthermore, our framework provides a principled explanation for the contradictory curation strategies recently observed in LLM mathematical reasoning.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-05T14:21:18+00:00",
    "updated": "2025-11-05T14:21:18+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03492v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03443v1",
    "title": "A Support-Set Algorithm for Optimization Problems with Nonnegative and Orthogonal Constraints",
    "authors": [
      "Lei Wang",
      "Xin Liu",
      "Xiaojun Chen"
    ],
    "abstract": "In this paper, we investigate optimization problems with nonnegative and orthogonal constraints, where any feasible matrix of size $n \\times p$ exhibits a sparsity pattern such that each row accommodates at most one nonzero entry. Our analysis demonstrates that, by fixing the support set, the global solution of the minimization subproblem for the proximal linearization of the objective function can be computed in closed form with at most $n$ nonzero entries. Exploiting this structural property offers a powerful avenue for dramatically enhancing computational efficiency. Guided by this insight, we propose a support-set algorithm preserving strictly the feasibility of iterates. A central ingredient is a strategically devised update scheme for support sets that adjusts the placement of nonzero entries. We establish the global convergence of the support-set algorithm to a first-order stationary point, and show that its iteration complexity required to reach an $ε$-approximate first-order stationary point is $O (ε^{-2})$. Numerical results are strongly in favor of our algorithm in real-world applications, including nonnegative PCA, clustering, and community detection.",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "published": "2025-11-05T13:03:41+00:00",
    "updated": "2025-11-05T13:03:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03443v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03369v2",
    "title": "Silenced Biases: The Dark Side LLMs Learned to Refuse",
    "authors": [
      "Rom Himelstein",
      "Amit LeVi",
      "Brit Youngmann",
      "Yaniv Nemcovsky",
      "Avi Mendelson"
    ],
    "abstract": "Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.",
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-05T11:24:50+00:00",
    "updated": "2025-11-16T16:50:48+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03369v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03256v1",
    "title": "Decoupled Entropy Minimization",
    "authors": [
      "Jing Ma",
      "Hanlin Li",
      "Xiang Xiang"
    ],
    "abstract": "Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.IT",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-05T07:36:46+00:00",
    "updated": "2025-11-05T07:36:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03256v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03216v1",
    "title": "RKUM: An R Package for Robust Kernel Unsupervised Methods",
    "authors": [
      "Md Ashad Alam"
    ],
    "abstract": "RKUM is an R package developed for implementing robust kernel-based unsupervised methods. It provides functions for estimating the robust kernel covariance operator (CO) and the robust kernel cross-covariance operator (CCO) using generalized loss functions instead of the conventional quadratic loss. These operators form the foundation of robust kernel learning and enable reliable analysis under contaminated or noisy data conditions. The package includes implementations of robust kernel canonical correlation analysis (Kernel CCA), as well as the influence function (IF) for both standard and multiple kernel CCA frameworks. The influence function quantifies sensitivity and helps detect influential or outlying observations across two-view and multi-view datasets. Experiments using synthesized two-view and multi-view data demonstrate that the IF of the standard kernel CCA effectively identifies outliers, while the robust kernel methods implemented in RKUM exhibit reduced sensitivity to contamination. Overall, RKUM provides an efficient and extensible platform for robust kernel-based analysis in high-dimensional data applications.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T06:08:23+00:00",
    "updated": "2025-11-05T06:08:23+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03216v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03202v2",
    "title": "Provable Separations between Memorization and Generalization in Diffusion Models",
    "authors": [
      "Zeqi Ye",
      "Qijie Zhu",
      "Molei Tao",
      "Minshuo Chen"
    ],
    "abstract": "Diffusion models have achieved remarkable success across diverse domains, but they remain vulnerable to memorization -- reproducing training data rather than generating novel outputs. This not only limits their creative potential but also raises concerns about privacy and safety. While empirical studies have explored mitigation strategies, theoretical understanding of memorization remains limited. We address this gap through developing a dual-separation result via two complementary perspectives: statistical estimation and network approximation. From the estimation side, we show that the ground-truth score function does not minimize the empirical denoising loss, creating a separation that drives memorization. From the approximation side, we prove that implementing the empirical score function requires network size to scale with sample size, spelling a separation compared to the more compact network representation of the ground-truth score function. Guided by these insights, we develop a pruning-based method that reduces memorization while maintaining generation quality in diffusion transformers.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T05:37:57+00:00",
    "updated": "2025-11-07T00:14:54+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03202v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03196v1",
    "title": "Cross-Modal Alignment via Variational Copula Modelling",
    "authors": [
      "Feng Wu",
      "Tsai Hor Chan",
      "Fuying Wang",
      "Guosheng Yin",
      "Lequan Yu"
    ],
    "abstract": "Various data modalities are common in real-world applications (e.g., electronic health records, medical images and clinical notes in healthcare). It is essential to develop multimodal learning methods to aggregate various information from multiple modalities. The main challenge is how to appropriately align and fuse the representations of different modalities into a joint distribution. Existing methods mainly rely on concatenation or the Kronecker product, oversimplifying the interaction structure between modalities and indicating a need to model more complex interactions. Additionally, the joint distribution of latent representations with higher-order interactions is underexplored. Copula is a powerful statistical structure for modelling the interactions among variables, as it naturally bridges the joint distribution and marginal distributions of multiple variables. We propose a novel copula-driven multimodal learning framework, which focuses on learning the joint distribution of various modalities to capture the complex interactions among them. The key idea is to interpret the copula model as a tool to align the marginal distributions of the modalities efficiently. By assuming a Gaussian mixture distribution for each modality and a copula model on the joint distribution, our model can generate accurate representations for missing modalities. Extensive experiments on public MIMIC datasets demonstrate the superior performance of our model over other competitors. The code is available at https://github.com/HKU-MedAI/CMCM.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-05T05:28:28+00:00",
    "updated": "2025-11-05T05:28:28+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03196v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03193v2",
    "title": "Statistical Properties of Rectified Flow",
    "authors": [
      "Gonzalo Mena",
      "Arun Kumar Kuchibhotla",
      "Larry Wasserman"
    ],
    "abstract": "Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for defining a transport map between two distributions, and enjoys popularity in machine learning, although theoretical results supporting the validity of these methods are scant. The rectified flow can be regarded as an approximation to optimal transport, but in contrast to other transport methods that require optimization over a function space, computing the rectified flow only requires standard statistical tools such as regression or density estimation. Because of this, one can leverage standard data analysis tools for regression and density estimation to develop empirical versions of transport maps. We study some structural properties of the rectified flow, including existence, uniqueness, and regularity, as well as the related statistical properties, such as rates of convergence and central limit theorems, for some selected estimators. To do so, we analyze separately the bounded and unbounded cases as each presents unique challenges. In both cases, we are able to establish convergence at faster rates than the ones for the usual nonparametric regression and density estimation.",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-05T05:09:12+00:00",
    "updated": "2025-11-06T01:42:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03193v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03756v1",
    "title": "Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields",
    "authors": [
      "Aniket Jivani",
      "Cosmin Safta",
      "Beckett Y. Zhou",
      "Xun Huan"
    ],
    "abstract": "We present a bifidelity Karhunen-Loève expansion (KLE) surrogate model for field-valued quantities of interest (QoIs) under uncertain inputs. The approach combines the spectral efficiency of the KLE with polynomial chaos expansions (PCEs) to preserve an explicit mapping between input uncertainties and output fields. By coupling inexpensive low-fidelity (LF) simulations that capture dominant response trends with a limited number of high-fidelity (HF) simulations that correct for systematic bias, the proposed method enables accurate and computationally affordable surrogate construction. To further improve surrogate accuracy, we form an active learning strategy that adaptively selects new HF evaluations based on the surrogate's generalization error, estimated via cross-validation and modeled using Gaussian process regression. New HF samples are then acquired by maximizing an expected improvement criterion, targeting regions of high surrogate error. The resulting BF-KLE-AL framework is demonstrated on three examples of increasing complexity: a one-dimensional analytical benchmark, a two-dimensional convection-diffusion system, and a three-dimensional turbulent round jet simulation based on Reynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy simulations (EDDES). Across these cases, the method achieves consistent improvements in predictive accuracy and sample efficiency relative to single-fidelity and random-sampling approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "physics.flu-dyn",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T04:14:44+00:00",
    "updated": "2025-11-05T04:14:44+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03756v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03125v1",
    "title": "Provable Accelerated Bayesian Optimization with Knowledge Transfer",
    "authors": [
      "Haitao Lin",
      "Boxin Zhao",
      "Mladen Kolar",
      "Chong Liu"
    ],
    "abstract": "We study how Bayesian optimization (BO) can be accelerated on a target task with historical knowledge transferred from related source tasks. Existing works on BO with knowledge transfer either do not have theoretical guarantees or achieve the same regret as BO in the non-transfer setting, $\\tilde{\\mathcal{O}}(\\sqrt{T γ_f})$, where $T$ is the number of evaluations of the target function and $γ_f$ denotes its information gain. In this paper, we propose the DeltaBO algorithm, in which a novel uncertainty-quantification approach is built on the difference function $δ$ between the source and target functions, which are allowed to belong to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions, we prove that the regret of DeltaBO is of order $\\tilde{\\mathcal{O}}(\\sqrt{T (T/N + γ_δ)})$, where $N$ denotes the number of evaluations from source tasks and typically $N \\gg T$. In many applications, source and target tasks are similar, which implies that $γ_δ$ can be much smaller than $γ_f$. Empirical studies on both real-world hyperparameter tuning tasks and synthetic functions show that DeltaBO outperforms other baseline methods and support our theoretical claims.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-05T02:25:20+00:00",
    "updated": "2025-11-05T02:25:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03125v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03087v1",
    "title": "Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models",
    "authors": [
      "Linglingzhi Zhu",
      "Jonghyeok Lee",
      "Yao Xie"
    ],
    "abstract": "Generalized linear models (GLMs) are fundamental tools for statistical modeling, with maximum likelihood estimation (MLE) serving as the classical method for parameter inference. While MLE performs well in canonical GLMs, it can become computationally inefficient near the true parameter value. In more general settings with non-canonical or fully general link functions, the resulting optimization landscape is often non-convex, non-smooth, and numerically unstable. To address these challenges, we investigate an alternative estimator based on solving the variational inequality (VI) formulation of the GLM likelihood equations, originally proposed by Juditsky and Nemirovski as an alternative for solving nonlinear least-squares problems. Unlike their focus on algorithmic convergence in monotone settings, we analyze the VI approach from a statistical perspective, comparing it systematically with the MLE. We also extend the theory of VI estimators to a broader class of link functions, including non-monotone cases satisfying a strong Minty condition, and show that it admits weaker smoothness requirements than MLE, enabling faster, more stable, and less locally trapped optimization. Theoretically, we establish both non-asymptotic estimation error bounds and asymptotic normality for the VI estimator, and further provide convergence guarantees for fixed-point and stochastic approximation algorithms. Numerical experiments show that the VI framework preserves the statistical efficiency of MLE while substantially extending its applicability to more challenging GLM settings.",
    "categories": [
      "stat.ME",
      "math.OC",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-05T00:23:42+00:00",
    "updated": "2025-11-05T00:23:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03087v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03070v1",
    "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge",
    "authors": [
      "Drago Plecko",
      "Patrik Okanovic",
      "Torsten Hoefler",
      "Elias Bareinboim"
    ],
    "abstract": "Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., \"what is the capital of England?\"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., \"what is the sex of a computer science graduate in the US?\"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "published": "2025-11-04T23:34:52+00:00",
    "updated": "2025-11-04T23:34:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03070v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03050v1",
    "title": "Precise asymptotic analysis of Sobolev training for random feature models",
    "authors": [
      "Katharine E Fisher",
      "Matthew TC Li",
      "Youssef Marzouk",
      "Timo Schorlepp"
    ],
    "abstract": "Gradient information is widely useful and available in applications, and is therefore natural to include in the training of neural networks. Yet little is known theoretically about the impact of Sobolev training -- regression with both function and gradient data -- on the generalization error of highly overparameterized predictive models in high dimensions. In this paper, we obtain a precise characterization of this training modality for random feature (RF) models in the limit where the number of trainable parameters, input dimensions, and training data tend proportionally to infinity. Our model for Sobolev training reflects practical implementations by sketching gradient data onto finite dimensional subspaces. By combining the replica method from statistical physics with linearizations in operator-valued free probability theory, we derive a closed-form description for the generalization errors of the trained RF models. For target functions described by single-index models, we demonstrate that supplementing function data with additional gradient data does not universally improve predictive performance. Rather, the degree of overparameterization should inform the choice of training method. More broadly, our results identify settings where models perform optimally by interpolating noisy function and gradient data.",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG",
      "math.PR",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T22:49:33+00:00",
    "updated": "2025-11-04T22:49:33+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03050v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03015v1",
    "title": "Discrete Bayesian Sample Inference for Graph Generation",
    "authors": [
      "Ole Petersen",
      "Marcel Kollovieh",
      "Marten Lienen",
      "Stephan Günnemann"
    ],
    "abstract": "Generating graph-structured data is crucial in applications such as molecular generation, knowledge graphs, and network analysis. However, their discrete, unordered nature makes them difficult for traditional generative models, leading to the rise of discrete diffusion and flow matching models. In this work, we introduce GraphBSI, a novel one-shot graph generative model based on Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI iteratively refines a belief over graphs in the continuous space of distribution parameters, naturally handling discrete structures. Further, we state BSI as a stochastic differential equation (SDE) and derive a noise-controlled family of SDEs that preserves the marginal distributions via an approximation of the score function. Our theoretical analysis further reveals the connection to Bayesian Flow Networks and Diffusion models. Finally, in our empirical evaluation, we demonstrate state-of-the-art performance on molecular and synthetic graph generation, outperforming existing one-shot graph generative models on the standard benchmarks Moses and GuacaMol.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-04T21:25:51+00:00",
    "updated": "2025-11-04T21:25:51+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03015v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.03000v1",
    "title": "Unifying Information-Theoretic and Pair-Counting Clustering Similarity",
    "authors": [
      "Alexander J. Gates"
    ],
    "abstract": "Comparing clusterings is central to evaluating unsupervised models, yet the many existing similarity measures can produce widely divergent, sometimes contradictory, evaluations. Clustering similarity measures are typically organized into two principal families, pair-counting and information-theoretic, reflecting whether they quantify agreement through element pairs or aggregate information across full cluster contingency tables. Prior work has uncovered parallels between these families and applied empirical normalization or chance-correction schemes, but their deeper analytical connection remains only partially understood. Here, we develop an analytical framework that unifies these families through two complementary perspectives. First, both families are expressed as weighted expansions of observed versus expected co-occurrences, with pair-counting arising as a quadratic, low-order approximation and information-theoretic measures as higher-order, frequency-weighted extensions. Second, we generalize pair-counting to $k$-tuple agreement and show that information-theoretic measures can be viewed as systematically accumulating higher-order co-assignment structure beyond the pairwise level. We illustrate the approaches analytically for the Rand index and Mutual Information, and show how other indices in each family emerge as natural extensions. Together, these views clarify when and why the two regimes diverge, relating their sensitivities directly to weighting and approximation order, and provide a principled basis for selecting, interpreting, and extending clustering similarity measures across applications.",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T21:13:32+00:00",
    "updated": "2025-11-04T21:13:32+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.03000v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02986v1",
    "title": "Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models",
    "authors": [
      "Giovanni Palla",
      "Sudarshan Babu",
      "Payam Dibaeinia",
      "James D. Pearce",
      "Donghui Li",
      "Aly A. Khan",
      "Theofanis Karaletsos",
      "Jakub M. Tomczak"
    ],
    "abstract": "Computational modeling of single-cell gene expression is crucial for understanding cellular processes, but generating realistic expression profiles remains a major challenge. This difficulty arises from the count nature of gene expression data and complex latent dependencies among genes. Existing generative models often impose artificial gene orderings or rely on shallow neural network architectures. We introduce a scalable latent diffusion model for single-cell gene expression data, which we refer to as scLDM, that respects the fundamental exchangeability property of the data. Our VAE uses fixed-size latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB) architecture, which serves dual roles: permutation-invariant pooling in the encoder and permutation-equivariant unpooling in the decoder. We enhance this framework by replacing the Gaussian prior with a latent diffusion model using Diffusion Transformers and linear interpolants, enabling high-quality generation with multi-conditional classifier-free guidance. We show its superior performance in a variety of experiments for both observational and perturbational single-cell data, as well as downstream tasks like cell-level classification.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-bio.GN"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T20:44:12+00:00",
    "updated": "2025-11-04T20:44:12+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02986v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.10661v1",
    "title": "Bayesian Evaluation of Large Language Model Behavior",
    "authors": [
      "Rachel Longjohn",
      "Shang Wu",
      "Saatvik Kher",
      "Catarina Belém",
      "Padhraic Smyth"
    ],
    "abstract": "It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "published": "2025-11-04T19:51:46+00:00",
    "updated": "2025-11-04T19:51:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.10661v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02944v1",
    "title": "Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics",
    "authors": [
      "Fengxu Li",
      "Stephanie M. Carpenter",
      "Matthew P. Buman",
      "Yonatan Mintz"
    ],
    "abstract": "A common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-04T19:46:42+00:00",
    "updated": "2025-11-04T19:46:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02944v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02757v1",
    "title": "ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models",
    "authors": [
      "Lejs Deen Behric",
      "Liang Zhang",
      "Bingcong Li",
      "Kiran Koshy Thekumparampil"
    ],
    "abstract": "Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy for finetuning large language models (LLMs) because it eliminates the memory overhead of backpropagation. However, it converges slowly due to the inherent curse of dimensionality when searching for descent directions in the high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a novel zeroth-order optimizer that accelerates convergence by adaptive directional sampling. Instead of drawing the direction uniformly at random, ConMeZO restricts the sampling to a cone centered around a momentum estimate. This concentrates the search in directions where the true gradient is more likely to lie and thus reduces the effect of high dimensions. We prove that ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically, when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than MeZO while retaining the low-memory footprint of zeroth-order methods.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-04T17:35:52+00:00",
    "updated": "2025-11-04T17:35:52+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02757v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02706v1",
    "title": "Optimizing Kernel Discrepancies via Subset Selection",
    "authors": [
      "Deyao Chen",
      "François Clément",
      "Carola Doerr",
      "Nathan Kirk"
    ],
    "abstract": "Kernel discrepancies are a powerful tool for analyzing worst-case errors in quasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such discrepancy measures, we extend the subset selection problem to the setting of kernel discrepancies, selecting an m-element subset from a large population of size $n \\gg m$. We introduce a novel subset selection algorithm applicable to general kernel discrepancies to efficiently generate low-discrepancy samples from both the uniform distribution on the unit hypercube, the traditional setting of classical QMC, and from more general distributions $F$ with known density functions by employing the kernel Stein discrepancy. We also explore the relationship between the classical $L_2$ star discrepancy and its $L_\\infty$ counterpart.",
    "categories": [
      "stat.ML",
      "cs.CG",
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T16:25:08+00:00",
    "updated": "2025-11-04T16:25:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02706v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02487v1",
    "title": "Learning CNF formulas from uniform random solutions in the local lemma regime",
    "authors": [
      "Weiming Feng",
      "Xiongxin Yang",
      "Yixiao Yu",
      "Yiyao Zhang"
    ],
    "abstract": "We study the problem of learning a $n$-variables $k$-CNF formula $Φ$ from its i.i.d. uniform random solutions, which is equivalent to learning a Boolean Markov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's algorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with bounded clause intersection size under Lovász local lemma type conditions, from $O(\\log n)$ samples; and (2) random $k$-CNFs near the satisfiability threshold, from $\\widetilde{O}(n^{\\exp(-\\sqrt{k})})$ samples. These results significantly improve the previous $O(n^k)$ sample complexity. We further establish new information-theoretic lower bounds on sample complexity for both exact and approximate learning from i.i.d. uniform random solutions.",
    "categories": [
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS",
    "published": "2025-11-04T11:22:05+00:00",
    "updated": "2025-11-04T11:22:05+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02487v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02452v1",
    "title": "An Adaptive Sampling Framework for Detecting Localized Concept Drift under Label Scarcity",
    "authors": [
      "Junghee Pyeon",
      "Davide Cacciarelli",
      "Kamran Paynabar"
    ],
    "abstract": "Concept drift and label scarcity are two critical challenges limiting the robustness of predictive models in dynamic industrial environments. Existing drift detection methods often assume global shifts and rely on dense supervision, making them ill-suited for regression tasks with local drifts and limited labels. This paper proposes an adaptive sampling framework that combines residual-based exploration and exploitation with EWMA monitoring to efficiently detect local concept drift under labeling budget constraints. Empirical results on synthetic benchmarks and a case study on electricity market demonstrate superior performance in label efficiency and drift detection accuracy.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T10:30:20+00:00",
    "updated": "2025-11-04T10:30:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02452v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02430v2",
    "title": "Efficient Solvers for SLOPE in R, Python, Julia, and C++",
    "authors": [
      "Johan Larsson",
      "Malgorzata Bogdan",
      "Krystyna Grzesiak",
      "Mathurin Massias",
      "Jonas Wallin"
    ],
    "abstract": "We present a suite of packages in R, Python, Julia, and C++ that efficiently solve the Sorted L-One Penalized Estimation (SLOPE) problem. The packages feature a highly efficient hybrid coordinate descent algorithm that fits generalized linear models (GLMs) and supports a variety of loss functions, including Gaussian, binomial, Poisson, and multinomial logistic regression. Our implementation is designed to be fast, memory-efficient, and flexible. The packages support a variety of data structures (dense, sparse, and out-of-memory matrices) and are designed to efficiently fit the full SLOPE path as well as handle cross-validation of SLOPE models, including the relaxed SLOPE. We present examples of how to use the packages and benchmarks that demonstrate the performance of the packages on both real and simulated data and show that our packages outperform existing implementations of SLOPE in terms of speed.",
    "categories": [
      "stat.CO",
      "cs.MS",
      "cs.SE",
      "stat.ML"
    ],
    "primary_category": "stat.CO",
    "published": "2025-11-04T10:03:15+00:00",
    "updated": "2025-11-17T09:31:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02430v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02419v1",
    "title": "Wasserstein Convergence of Critically Damped Langevin Diffusions",
    "authors": [
      "Stanislas Strasman",
      "Sobihan Surendran",
      "Claire Boyer",
      "Sylvain Le Corff",
      "Vincent Lemaire",
      "Antonio Ocello"
    ],
    "abstract": "Score-based Generative Models (SGMs) have achieved impressive performance in data generation across a wide range of applications and benefit from strong theoretical guarantees. Recently, methods inspired by statistical mechanics, in particular, Hamiltonian dynamics, have introduced Critically-damped Langevin Diffusions (CLDs), which define diffusion processes on extended spaces by coupling the data with auxiliary variables. These approaches, along with their associated score-matching and sampling procedures, have been shown to outperform standard diffusion-based samplers numerically. In this paper, we analyze a generalized dynamic that extends classical CLDs by introducing an additional hyperparameter controlling the noise applied to the data coordinate, thereby better exploiting the extended space. We further derive a novel upper bound on the sampling error of CLD-based generative models in the Wasserstein metric. This additional hyperparameter influences the smoothness of sample paths, and our discretization error analysis provides practical guidance for its tuning, leading to improved sampling performance.",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-04T09:49:07+00:00",
    "updated": "2025-11-04T09:49:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02419v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02401v1",
    "title": "Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks",
    "authors": [
      "Yessin Moakher",
      "Malik Tiomoko",
      "Cosme Louart",
      "Zhenyu Liao"
    ],
    "abstract": "We first study the generalization error of models that use a fixed feature representation (frozen intermediate layers) followed by a trainable readout layer. This setting encompasses a range of architectures, from deep random-feature models to echo-state networks (ESNs) with recurrent dynamics. Working in the high-dimensional regime, we apply Random Matrix Theory to derive a closed-form expression for the asymptotic generalization error. We then apply this analysis to recurrent representations and obtain concise formula that characterize their performance. Surprisingly, we show that a linear ESN is equivalent to ridge regression with an exponentially time-weighted (''memory'') input covariance, revealing a clear inductive bias toward recent inputs. Experiments match predictions: ESNs win in low-sample, short-memory regimes, while ridge prevails with more data or long-range dependencies. Our methodology provides a general framework for analyzing overparameterized models and offers insights into the behavior of deep learning networks.",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "published": "2025-11-04T09:30:31+00:00",
    "updated": "2025-11-04T09:30:31+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02401v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02373v1",
    "title": "A new class of Markov random fields enabling lightweight sampling",
    "authors": [
      "Jean-Baptiste Courbot",
      "Hugo Gangloff",
      "Bruno Colicchio"
    ],
    "abstract": "This work addresses the problem of efficient sampling of Markov random fields (MRF). The sampling of Potts or Ising MRF is most often based on Gibbs sampling, and is thus computationally expensive. We consider in this work how to circumvent this bottleneck through a link with Gaussian Markov Random fields. The latter can be sampled in several cost-effective ways, and we introduce a mapping from real-valued GMRF to discrete-valued MRF. The resulting new class of MRF benefits from a few theoretical properties that validate the new model. Numerical results show the drastic performance gain in terms of computational efficiency, as we sample at least 35x faster than Gibbs sampling using at least 37x less energy, all the while exhibiting empirical properties close to classical MRFs.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SP",
      "stat.CO"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T08:53:17+00:00",
    "updated": "2025-11-04T08:53:17+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02373v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02345v1",
    "title": "Reducing normalizing flow complexity for MCMC preconditioning",
    "authors": [
      "David Nabergoj",
      "Erik Štrumbelj"
    ],
    "abstract": "Preconditioning is a key component of MCMC algorithms that improves sampling efficiency by facilitating exploration of geometrically complex target distributions through an invertible map. While linear preconditioners are often sufficient for moderately complex target distributions, recent work has explored nonlinear preconditioning with invertible neural networks as components of normalizing flows (NFs). However, empirical and theoretical studies show that overparameterized NF preconditioners can degrade sampling efficiency and fit quality. Moreover, existing NF-based approaches do not adapt their architectures to the target distribution. Related work outside of MCMC similarly finds that suitably parameterized NFs can achieve comparable or superior performance with substantially less training time or data. We propose a factorized preconditioning architecture that reduces NF complexity by combining a linear component with a conditional NF, improving adaptability to target geometry. The linear preconditioner is applied to dimensions that are approximately Gaussian, as estimated from warmup samples, while the conditional NF models more complex dimensions. Our method yields significantly better tail samples on two complex synthetic distributions and consistently better performance on a sparse logistic regression posterior across varying likelihood and prior strengths. It also achieves higher effective sample sizes on hierarchical Bayesian model posteriors with weak likelihoods and strong funnel geometries. This approach is particularly relevant for hierarchical Bayesian model analyses with limited data and could inform current theoretical and software strides in neural MCMC design.",
    "categories": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-04T08:08:00+00:00",
    "updated": "2025-11-04T08:08:00+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02345v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02306v1",
    "title": "A Stable Lasso",
    "authors": [
      "Mahdi Nouraie",
      "Houying Zhu",
      "Samuel Muller"
    ],
    "abstract": "The Lasso has been widely used as a method for variable selection, valued for its simplicity and empirical performance. However, Lasso's selection stability deteriorates in the presence of correlated predictors. Several approaches have been developed to mitigate this limitation. In this paper, we provide a brief review of existing approaches, highlighting their limitations. We then propose a simple technique to improve the selection stability of Lasso by integrating a weighting scheme into the Lasso penalty function, where the weights are defined as an increasing function of a correlation-adjusted ranking that reflects the predictive power of predictors. Empirical evaluations on both simulated and real-world datasets demonstrate the efficacy of the proposed method. Additional numerical results demonstrate the effectiveness of the proposed approach in stabilizing other regularization-based selection methods, indicating its potential as a general-purpose solution.",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2025-11-04T06:41:47+00:00",
    "updated": "2025-11-04T06:41:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02306v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02272v2",
    "title": "Probabilistic Graph Cuts",
    "authors": [
      "Ayoub Ghriss"
    ],
    "abstract": "Probabilistic relaxations of graph cuts offer a differentiable alternative to spectral clustering, enabling end-to-end and online learning without eigendecompositions, yet prior work centered on RatioCut and lacked general guarantees and principled gradients. We present a unified probabilistic framework that covers a wide class of cuts, including Normalized Cut. Our framework provides tight analytic upper bounds on expected discrete cuts via integral representations and Gauss hypergeometric functions with closed-form forward and backward. Together, these results deliver a rigorous, numerically stable foundation for scalable, differentiable graph partitioning covering a wide range of clustering and contrastive learning objectives.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-04T05:24:56+00:00",
    "updated": "2025-11-05T06:31:46+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02272v2",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02258v1",
    "title": "Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks",
    "authors": [
      "Parsa Rangriz"
    ],
    "abstract": "This paper studies the high-dimensional scaling limits of online stochastic gradient descent (SGD) for single-layer networks. Building on the seminal work of Saad and Solla, which analyzed the deterministic (ballistic) scaling limits of SGD corresponding to the gradient flow of the population loss, we focus on the critical scaling regime of the step size. Below this critical scale, the effective dynamics are governed by ballistic (ODE) limits, but at the critical scale, new correction term appears that changes the phase diagram. In this regime, near the fixed points, the corresponding diffusive (SDE) limits of the effective dynamics reduces to an Ornstein-Uhlenbeck process under certain conditions. These results highlight how the information exponent controls sample complexity and illustrates the limitations of deterministic scaling limit in capturing the stochastic fluctuations of high-dimensional learning dynamics.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T04:52:19+00:00",
    "updated": "2025-11-04T04:52:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02258v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02137v1",
    "title": "DoFlow: Causal Generative Flows for Interventional and Counterfactual Time-Series Prediction",
    "authors": [
      "Dongze Wu",
      "Feng Qiu",
      "Yao Xie"
    ],
    "abstract": "Time-series forecasting increasingly demands not only accurate observational predictions but also causal forecasting under interventional and counterfactual queries in multivariate systems. We present DoFlow, a flow based generative model defined over a causal DAG that delivers coherent observational and interventional predictions, as well as counterfactuals through the natural encoding and decoding mechanism of continuous normalizing flows (CNFs). We also provide a supporting counterfactual recovery result under certain assumptions. Beyond forecasting, DoFlow provides explicit likelihoods of future trajectories, enabling principled anomaly detection. Experiments on synthetic datasets with various causal DAG and real world hydropower and cancer treatment time series show that DoFlow achieves accurate system-wide observational forecasting, enables causal forecasting over interventional and counterfactual queries, and effectively detects anomalies. This work contributes to the broader goal of unifying causal reasoning and generative modeling for complex dynamical systems.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-04T00:01:25+00:00",
    "updated": "2025-11-04T00:01:25+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02137v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02123v1",
    "title": "Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits",
    "authors": [
      "Xuheng Li",
      "Quanquan Gu"
    ],
    "abstract": "Variance-dependent regret bounds have received increasing attention in recent studies on contextual bandits. However, most of these studies are focused on upper confidence bound (UCB)-based bandit algorithms, while sampling based bandit algorithms such as Thompson sampling are still understudied. The only exception is the LinVDTS algorithm (Xu et al., 2023), which is limited to linear reward function and its regret bound is not optimal with respect to the model dimension. In this paper, we present FGTSVA, a variance-aware Thompson Sampling algorithm for contextual bandits with general reward function with optimal regret bound. At the core of our analysis is an extension of the decoupling coefficient, a technique commonly used in the analysis of Feel-good Thompson sampling (FGTS) that reflects the complexity of the model space. With the new decoupling coefficient denoted by $\\mathrm{dc}$, FGTS-VA achieves the regret of $\\tilde{O}(\\sqrt{\\mathrm{dc}\\cdot\\log|\\mathcal{F}|\\sum_{t=1}^Tσ_t^2}+\\mathrm{dc})$, where $|\\mathcal{F}|$ is the size of the model space, $T$ is the total number of rounds, and $σ_t^2$ is the subgaussian norm of the noise (e.g., variance when the noise is Gaussian) at round $t$. In the setting of contextual linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms using weighted linear regression (Zhou and Gu, 2022).",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T23:25:41+00:00",
    "updated": "2025-11-03T23:25:41+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02123v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02102v1",
    "title": "Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning",
    "authors": [
      "Melanie Mayer",
      "Kimberly Lactaoen",
      "Gary E. Weissman",
      "Blanca E. Himes",
      "Rebecca A. Hubbard"
    ],
    "abstract": "Objectives: Unsupervised learning with electronic health record (EHR) data has shown promise for phenotype discovery, but approaches typically disregard existing clinical information, limiting interpretability. We operationalize a Bayesian latent class framework for phenotyping that incorporates domain-specific knowledge to improve clinical meaningfulness of EHR-derived phenotypes and illustrate its utility by identifying an asthma sub-phenotype informed by features of Type 2 (T2) inflammation.   Materials and methods: We illustrate a framework for incorporating clinical knowledge into a Bayesian latent class model via informative priors to guide unsupervised clustering toward clinically relevant subgroups. This approach models missingness, accounting for potential missing-not-at-random patterns, and provides patient-level probabilities for phenotype assignment with uncertainty. Using reusable and flexible code, we applied the model to a large asthma EHR cohort, specifying informative priors for T2 inflammation-related features and weakly informative priors for other clinical variables, allowing the data to inform posterior distributions.   Results and Conclusion: Using encounter data from January 2017 to February 2024 for 44,642 adult asthma patients, we found a bimodal posterior distribution of phenotype assignment, indicating clear class separation. The T2 inflammation-informed class (38.7%) was characterized by elevated eosinophil levels and allergy markers, plus high healthcare utilization and medication use, despite weakly informative priors on the latter variables. These patterns suggest an \"uncontrolled T2-high\" sub-phenotype. This demonstrates how our Bayesian latent class modeling approach supports hypothesis generation and cohort identification in EHR-based studies of heterogeneous diseases without well-established phenotype definitions.",
    "categories": [
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "stat.AP",
    "published": "2025-11-03T22:25:07+00:00",
    "updated": "2025-11-03T22:25:07+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02102v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.02053v1",
    "title": "Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications",
    "authors": [
      "Jinchao Feng",
      "Charles Kulick",
      "Sui Tang"
    ],
    "abstract": "We develop a Gaussian process framework for learning interaction kernels in multi-species interacting particle systems from trajectory data. Such systems provide a canonical setting for multiscale modeling, where simple microscopic interaction rules generate complex macroscopic behaviors. While our earlier work established a Gaussian process approach and convergence theory for single-species systems, and later extended to second-order models with alignment and energy-type interactions, the multi-species setting introduces new challenges: heterogeneous populations interact both within and across species, the number of unknown kernels grows, and asymmetric interactions such as predator-prey dynamics must be accommodated. We formulate the learning problem in a nonparametric Bayesian setting and establish rigorous statistical guarantees. Our analysis shows recoverability of the interaction kernels, provides quantitative error bounds, and proves statistical optimality of posterior estimators, thereby unifying and generalizing previous single-species theory. Numerical experiments confirm the theoretical predictions and demonstrate the effectiveness of the proposed approach, highlighting its advantages over existing kernel-based methods. This work contributes a complete statistical framework for data-driven inference of interaction laws in multi-species systems, advancing the broader multiscale modeling program of connecting microscopic particle dynamics with emergent macroscopic behavior.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.NA",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T20:38:38+00:00",
    "updated": "2025-11-03T20:38:38+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.02053v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01847v1",
    "title": "Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure",
    "authors": [
      "Zhi Wang",
      "Chicheng Zhang",
      "Ramya Korlakai Vinayak"
    ],
    "abstract": "In lifelong learning, a learner faces a sequence of tasks with shared structure and aims to identify and leverage it to accelerate learning. We study the setting where such structure is captured by a common representation of data. Unlike multi-task learning or learning-to-learn, where tasks are available upfront to learn the representation, lifelong learning requires the learner to make use of its existing knowledge while continually gathering partial information in an online fashion. In this paper, we consider a generalized framework of lifelong representation learning. We propose a simple algorithm that uses multi-task empirical risk minimization as a subroutine and establish a sample complexity bound based on a new notion we introduce--the task-eluder dimension. Our result applies to a wide range of learning problems involving general function classes. As concrete examples, we instantiate our result on classification and regression tasks under noise.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T18:54:56+00:00",
    "updated": "2025-11-03T18:54:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01847v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01795v1",
    "title": "Fractional Diffusion Bridge Models",
    "authors": [
      "Gabriel Nobis",
      "Maximilian Springenberg",
      "Arina Belova",
      "Rembert Daems",
      "Christoph Knochenhauer",
      "Manfred Opper",
      "Tolga Birdal",
      "Wojciech Samek"
    ],
    "abstract": "We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schrödinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_α$ atomic positions in protein structure prediction and lower Fréchet Inception Distance (FID) in unpaired image translation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T17:51:10+00:00",
    "updated": "2025-11-03T17:51:10+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01795v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01734v1",
    "title": "A Proof of Learning Rate Transfer under $μ$P",
    "authors": [
      "Soufiane Hayou"
    ],
    "abstract": "We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $μ$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $μP$, the optimal learning rate converges to a \\emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T16:45:47+00:00",
    "updated": "2025-11-03T16:45:47+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01734v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01641v1",
    "title": "Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking",
    "authors": [
      "Xiaopeng Ke",
      "Yihan Yu",
      "Ruyue Zhang",
      "Zhishuo Zhou",
      "Fangzhou Shi",
      "Chang Men",
      "Zhengdan Zhu"
    ],
    "abstract": "Counterfactual causal inference faces significant challenges when extended to multi-category, multi-valued treatments, where complex cross-effects between heterogeneous interventions are difficult to model. Existing methodologies remain constrained to binary or single-type treatments and suffer from restrictive assumptions, limited scalability, and inadequate evaluation frameworks for complex intervention scenarios.   We present XTNet, a novel network architecture for multi-category, multi-valued treatment effect estimation. Our approach introduces a cross-effect estimation module with dynamic masking mechanisms to capture treatment interactions without restrictive structural assumptions. The architecture employs a decomposition strategy separating basic effects from cross-treatment interactions, enabling efficient modeling of combinatorial treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that accounts for treatment costs and interaction effects. Extensive experiments on synthetic and real-world datasets demonstrate that XTNet consistently outperforms state-of-the-art baselines in both ranking accuracy and effect estimation quality. The results of the real-world A/B test further confirm its effectiveness.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T14:50:02+00:00",
    "updated": "2025-11-03T14:50:02+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01641v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01628v1",
    "title": "Partial Trace-Class Bayesian Neural Networks",
    "authors": [
      "Arran Carter",
      "Torben Sell"
    ],
    "abstract": "Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in deep learning, but often come at a prohibitive computational cost. We propose three different innovative architectures of partial trace-class Bayesian neural networks (PaTraC BNNs) that enable uncertainty quantification comparable to standard BNNs but use significantly fewer Bayesian parameters. These PaTraC BNNs have computational and statistical advantages over standard Bayesian neural networks in terms of speed and memory requirements. Our proposed methodology therefore facilitates reliable, robust, and scalable uncertainty quantification in neural networks. The three architectures build on trace-class neural network priors which induce an ordering of the neural network parameters, and are thus a natural choice in our framework. In a numerical simulation study, we verify the claimed benefits, and further illustrate the performance of our proposed methodology on a real-world dataset.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T14:38:35+00:00",
    "updated": "2025-11-03T14:38:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01628v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01605v1",
    "title": "Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent",
    "authors": [
      "Daniel Busbib",
      "Ami Wiesel"
    ],
    "abstract": "We consider covariance estimation under Toeplitz structure. Numerous sophisticated optimization methods have been developed to maximize the Gaussian log-likelihood under Toeplitz constraints. In contrast, recent advances in deep learning demonstrate the surprising power of simple gradient descent (GD) applied to overparameterized models. Motivated by this trend, we revisit Toeplitz covariance estimation through the lens of overparameterized GD. We model the $P\\times P$ covariance as a sum of $K$ complex sinusoids with learnable parameters and optimize them via GD. We show that when $K = P$, GD may converge to suboptimal solutions. However, mild overparameterization ($K = 2P$ or $4P$) consistently enables global convergence from random initializations. We further propose an accelerated GD variant with separate learning rates for amplitudes and frequencies. When frequencies are fixed and only amplitudes are optimized, we prove that the optimization landscape is asymptotically benign and any stationary point recovers the true covariance. Finally, numerical experiments demonstrate that overparameterized GD can match or exceed the accuracy of state-of-the-art methods in challenging settings, while remaining simple and scalable.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T14:07:53+00:00",
    "updated": "2025-11-03T14:07:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01605v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01535v1",
    "title": "HFNO: an interpretable data-driven decomposition strategy for turbulent flows",
    "authors": [
      "Marco Cayuela",
      "Vincent Le Chenadec",
      "Peter Schmid",
      "Taraneh Sayadi"
    ],
    "abstract": "Fourier Neural Operators (FNOs) have demonstrated exceptional accuracy in mapping functional spaces by leveraging Fourier transforms to establish a connection with underlying physical principles. However, their opaque inner workings often constitute an obstacle to physical interpretability. This work introduces Hierarchical Fourier Neural Operators (HFNOs), a novel FNO-based architecture tailored for reduced-order modeling of turbulent fluid flows, designed to enhance interpretability by explicitly separating fluid behavior across scales. The proposed architecture processes wavenumber bins in parallel, enabling the approximation of dispersion relations and non-linear interactions. Inputs are lifted to a higher-dimensional space, Fourier-transformed, and partitioned into wavenumber bins. Each bin is processed by a Fully Connected Neural Network (FCNN), with outputs subsequently padded, summed, and inverse-transformed back into physical space. A final transformation refines the output in physical space as a correction model, by means of one of the following architectures: Convolutional Neural Network (CNN) and Echo State Network (ESN). We evaluate the proposed model on a series of increasingly complex dynamical systems: first on the one-dimensional Kuramoto-Sivashinsky equation, then on the two-dimensional Kolmogorov flow, and finally on the prediction of wall shear stress in turbulent channel flow, given the near-wall velocity field. In all test cases, the model demonstrates its ability to decompose turbulent flows across various scales, opening up the possibility of increased interpretability and multiscale modeling of such flows.",
    "categories": [
      "physics.flu-dyn",
      "stat.ML"
    ],
    "primary_category": "physics.flu-dyn",
    "published": "2025-11-03T12:57:19+00:00",
    "updated": "2025-11-03T12:57:19+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01535v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01292v1",
    "title": "Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift",
    "authors": [
      "Samet Demir",
      "Zafer Dogan"
    ],
    "abstract": "Pretrained Transformers excel at in-context learning (ICL), inferring new tasks from only a handful of examples. Yet, their ICL performance can degrade sharply under distribution shift between pretraining and test data, a regime increasingly common in real-world deployments. While recent empirical work hints that adjusting the attention temperature in the softmax can enhance Transformer performance, the attention temperature's role in ICL under distribution shift remains unexplored. This paper provides the first theoretical and empirical study of attention temperature for ICL under distribution shift. Using a simplified but expressive \"linearized softmax\" framework, we derive closed-form generalization error expressions and prove that shifts in input covariance or label noise substantially impair ICL, but that an optimal attention temperature exists which minimizes this error. We then validate our predictions through extensive simulations on linear regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on question-answering benchmarks. Our results establish attention temperature as a principled and powerful mechanism for improving the robustness of ICL in pretrained Transformers, advancing theoretical understanding and providing actionable guidance for selecting attention temperature in practice.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T07:18:27+00:00",
    "updated": "2025-11-03T07:18:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01292v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01267v1",
    "title": "A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation",
    "authors": [
      "Yiyang Yang",
      "Xiejian Chi",
      "Shanxing Gao",
      "Kaidong Wang",
      "Yao Wang"
    ],
    "abstract": "Data quality is critical to Intelligent Transportation Systems (ITS), as complete and accurate traffic data underpin reliable decision-making in traffic control and management. Recent advances in low-rank tensor recovery algorithms have shown strong potential in capturing the inherent structure of high-dimensional traffic data and restoring degraded observations. However, traditional batch-based methods demand substantial computational and storage resources, which limits their scalability in the face of continuously expanding traffic data volumes. Moreover, recent online tensor recovery methods often suffer from severe performance degradation in complex real-world scenarios due to their insufficient exploitation of the intrinsic structural properties of traffic data. To address these challenges, we reformulate the traffic data recovery problem within a streaming framework, and propose a novel online robust tensor recovery algorithm that simultaneously leverages both the global spatio-temporal correlations and local consistency of traffic data, achieving high recovery accuracy and significantly improved computational efficiency in large-scale scenarios. Our method is capable of simultaneously handling missing and anomalous values in traffic data, and demonstrates strong adaptability across diverse missing patterns. Experimental results on three real-world traffic datasets demonstrate that the proposed approach achieves high recovery accuracy while significantly improving computational efficiency by up to three orders of magnitude compared to state-of-the-art batch-based methods. These findings highlight the potential of the proposed approach as a scalable and effective solution for traffic data quality enhancement in ITS.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T06:39:56+00:00",
    "updated": "2025-11-03T06:39:56+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01267v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01234v1",
    "title": "A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization",
    "authors": [
      "Min Gan",
      "Guang-Yong Chen",
      "Yang Yi",
      "Lin Yang"
    ],
    "abstract": "The proliferation of saddle points, rather than poor local minima, is increasingly understood to be a primary obstacle in large-scale non-convex optimization for machine learning. Variable elimination algorithms, like Variable Projection (VarPro), have long been observed to exhibit superior convergence and robustness in practice, yet a principled understanding of why they so effectively navigate these complex energy landscapes has remained elusive. In this work, we provide a rigorous geometric explanation by comparing the optimization landscapes of the original and reduced formulations. Through a rigorous analysis based on Hessian inertia and the Schur complement, we prove that variable elimination fundamentally reshapes the critical point structure of the objective function, revealing that local maxima in the reduced landscape are created from, and correspond directly to, saddle points in the original formulation. Our findings are illustrated on the canonical problem of non-convex matrix factorization, visualized directly on two-parameter neural networks, and finally validated in training deep Residual Networks, where our approach yields dramatic improvements in stability and convergence to superior minima. This work goes beyond explaining an existing method; it establishes landscape simplification via saddle point transformation as a powerful principle that can guide the design of a new generation of more robust and efficient optimization algorithms.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T05:19:43+00:00",
    "updated": "2025-11-03T05:19:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01234v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01196v1",
    "title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation",
    "authors": [
      "Jicong Fan"
    ],
    "abstract": "Missing data is a fundamental challenge in data science, significantly hindering analysis and decision-making across a wide range of disciplines, including healthcare, bioinformatics, social science, e-commerce, and industrial monitoring. Despite decades of research and numerous imputation methods, the literature remains fragmented across fields, creating a critical need for a comprehensive synthesis that connects statistical foundations with modern machine learning advances. This work systematically reviews core concepts-including missingness mechanisms, single versus multiple imputation, and different imputation goals-and examines problem characteristics across various domains. It provides a thorough categorization of imputation methods, spanning classical techniques (e.g., regression, the EM algorithm) to modern approaches like low-rank and high-rank matrix completion, deep learning models (autoencoders, GANs, diffusion models, graph neural networks), and large language models. Special attention is given to methods for complex data types, such as tensors, time series, streaming data, graph-structured data, categorical data, and multimodal data. Beyond methodology, we investigate the crucial integration of imputation with downstream tasks like classification, clustering, and anomaly detection, examining both sequential pipelines and joint optimization frameworks. The review also assesses theoretical guarantees, benchmarking resources, and evaluation metrics. Finally, we identify critical challenges and future directions, emphasizing model selection and hyperparameter optimization, the growing importance of privacy-preserving imputation via federated learning, and the pursuit of generalizable models that can adapt across domains and data types, thereby outlining a roadmap for future research.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T03:43:43+00:00",
    "updated": "2025-11-03T03:43:43+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01196v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01190v1",
    "title": "Analyzing the Power of Chain of Thought through Memorization Capabilities",
    "authors": [
      "Lijia Yu",
      "Xiao-Shan Gao",
      "Lijun Zhang"
    ],
    "abstract": "It has been shown that the chain of thought (CoT) can enhance the power of large language models (LLMs) to solve certain mathematical reasoning problems. However, the capacity of CoT is still not fully explored. As an important instance, the following basic question has not yet been answered: Does CoT expand the capability of transformers across all reasoning tasks? We demonstrate that reasoning with transformers is essentially a memorization problem for reasoning datasets. Thus, examining the power of CoT across all reasoning tasks amounts to analyzing the memorization capabilities of CoT transformers. In this paper, we give a complete description of the memorization capabilities of fixed-precision transformers with or without CoT and give a negative answer to the above-mentioned question. Precisely, we first give necessary and sufficient conditions for fixed-precision transformers with and without CoT to memorize a finite reasoning dataset and show that these two conditions do not imply each other. Then, we give lower and upper bounds for the number of parameters needed for transformers with or without CoT to memorize a finite reasoning dataset with $N$ elements, which are $\\overlineΘ(N)$ in all cases. This implies that there exist reasoning tasks for which CoT does not enhance the reasoning power of transformers, leading to a negative answer to the above-mentioned question. Finally, we give the first results on memorizing infinite reasoning datasets by CoT transformers and show that some simple infinite datasets cannot be memorized by transformers with or without CoT.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T03:31:42+00:00",
    "updated": "2025-11-03T03:31:42+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01190v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01140v1",
    "title": "Few-Shot Multimodal Medical Imaging: A Theoretical Framework",
    "authors": [
      "Md Talha Mohsin",
      "Ismail Abdulrashid"
    ],
    "abstract": "Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-03T01:21:50+00:00",
    "updated": "2025-11-03T01:21:50+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01140v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01137v1",
    "title": "Regularization Implies balancedness in the deep linear network",
    "authors": [
      "Kathryn Lindsey",
      "Govind Menon"
    ],
    "abstract": "We use geometric invariant theory (GIT) to study the deep linear network (DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer is minimized on the balanced manifold. This allows us to decompose the training dynamics into two distinct gradient flows: a regularizing flow on fibers and a learning flow on the balanced manifold. We show that the regularizing flow is exactly solvable using the moment map.   This approach provides a common mathematical framework for balancedness in deep learning and linear systems theory. We use this framework to interpret balancedness in terms of model reduction and Bayesian principles.",
    "categories": [
      "cs.LG",
      "math.AG",
      "math.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-03T01:19:26+00:00",
    "updated": "2025-11-03T01:19:26+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01137v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01096v1",
    "title": "Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes",
    "authors": [
      "Alex Boyd",
      "Andrew Warrington",
      "Taha Kass-Hout",
      "Parminder Bhatia",
      "Danica Xiao"
    ],
    "abstract": "Foundational marked temporal point process (MTPP) models, such as the Hawkes process, often use inexpressive model families in order to offer interpretable parameterizations of event data. On the other hand, neural MTPPs models forego this interpretability in favor of absolute predictive performance. In this work, we present a new family MTPP models: the hyper Hawkes process (HHP), which aims to be as flexible and performant as neural MTPPs, while retaining interpretable aspects. To achieve this, the HHP extends the classical Hawkes process to increase its expressivity by first expanding the dimension of the process into a latent space, and then introducing a hypernetwork to allow time- and data-dependent dynamics. These extensions define a highly performant MTPP family, achieving state-of-the-art performance across a range of benchmark tasks and metrics. Furthermore, by retaining the linearity of the recurrence, albeit now piecewise and conditionally linear, the HHP also retains much of the structure of the original Hawkes process, which we exploit to create direct probes into how the model creates predictions. HHP models therefore offer both state-of-the-art predictions, while also providing an opportunity to ``open the box'' and inspect how predictions were generated.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-02T22:10:08+00:00",
    "updated": "2025-11-02T22:10:08+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01096v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01069v1",
    "title": "Happiness as a Measure of Fairness",
    "authors": [
      "Georg Pichler",
      "Marco Romanelli",
      "Pablo Piantanida"
    ],
    "abstract": "In this paper, we propose a novel fairness framework grounded in the concept of happiness, a measure of the utility each group gains fromdecisionoutcomes. Bycapturingfairness through this intuitive lens, we not only offer a more human-centered approach, but also one that is mathematically rigorous: In order to compute the optimal, fair post-processing strategy, only a linear program needs to be solved. This makes our method both efficient and scalable with existing optimization tools. Furthermore, it unifies and extends several well-known fairness definitions, and our empirical results highlight its practical strengths across diverse scenarios.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-02T20:27:16+00:00",
    "updated": "2025-11-02T20:27:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01069v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01064v1",
    "title": "Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry",
    "authors": [
      "Charles C. Margossian",
      "Lawrence K. Saul"
    ],
    "abstract": "We extend several recent results providing symmetry-based guarantees for variational inference (VI) with location-scale families. VI approximates a target density~$p$ by the best match $q^*$ in a family $Q$ of tractable distributions that in general does not contain $p$. It is known that VI can recover key properties of $p$, such as its mean and correlation matrix, when $p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the reverse Kullback-Leibler divergence. We extend these guarantees in two important directions. First, we provide symmetry-based guarantees for a broader family of divergences, highlighting the properties of variational objectives under which VI provably recovers the mean and correlation matrix. Second, we obtain further guarantees for VI when the target density $p$ exhibits even and elliptical symmetries in some but not all of its coordinates. These partial symmetries arise naturally in Bayesian hierarchical models, where the prior induces a challenging geometry but still possesses axes of symmetry. We illustrate these theoretical results in a number of experimental settings.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-02T20:10:57+00:00",
    "updated": "2025-11-02T20:10:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01064v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01037v1",
    "title": "Binary perceptron computational gap -- a parametric fl RDT view",
    "authors": [
      "Mihailo Stojnic"
    ],
    "abstract": "Recent studies suggest that asymmetric binary perceptron (ABP) likely exhibits the so-called statistical-computational gap characterized with the appearance of two phase transitioning constraint density thresholds: \\textbf{\\emph{(i)}} the \\emph{satisfiability threshold} $α_c$, below/above which ABP succeeds/fails to operate as a storage memory; and \\textbf{\\emph{(ii)}} \\emph{algorithmic threshold} $α_a$, below/above which one can/cannot efficiently determine ABP's weight so that it operates as a storage memory.   We consider a particular parametric utilization of \\emph{fully lifted random duality theory} (fl RDT) [85] and study its potential ABP's algorithmic implications. A remarkable structural parametric change is uncovered as one progresses through fl RDT lifting levels. On the first two levels, the so-called $\\c$ sequence -- a key parametric fl RDT component -- is of the (natural) decreasing type. A change of such phenomenology on higher levels is then connected to the $α_c$ -- $α_a$ threshold change. Namely, on the second level concrete numerical values give for the critical constraint density $α=α_c\\approx 0.8331$. While progressing through higher levels decreases this estimate, already on the fifth level we observe a satisfactory level of convergence and obtain $α\\approx 0.7764$. This allows to draw two striking parallels: \\textbf{\\emph{(i)}} the obtained constraint density estimate is in a remarkable agrement with range $α\\in (0.77,0.78)$ of clustering defragmentation (believed to be responsible for failure of locally improving algorithms) [17,88]; and \\textbf{\\emph{(ii)}} the observed change of $\\c$ sequence phenomenology closely matches the one of the negative Hopfield model for which the existence of efficient algorithms that closely approach similar type of threshold has been demonstrated recently [87].",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-02T18:23:49+00:00",
    "updated": "2025-11-02T18:23:49+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01037v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.01937v1",
    "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR",
    "authors": [
      "Abdelaziz Bounhar",
      "Hadi Abdine",
      "Evan Dufraisse",
      "Ahmad Chamma",
      "Amr Mohamed",
      "Dani Bouch",
      "Michalis Vazirgiannis",
      "Guokan Shang"
    ],
    "abstract": "Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a \\textbf{model that conflates ``thinking longer'' with ``thinking better''}. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \\textbf{\\emph{emergent brevity for free}}: the model learns to solve harder problems without inflating the output length, \\textbf{ despite the absence of any explicit length penalization}. RLVR experiments using this approach on \\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at \\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and models on \\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging Face}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-02T17:29:16+00:00",
    "updated": "2025-11-02T17:29:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.01937v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00958v1",
    "title": "The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks",
    "authors": [
      "Khoat Than"
    ],
    "abstract": "Normalization methods are fundamental components of modern deep neural networks (DNNs). Empirically, they are known to stabilize optimization dynamics and improve generalization. However, the underlying theoretical mechanism by which normalization contributes to both optimization and generalization remains largely unexplained, especially when using many normalization layers in a DNN architecture.   In this work, we develop a theoretical framework that elucidates the role of normalization through the lens of capacity control. We prove that an unnormalized DNN can exhibit exponentially large Lipschitz constants with respect to either its parameters or inputs, implying excessive functional capacity and potential overfitting. Such bad DNNs are uncountably many. In contrast, the insertion of normalization layers provably can reduce the Lipschitz constant at an exponential rate in the number of normalization operations. This exponential reduction yields two fundamental consequences: (1) it smooths the loss landscape at an exponential rate, facilitating faster and more stable optimization; and (2) it constrains the effective capacity of the network, thereby enhancing generalization guarantees on unseen data. Our results thus offer a principled explanation for the empirical success of normalization methods in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-02T14:38:20+00:00",
    "updated": "2025-11-02T14:38:20+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00958v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00904v1",
    "title": "Random Spiking Neural Networks are Stable and Spectrally Simple",
    "authors": [
      "Ernesto Araya",
      "Massimiliano Datres",
      "Gitta Kutyniok"
    ],
    "abstract": "Spiking neural networks (SNNs) are a promising paradigm for energy-efficient computation, yet their theoretical foundations-especially regarding stability and robustness-remain limited compared to artificial neural networks. In this work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the lens of Boolean function analysis. We focus on noise sensitivity and stability in classification tasks, quantifying how input perturbations affect outputs. Our main result shows that wide LIF-SNN classifiers are stable on average, a property explained by the concentration of their Fourier spectrum on low-frequency components. Motivated by this, we introduce the notion of spectral simplicity, which formalizes simplicity in terms of Fourier spectrum concentration and connects our analysis to the simplicity bias observed in deep networks. Within this framework, we show that random LIF-SNNs are biased toward simple functions. Experiments on trained networks confirm that these stability properties persist in practice. Together, these results provide new insights into the stability and robustness properties of SNNs.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-02T11:55:27+00:00",
    "updated": "2025-11-02T11:55:27+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00904v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00849v1",
    "title": "Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection",
    "authors": [
      "Zhexiao Huang",
      "Weihao He",
      "Shutao Deng",
      "Junzhe Chen",
      "Chao Yuan",
      "Hongxin Wang",
      "Changsheng Zhou"
    ],
    "abstract": "Out-of-distribution (OOD) detection is essential for deploying deep learning models in open-world environments. Existing approaches, such as energy-based scoring and gradient-projection methods, typically rely on high-dimensional representations to separate in-distribution (ID) and OOD samples. We introduce P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and theoretically grounded method that operates in the orthogonal complement of the principal subspace defined by ID features. P-OCS applies a single projected perturbation restricted to this complementary subspace, enhancing subtle ID-OOD distinctions while preserving the geometry of ID representations. We show that a one-step update is sufficient in the small-perturbation regime and provide convergence guarantees for the resulting detection score. Experiments across multiple architectures and datasets demonstrate that P-OCS achieves state-of-the-art OOD detection with negligible computational cost and without requiring model retraining, access to OOD data, or changes to model architecture.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "published": "2025-11-02T08:21:13+00:00",
    "updated": "2025-11-02T08:21:13+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00849v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00727v1",
    "title": "Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data",
    "authors": [
      "Xuelin Yang",
      "Licong Lin",
      "Susan Athey",
      "Michael I. Jordan",
      "Guido W. Imbens"
    ],
    "abstract": "We develop new methods to integrate experimental and observational data in causal inference. While randomized controlled trials offer strong internal validity, they are often costly and therefore limited in sample size. Observational data, though cheaper and often with larger sample sizes, are prone to biases due to unmeasured confounders. To harness their complementary strengths, we propose a systematic framework that formulates causal estimation as an empirical risk minimization (ERM) problem. A full model containing the causal parameter is obtained by minimizing a weighted combination of experimental and observational losses--capturing the causal parameter's validity and the full model's fit, respectively. The weight is chosen through cross-validation on the causal parameter across experimental folds. Our experiments on real and synthetic data show the efficacy and reliability of our method. We also provide theoretical non-asymptotic error bounds.",
    "categories": [
      "econ.EM",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "econ.EM",
    "published": "2025-11-01T22:24:16+00:00",
    "updated": "2025-11-01T22:24:16+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00727v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00708v1",
    "title": "Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition",
    "authors": [
      "Quan Zhou"
    ],
    "abstract": "We study the theoretical complexity of simulated tempering for sampling from mixtures of log-concave components differing only by location shifts. The main result establishes the first polynomial-time guarantee for simulated tempering combined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to the problem dimension $d$, maximum mode displacement $D$, and logarithmic accuracy $\\log ε^{-1}$. The proof builds on a general state decomposition theorem for $s$-conductance, applied to an auxiliary Markov chain constructed on an augmented space. We also obtain an improved complexity estimate for simulated tempering combined with random-walk Metropolis. Our bounds assume an inverse-temperature ladder with smallest value $β_1 = O(D^{-2})$ and spacing $β_{i+1}/β_i = 1 + O( d^{-1/2} )$, both of which are shown to be asymptotically optimal up to logarithmic factors.",
    "categories": [
      "stat.CO",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "stat.CO",
    "published": "2025-11-01T21:16:35+00:00",
    "updated": "2025-11-01T21:16:35+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00708v1",
    "venue": "arXiv",
    "year": 2025
  },
  {
    "paper_id": "2511.00704v2",
    "title": "Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift",
    "authors": [
      "Morgan Lee",
      "Artem Frenk",
      "Eamon Worden",
      "Karish Gupta",
      "Thinh Pham",
      "Ethan Croteau",
      "Neil Heffernan"
    ],
    "abstract": "Knowledge Tracing (KT) has been an established problem in the educational data mining field for decades, and it is commonly assumed that the underlying learning process being modeled remains static. Given the ever-changing landscape of online learning platforms (OLPs), we investigate how concept drift and changing student populations can impact student behavior within an OLP through testing model performance both within a single academic year and across multiple academic years. Four well-studied KT models were applied to five academic years of data to assess how susceptible KT models are to concept drift. Through our analysis, we find that all four families of KT models can exhibit degraded performance, Bayesian Knowledge Tracing (BKT) remains the most stable KT model when applied to newer data, while more complex, attention based models lose predictive power significantly faster.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2025-11-01T20:56:16+00:00",
    "updated": "2025-11-04T09:15:06+00:00",
    "pdf_url": "https://arxiv.org/pdf/2511.00704v2",
    "venue": "arXiv",
    "year": 2025
  }
]