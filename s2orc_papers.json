[
  {
    "paper_id":"f5e71b2f6d5ac644c45d6186cef2a79f0b383de2",
    "title":"Revolutionising Translation Technology: A Comparative Study of Variant Transformer Models - BERT, GPT, and T5",
    "authors":[
      "Zaki Muhammad Zayyanu"
    ],
    "abstract":"Recently, transformer-based models have reshaped the landscape of Natural Language Processing (NLP), particularly in the domain of Machine Translation (MT). this study explores three revolutionary transformer models: Bidirectional Encoder Representations from Transformers (BERT), Generative Pretrained Transformer (GPT), and Text-to-Text Transfer Transformer (T5). The study delves into their architecture, capabilities, and applications in the context of translation technology. The study begins by discussing the evolution of machine translation from rule-based to statistical machine translation and finally to transformer models. The models have distinct architectures and purposes which pushed the limits of MT and have been instrumental in revolutionising the field. The study found significant contributions of the models in the advancement of NLP tasks including translation technology. Using comparative approach, the study further elaborates on each model’s design and utility. BERT is strong in excelling in tasks requiring a deep understanding of the context. GPT is excellent for tasks such as text generation, translation and creative writing. While the strengths of T5 is text-to-text framework by simplifying the taskspecific architectures, making it easy to perform different NLP tasks. Recognising these models’ unique features allows translators to select the best one for particular translation tasks and adjust them for better accuracy, fluency, and cultural relevance in translations. The study concludes that the models bridge language barriers, improve cross-cultural communication and pave way for more accurate and natural translations in the future. The study also points out that language processing models are continually evolving but understanding BERT, GPT, and T5’s specific features is key for ongoing development in translation technology.",
    "venue":"Computer Science &amp; Engineering: An International Journal",
    "year":2024,
    "publication_date":"2024-06-28",
    "citation_count":18,
    "reference_count":38,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.5121\/cseij.2024.14302",
      "CorpusId":270739858
    }
  },
  {
    "paper_id":"4d4195380b7848a1306d9badc51f744a0fb69380",
    "title":"Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey",
    "authors":[
      "Benyamin Ghojogh",
      "A. Ghodsi"
    ],
    "abstract":"This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequence-to-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.",
    "venue":"",
    "year":2020,
    "publication_date":"2020-12-17",
    "citation_count":64,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.31219\/osf.io\/m6gcn",
      "CorpusId":242275301
    }
  },
  {
    "paper_id":"943c992045ad69ae34a31ad5d9a022b9222d7430",
    "title":"Enhancing Agricultural Knowledge Management using an efficient & Novel Ontology-Based Approach Leveraging BERT-GPT and Graph Recurrent Q Learning Network",
    "authors":[
      "Saurabh Bhattacharya",
      "D. Pandey"
    ],
    "abstract":": - Agriculture, as one of the key drivers for human civilization, demands efficient methods for managing the extensive domain-specific knowledge. While ontologies, structured sets of terms and relationships within a specific domain, have played a pivotal role in structuring agricultural data, existing models lack the precision and adaptability necessary for the dynamic agricultural environment. This paper proposes a novel framework that integrates the strengths of BERT (Bidirectional Encoder Representations from Transformers) GPT (Generative Pre-trained Transformer), and Graph Recurrent Q Learning Network (GRQLN) for developing a dynamic and efficient ontology tool. BERT-GPT, with its powerful natural language processing capabilities, allows for accurate feature extraction from diverse text data, including government and farming sources. Meanwhile, GRQLN leverages graph neural networks and reinforcement learning to convert these features into an ontology graph, optimizing the representation process. The integration of these advanced technologies not only addresses the limitations of existing models but also enhances the precision of ontological query retrieval by 8.5%, accuracy by 8.3%, recall by 10.4%, AUC by 9.5%, and specificity by 9.4%, while reducing delay by 4.5%. The proposed model is a significant contribution to the field, offering a robust tool that empowers stakeholders with actionable insights derived from a vast expanse of agricultural knowledge, ultimately facilitating informed decision-making in the ever-evolving landscape of agricultural operations.",
    "venue":"Journal of Electrical Systems",
    "year":2024,
    "publication_date":null,
    "citation_count":1,
    "reference_count":25,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.52783\/jes.770",
      "CorpusId":269201304
    }
  },
  {
    "paper_id":"1bd92511507ec6c42edbf3679dbafd5a583b3559",
    "title":"Enhanced Multi-Class Model Evaluation: Analyzing BERT, GPT-2, and LLaMA with Precision, Recall, and F1-Score Metrics",
    "authors":[
      "Shreyan Simhadri",
      "Manoj Ponnam",
      "Ramasani Rajitha",
      "R. Balamurugan"
    ],
    "abstract":"An effective evaluation of the multi-class classification model is very essential in evaluating the performances of the model in varied applications. Common accuracy percentages give insufficient information, especially in the case of imbalanced datasets and complex classification tasks. This paper enhances the evaluation process through the use of confusion matrix and derived metrics such as precision, recall, and F1-score for each class. Among the architectural modifications that significantly improved the model's stability and performance were greater model depth, varied attention heads, and improved regularization. Among other models, Bidirectional Encoder Representations from Transformers (BERT), Generative Pre-trained Transformer 2(GPT 2), and Large Language Model Meta AI(LLaMA) were evaluated along with a modified version across four different classes. Results indicate that BERT achieves an overall accuracy percentage of 98% with class-wise F1-scores of 0.97, 0.98, 0.97, and 0.97 for classes 0, 1, 2, and 3 respectively. GPT-2, on the other hand, has an overall accuracy percentage of 97%, along with class-wise F1-scores of 0.97, 0.98, 0.96, and 0.97 for classes 0, 1, 2, and 3 respectively. LLaMA yields 98% accuracy, with F1-scores of 0.98 for class 0, 0.99 for class 1, 0.97 for class 2, and 0.97 for class 3. The modified version of LLaMA also performs up to 98% accuracy which is fairly similar to the F1-scores of BERT.",
    "venue":"International Conference on Innovative Mechanisms for Industry Applications",
    "year":2025,
    "publication_date":"2025-09-03",
    "citation_count":0,
    "reference_count":18,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICIMIA67127.2025.11200914",
      "CorpusId":282256335
    }
  },
  {
    "paper_id":"2c7d903bb758918e720764786eaecabba461a4b2",
    "title":"Ensemble approach to rumor detection with BERT, GPT, and POS features",
    "authors":[
      "Barsha Pattanaik",
      "Sourav Mandal",
      "R. M. Tripathy",
      "A. Sekh"
    ],
    "abstract":"As vast amounts of rumor content are transmitted on social media, it is very challenging to detect them. This study explores an ensemble approach to rumor detection in social media messages, leveraging the strengths of advanced natural language processing (NLP) models. Specifically, we implemented three distinct models: (i) generative pre-trained transformer (GPT) combined with a bidirectional long short-term memory (BiLSTM) network; (ii) a model integrating part-of-speech (POS) tagging with bidirectional encoder representations from transformers (BERT) and BiLSTM, and (iii) a model that merges POS tagging with GPT and BiLSTM. We included additional features from the dataset in all these models. Each model captures different linguistic, syntactical, and contextual features within the text, contributing uniquely to the classification task. To enhance the robustness and accuracy of our predictions, we employed an ensemble method using hard voting. This technique aggregates the predictions from each model, determining the final classification based on the majority vote. Our experimental results demonstrate that the ensemble approach significantly outperforms individual models, achieving superior accuracy in identifying rumors. To determine the performance of our model, we used PHEME and Weibo datasets available publicly. We found our model gave 97.6% and 98.4% accuracy, respectively, on the datasets and has outperformed the state-of-the-art models.",
    "venue":"International Journal of Informatics and Communication Technology (IJ-ICT)",
    "year":2025,
    "publication_date":null,
    "citation_count":0,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.11591\/ijict.v14i1.pp276-286",
      "CorpusId":274984579
    }
  },
  {
    "paper_id":"548329022e9b902cbce79712eb7444067491b4d6",
    "title":"USING KAZAKH NER DATASETS FOR MULTICLASS CLASSIFICATION IN THE LEGAL DOMAIN: A COMPARATIVE STUDY OF BERT, GPT, AND LSTM MODELS",
    "authors":[
      "D. Oralbekova",
      "Orken J. Mamyrbayev",
      "A. Akhmediyarova",
      "Dinara Kassymova",
      "Z. Alibiyeva"
    ],
    "abstract":"This study presents an in-depth comparative analysis of the performance of three key approaches in natural language processing (NLP) — transformers, recurrent neural networks (RNNs), and traditional machine learning methods — in the task of multiclass text classification within the legal domain in the Kazakh language. A specialized dataset for named entity recognition (NER), adapted to legal topics, was utilized for the analysis. The primary focus is on the classification of legal texts. Standard evaluation metrics, such as accuracy, recall, precision, and the area under the curve (AUC), were employed to provide an objective assessment of the models' efficiency in text classification tasks. Special attention was given to the processing of the Kazakh language, which belongs to the category of under-researched languages in computational linguistics. This highlights the necessity of developing specialized algorithms and adapting existing methods to work effectively with this language and its legal terminology. The conducted research not only expands the understanding of the capabilities of existing models for text processing in low-resource languages but also emphasizes the importance of further efforts to automate legal services. This can facilitate the development of more accessible and scalable tools for legal assistance.",
    "venue":"International Conference on Software and Computer Applications",
    "year":2025,
    "publication_date":"2025-02-20",
    "citation_count":0,
    "reference_count":9,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icsca\/OralbekovaMAKZ25",
      "DOI":"10.1145\/3731806.3731855",
      "CorpusId":281527112
    }
  },
  {
    "paper_id":"ba900412ab47fd890e69bfa7e909d34ae476b870",
    "title":"Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet",
    "authors":[
      "M. O. Topal",
      "Anil Bas",
      "Imke van Heerden"
    ],
    "abstract":"Recent years have seen a proliferation of attention mechanisms and the rise of Transformers in Natural Language Generation (NLG). Previously, state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing gradient problems; as sentences grew larger, distance between positions remained linear, and sequential computation hindered parallelization since sentences were processed word by word. Transformers usher in a new era. In this paper, we explore three major Transformer-based models, namely GPT, BERT, and XLNet, that carry significant implications for the field. NLG is a burgeoning area that is now bolstered with rapid developments in attention mechanisms. From poetry generation to summarization, text generation derives benefit as Transformer-based language models achieve groundbreaking results.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-02-16",
    "citation_count":103,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2102.08036",
      "DBLP":"journals\/corr\/abs-2102-08036",
      "CorpusId":231933669
    }
  },
  {
    "paper_id":"50a3f0dd12114fb2ca90a5511a6325524c3f6013",
    "title":"Evaluating GPT and BERT models for protein–protein interaction identification in biomedical text",
    "authors":[
      "Hasin Rehana",
      "Nur Bengisu Çam",
      "Mert Basmaci",
      "Jie Zheng",
      "Christianah Jemiyo",
      "Yongqun He",
      "Arzucan Özgür",
      "J. Hur"
    ],
    "abstract":"Abstract Motivation Detecting protein–protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. As biomedical literature continues to grow rapidly, there is an increasing need for automated and accurate extraction of these interactions to facilitate scientific discovery. Pretrained language models, such as generative pretrained transformers and bidirectional encoder representations from transformers, have shown promising results in natural language processing tasks. Results We evaluated the performance of PPI identification using multiple transformer-based models across three manually curated gold-standard corpora: Learning Language in Logic with 164 interactions in 77 sentences, Human Protein Reference Database with 163 interactions in 145 sentences, and Interaction Extraction Performance Assessment with 335 interactions in 486 sentences. Models based on bidirectional encoder representations achieved the best overall performance, with BioBERT achieving the highest recall of 91.95% and F1 score of 86.84% on the Learning Language in Logic dataset. Despite not being explicitly trained for biomedical texts, GPT-4 showed commendable performance, comparable to the bidirectional encoder models. Specifically, GPT-4 achieved the highest precision of 88.37%, a recall of 85.14%, and an F1 score of 86.49% on the same dataset. These results suggest that GPT-4 can effectively detect protein interactions from text, offering valuable applications in mining biomedical literature. Availability and implementation The source code and datasets used in this study are available at https:\/\/github.com\/hurlab\/PPI-GPT-BERT.",
    "venue":"Bioinformatics Advances",
    "year":2024,
    "publication_date":"2024-09-11",
    "citation_count":18,
    "reference_count":50,
    "fields_of_study":[
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"11419952",
      "DOI":"10.1093\/bioadv\/vbae133",
      "CorpusId":272678278,
      "PubMed":"39319026"
    }
  },
  {
    "paper_id":"090ecca83ed4040dcca1737a8600703ba34fe084",
    "title":"An Iterative Systematic Analytical Review of Large Language Models for Medical Applications Using GPT-4, BERT Variants, and Vision Transformers",
    "authors":[
      "Wani H. Bisen",
      "Avinash J. Agrawal"
    ],
    "abstract":"Introduction: The increasing adoption of Large Language Models (LLMs) in healthcare necessitates a comprehensive review of their applications, limitations, and potential. Existing literature lacks a systematic assessment of LLM performance across diverse healthcare tasks and does not adequately address critical aspects such as model-specific optimizations, domain adaptability, and real-world deployment constraints. \nObjectives : This paper aims to fill the identified gaps by conducting an extensive and structured review of current research on LLM applications in medical reports, diagnostics, and decision-making. It seeks to classify and evaluate studies based on methods used, performance measures, key takeaways, strengths, and limitations. \nMethods : A PRISMA-based methodology was employed to systematically categorize studies according to their approaches and outcomes. The analysis focused on multiple LLMs, including GPT-3, GPT-4, BERT variants, Med-PaLM, and domain-specific adaptations such as BioGPT and COMCARE. For vision-language transformer-based auto-report generation, PEGASUS and ETB MII were examined. Additionally, the study explored KELLM for causal reasoning with knowledge graphs and OpenMedLM for equitable healthcare solutions. The selected models were evaluated based on key performance metrics such as accuracy, sensitivity, and explainability. \nResults : The findings indicate that specific LLMs show significant promise in enhancing healthcare applications. Models like Med-PaLM and BioGPT demonstrate improved diagnostic accuracy, while vision-language transformers such as PEGASUS enhance automated medical report generation. The integration of knowledge graphs in KELLM ensures greater interpretability and safety. Open-source models like OpenMedLM contribute to equitable access to AI-driven healthcare solutions. Overall, LLMs can reduce clinician workload, enhance diagnostic precision, and optimize healthcare workflows. \nConclusion : This study highlights the transformative potential of LLMs in medicine while also addressing challenges such as ethical considerations, energy efficiency, and scalability. By providing a systematic evaluation, this review paves the way for future advancements in AI-driven healthcare applications, fostering innovation and improved patient care.",
    "venue":"Communications on Applied Nonlinear Analysis",
    "year":2025,
    "publication_date":"2025-02-21",
    "citation_count":0,
    "reference_count":40,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.52783\/cana.v32.3965",
      "CorpusId":277527911
    }
  },
  {
    "paper_id":"1ad9a295ea841599383e5ae3e88381438d4a7db3",
    "title":"Evaluation of GPT and BERT-based models on identifying proteinprotein interactions in biomedical text",
    "authors":[
      "Hasin Rehana",
      "Nur Bengisu Çam",
      "Mert Basmaci",
      "Y. He",
      "Arzucan Özgür",
      "J. Hur"
    ],
    "abstract":"Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformers (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the performance of PPI identification of multiple GPT and BERT models using three manually curated gold-standard corpora: Learning Language in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference Database with 163 PPIs in 145 sentences, and Interaction Extraction Performance Assessment with 335 PPIs in 486 sentences. BERT-based models achieved the best overall performance, with BioBERT achieving the highest recall (91.95%) and F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%). Interestingly, despite not being explicitly trained for biomedical texts, GPT-4 achieved commendable performance, comparable to the top-performing BERT models. It achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of 86.49% on the LLL dataset. These results suggest that GPT models can effectively detect PPIs from text data, offering promising avenues for application in biomedical literature mining. Further research could explore how these models might be fine-tuned for even more specialized tasks within the biomedical domain.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-03-30",
    "citation_count":40,
    "reference_count":52,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2303.17728",
      "DBLP":"journals\/corr\/abs-2303-17728",
      "PubMedCentral":"11101131",
      "DOI":"10.48550\/arXiv.2303.17728",
      "CorpusId":257900846,
      "PubMed":"38764593"
    }
  },
  {
    "paper_id":"a398cf89781a11fb371028ba0fba9cc10642c8a0",
    "title":"Impact of Transformer-Based Models in NLP: An In-Depth Study on BERT and GPT",
    "authors":[
      "Mustafa Salıcı",
      "Üyesi Ercan Ölçer"
    ],
    "abstract":"This article examines in depth the effects of BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer) models in the field of natural language processing (NLP). NLP, as a sub-branch of computer science and artificial intelligence, has an important place in the processes of understanding, processing and producing human language. Transformer-based models have provided revolutionary advances in many NLP tasks, especially language modeling, sentiment analysis, machine translation and question answering. In this article, the technical structures of BERT and GPT models, their performance in low-resource languages such as Turkish and the difficulties encountered in these languages are discussed in detail. The BERT model stands out with its ability to evaluate the contextual meaning of the language from both directions using bidirectional context and offers high success rates especially in tasks such as text classification, named entity recognition (NER) and sentiment analysis. The GPT model, on the other hand, provides superiority in text production with its autoregressive structure and gives successful results in tasks such as creative writing and chatbots. However, the application of these models to morphologically rich and agglutinative languages such as Turkish poses certain challenges. These challenges arise due to the structural features of the language and the limitations of the datasets. The article also discusses the development of Turkish-specific models such as BERTurk, data augmentation techniques, and the success of modifications appropriate to the characteristics of the language in overcoming these challenges. In conclusion, the development of BERT and GPT models in the field of NLP has been a major step in terms of natural language processing and modeling. However, further research and development is required for these models to perform better in languages such as Turkish. The article predicts that future developments of these models will enable language technologies to reach wider audiences and be used in various applications.",
    "venue":"2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)",
    "year":2024,
    "publication_date":"2024-09-21",
    "citation_count":9,
    "reference_count":17,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/IDAP64064.2024.10710796",
      "CorpusId":273376348
    }
  },
  {
    "paper_id":"e22df6ef067a7663c9fcc46c4e7c2510e8eec8c3",
    "title":"Email subjects generation with large language models: GPT-3.5, PaLM 2, and BERT",
    "authors":[
      "Soumaya Loukili",
      "A. Fennan",
      "Lotfi Elaachak"
    ],
    "abstract":"In order to enhance marketing efforts and improve the performance of marketing campaigns, the effectiveness of language generation models needs to be evaluated. This study examines the performance of large language models (LLMs), namely GPT-3.5, PaLM 2, and bidirectional encoder representations from transformers (BERT), in generating email subjects for advertising campaigns. By comparing their results, the authors evaluate the efficacy of these models in enhancing marketing efforts. The objective is to explore how LLMs contribute to creating compelling email subject lines and improving opening rates and campaign performance, which gives us an insight into the impact of these models in digital marketing. In this paper, the authors first go over the different types of language models and the differences between them, before giving an overview of the most popular ones that will be used in the study, such as GPT-3.5, PaLM 2, and BERT. This study assesses the relevance, engagement, and uniqueness of GPT-3.5, PaLM 2, and BERT by training and fine-tuning them on marketing texts. The findings provide insights into the major positive impact of artificial intelligence (AI) on digital marketing, enabling informed decision-making for AI-driven email marketing strategies.",
    "venue":"International Journal of Electrical and Computer Engineering (IJECE)",
    "year":2024,
    "publication_date":"2024-08-01",
    "citation_count":6,
    "reference_count":41,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.11591\/ijece.v14i4.pp4655-4663",
      "CorpusId":270274304
    }
  },
  {
    "paper_id":"fadca1a14712b1f652521ce0d17cb3c029847d7e",
    "title":"BERT vs GPT for financial engineering",
    "authors":[
      "Edward Sharkey",
      "Philip C. Treleaven"
    ],
    "abstract":"The paper benchmarks several Transformer models [4], to show how these models can judge sentiment from a news event. This signal can then be used for downstream modelling and signal identification for commodity trading. We find that fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this task. Transformer models have revolutionized the field of natural language processing (NLP) in recent years, achieving state-of-the-art results on various tasks such as machine translation, text summarization, question answering, and natural language generation. Among the most prominent transformer models are Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), which differ in their architectures and objectives. A CopBERT model training data and process overview is provided. The CopBERT model outperforms similar domain specific BERT trained models such as FinBERT. The below confusion matrices show the performance on CopBERT&CopGPT respectively. We see a ~10 percent increase in f1_score when compare CopBERT vs GPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights the importance of considering alternatives to GPT models for financial engineering tasks, given risks of hallucinations, and challenges with interpretability. We unsurprisingly see the larger LLMs outperform the BERT models, with predictive power. In summary BERT is partially the new XGboost, what it lacks in predictive power it provides with higher levels of interpretability. Concluding that BERT models might not be the next XGboost [2], but represent an interesting alternative for financial engineering tasks, that require a blend of interpretability and accuracy.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-04-24",
    "citation_count":4,
    "reference_count":18,
    "fields_of_study":[
      "Computer Science",
      "Economics"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2405-12990",
      "ArXiv":"2405.12990",
      "DOI":"10.48550\/arXiv.2405.12990",
      "CorpusId":269982833
    }
  },
  {
    "paper_id":"72544215e867f97eb27607b5117c3ebeed6df61f",
    "title":"Identification of social network automated hate speech using GLTR with BERT and GPT-2 : A novel approach",
    "authors":[
      "Monika Chhikara",
      "S. Malik",
      "Vanita Jain"
    ],
    "abstract":"Automated hate speech on social media is a serious issue for online safety and wellbeing. A promising strategy to address the issue is the identification of artificial hate speech using machine learning techniques. The authors present research on the detection of automated hate speech on social networks with the use of GLTR (Giant Language model Test Room) along with BERT (Bidirectional Encoder Representations from Transformers) as well as GPT-2 (Generative Pretrained Transformer 2) on the four widely used benchmark datasets (ETHOS, Founta, Waseem, and Davidson). The performance of the proposed approach was evaluated by comparing F1 scores, precision, recall, and accuracy with other state-of-the-art machine algorithms. The results show that the proposed approach out performs other algorithms on all four datasets. The results indicate that the GLTR approach with BERT and GPT-2 models is a promising method for automated hate speech detection on social networks. ",
    "venue":"Journal of Information and Optimization Sciences",
    "year":2024,
    "publication_date":null,
    "citation_count":1,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.47974\/jios-1549",
      "CorpusId":268568683
    }
  },
  {
    "paper_id":"289d403813b86e3ec19e9b253c2fbe623647363c",
    "title":"Comparative Evaluation of GPT, BERT, and XLNet: Insights into Their Performance and Applicability in NLP Tasks",
    "authors":[
      "Chuxi Zhou"
    ],
    "abstract":"Natural Language Processing (NLP) is a pivotal area in artificial intelligence, aiming to make computers capable of understanding and generating human language. This study evaluates and compares three prominent NLP models—the Generative Pre-trained Transformer (GPT) model, Bidirectional Encoder Representations from Transformers (BERT) model, and Generalized Autoregressive Pretraining for Language Understanding (XLNet)—to determine their strengths, limitations, and suitability for various tasks. The research involves a comprehensive analysis of these models, utilizing well-established datasets such as the Stanford Question Answering Dataset (SQuAD), General Language Understanding Evaluation (GLUE), Reading Comprehension from Examinations (RACE), and the Situations with Adversarial Generations (SWAG). The study explores each model's architecture, pre-training, and fine-tuning processes: GPT’s unidirectional approach is assessed for its language generation and handling of long-range dependencies; Bidirectional encoding is examined for its effectiveness in context understanding, and XLNet permutation-based training is analyzed for its robust contextual comprehension. The experimental results reveal that GPT excels in generative tasks but is constrained by its unidirectional nature. BERT achieves superior accuracy in comprehension tasks but is computationally demanding and susceptible to pre-training bias. XLNet outperforms both GPT and BERT in accuracy and contextual understanding, though at the cost of increased complexity. The results offer a significant understanding of the effectiveness and applicability of these models, suggesting future research directions such as hybrid models and improvements in efficiency.",
    "venue":"Transactions on Computer Science and Intelligent Systems Research",
    "year":2024,
    "publication_date":"2024-11-25",
    "citation_count":0,
    "reference_count":14,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.62051\/h08exg91",
      "CorpusId":275318742
    }
  },
  {
    "paper_id":"dee05e9ba2a766c954dc55a53dae53c20a704433",
    "title":"A Hybrid Approach To Solving The Sarcasm Detection Problem: BERT vs GPT-BERT",
    "authors":[
      "Ayush Yajaman"
    ],
    "abstract":": With the boom of Natural Language Processing (NLP) machines like ChatGPT in recent years, the demand for accurate sarcasm detection has surged. However, to the best of our knowledge, there is not an existing approach that addresses the three main challenges of sarcasm detection simultaneously: understanding context, implicit references, and common sense. Hence, we propose a novel Deep Learning (DL) based hybrid approach to tackle these challenges. We utilize Generative Pre-Trained Transformer-3.5 Turbo (GPT-3.5 Turbo) for appending additional information in each dataset example if any historical, cultural, or political reference or renowned figure is identified. This modified dataset is used to train Bidirectional Encoder Representations from Transformers (BERT). We will be using a large, well-labelled News-headlines dataset for training and testing. A current study claims to have obtained 91.47% accuracy on the same dataset using BERT. This will be used as the baseline to compare with our hybrid approach (GPT-BERT) and a BERT-based approach with modified hyperparameters. The GPT-BERT approach resulted in greater precision and lower validation loss of 0.94% each, compared to the BERT-based approach. However, it underperformed in terms of accuracy (by 0.6%), recall (by 2.5%), and F1 score (by 0.78%). Nevertheless, both approaches outperformed the baseline.",
    "venue":"International Journal of High School Research",
    "year":2024,
    "publication_date":"2024-09-30",
    "citation_count":0,
    "reference_count":14,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.36838\/v6i9.4",
      "CorpusId":275985848
    }
  },
  {
    "paper_id":"4add28cabacf0824bfe8412faa11726c65c552fd",
    "title":"Evaluating Bert and GPT-2 Models for Personalised Linkedin Post Recommendation",
    "authors":[
      "Prerna Singh",
      "Bhawna Jain",
      "Kirti Sinha"
    ],
    "abstract":"Social networking platforms have become essential tools for individuals and organisations to connect, communicate, and collaborate in today’s digital age. LinkedIn is a professional social networking platform facilitating career development and job searching. LinkedIn’s traditional post recommendation system limits users to seeing only those posts that have been engaged upon by their following or first-degree connections, limiting the user’s perspective. The proposed system leverages the content posted by the user to provide personalised content delivery on LinkedIn, potentially enhancing user engagement and satisfaction. This research study proposes and examines the performance of three content-based recommender models developed with Machine Learning (ML), Generative Pre-Trained Transformer (GPT-2), and Bidirectional Encoder Representations from Transformers (BERT). In terms of capturing the similarity between user-generated and recommended posts, BERT outperformed the other models, achieving the highest similarity score of 97.13%, compared to GPT-2 (96.27%) and basic ML (95.69%).",
    "venue":"International Conference on Computing Communication and Networking Technologies",
    "year":2023,
    "publication_date":"2023-07-06",
    "citation_count":1,
    "reference_count":22,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icccnt\/SinghJS23a",
      "DOI":"10.1109\/ICCCNT56998.2023.10307957",
      "CorpusId":265407610
    }
  },
  {
    "paper_id":"4ce2ceb4ee975b032578e8816cb8f50a9984c76e",
    "title":"Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition",
    "authors":[
      "Xianrui Zheng",
      "Chao Zhang",
      "P. Woodland"
    ],
    "abstract":"Language models (LMs) pre-trained on massive amounts of text, in particular bidirectional encoder representations from Transformers (BERT), generative pre-training (GPT), and GPT-2, have become a key technology for many natural language processing tasks. In this paper, we present results using fine-tuned GPT, GPT-2, and their combination for automatic speech recognition (ASR). Unlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct product of the output probabilities is no longer a valid language prior probability. A conversion method is proposed to compute the correct language prior probability based on bidirectional LM outputs in a mathematically exact way. Experimental results on the widely used AMI and Switchboard ASR tasks showed that the combination of the fine-tuned GPT and GPT-2 outperformed the combination of three neural LMs with different architectures trained from scratch on the in-domain text by up to a 12% relative word error rate reduction (WERR). Furthermore, on the AMI corpus, the proposed conversion for language prior probabilities enables BERT to obtain an extra 3% relative WERR, and the combination of BERT, GPT and GPT-2 results in further improvements.",
    "venue":"Automatic Speech Recognition & Understanding",
    "year":2021,
    "publication_date":"2021-07-29",
    "citation_count":61,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "ArXiv":"2108.07789",
      "DBLP":"journals\/corr\/abs-2108-07789",
      "DOI":"10.1109\/ASRU51503.2021.9688232",
      "CorpusId":237142586
    }
  },
  {
    "paper_id":"e8a792ef9f46fc917ca7d4632d3654ea793a7bb6",
    "title":"Scalable and Advanced Framework for Hate Speech Detection on Social Media Using BERT and GPT-2 through Encoder and Decoder Architectures",
    "authors":[
      "Usman",
      "Nabeela Hasan",
      "Syed Mohammad Khurshid Quadri"
    ],
    "abstract":": Hate speech is a major problem on social media platforms. Every day, numerous instances of hateful behavior based on race, ethnicity, religion, or gender are witnessed on social media. Most of the leading social media platforms like Instagram, Facebook, Twitter Reddit, etc., have strong community guidelines that condemn and restrict the exchange of hateful language\/content in any form. Despite the guidelines, some of these instances go unnoticed due to the suitability of the language and expression. This encourages the need for strong automated hate speech detection techniques that can flag such content and ensure a safer environment for users belonging to all domains of life. There is a concept of transformers model it is based on two encoders and decoder blocks. The model which has only an encoder block is called Bidirectional Encoder Representations from Transformers (BERT) and the model which contains only a decoder block is called Generative Pre-trained Transformer (GPT). In this study, we propose a method that uses a pre-trained BERT model for hate speech detection on Twitter data. The dataset contains tweets belonging to three different classes i.e., hate speech (0), offensive language (1), and neither of these (2). We evaluated our proposed model on this dataset: Without data augmentation and with data augmentation using Generative Pre-trained Transformer-2 (GPT-2). It shows that data augmentation with GPT-2 enhances the performance of the BERT model by achieving 81% accuracy in comparison to un-augmented data. Despite strong community guidelines, subtle forms of hate speech on social media often go undetected, highlighting the need for robust detection methods. The suggested method uses a pre-trained BERT algorithm to categorize tweets as hate speech, inflammatory language, or neutral content. Data augmentation with GPT-2 considerably improves the BERT model's performance, obtaining an 81% accuracy rate.",
    "venue":"Journal of Computer Science",
    "year":2025,
    "publication_date":"2025-03-01",
    "citation_count":0,
    "reference_count":34,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.3844\/jcssp.2025.584.594",
      "CorpusId":276798729
    }
  },
  {
    "paper_id":"129d9e91b29ea26e2a0e300e23d3397d1a2d1ac1",
    "title":"Financial text analysis and credit risk assessment using a GPT-4 and improved BERT fusion model",
    "authors":[
      "Huirong Tan",
      "Yanruixue Xie"
    ],
    "abstract":"This study aims to improve the identification of potential credit risks in unstructured financial texts. It addresses the core problem of financial text analysis and credit risk assessment by proposing a hybrid model that combines the generative semantic understanding of Generative Pre-trained Transformer-4 (GPT-4) with the enhanced feature extraction of Bidirectional Encoder Representations from Transformers (BERT). To overcome the limitations of traditional methods—such as weak contextual reasoning in long texts, insufficient recognition of industry-specific terminology, and implicit credit risk expressions—the model incorporates a financial dictionary enhancement module and a named entity recognition (NER) component. GPT-4 is leveraged for prompt-based generation to extract latent risk information from complex texts, including annual reports. A dual-model semantic fusion mechanism with attention weighting constructs a multi-level risk assessment system that integrates contextual understanding, industry adaptability, and interpretability. Experiments on multiple publicly available financial datasets and real-world annual reports demonstrate the model’s effectiveness. Results show that the proposed approach outperforms representative baseline models in accuracy, adaptability, and interpretability. This work carries both theoretical and practical significance for research at the intersection of financial technology and natural language processing.",
    "venue":"PLoS ONE",
    "year":2025,
    "publication_date":"2025-11-18",
    "citation_count":0,
    "reference_count":26,
    "fields_of_study":[

    ],
    "external_ids":{
      "PubMedCentral":"12626313",
      "DOI":"10.1371\/journal.pone.0336217",
      "CorpusId":283084613
    }
  },
  {
    "paper_id":"5a09edeb26f9f116f2c0503cd020f38fb943f79b",
    "title":"BERT Busters: Outlier Dimensions that Disrupt Transformers",
    "authors":[
      "Olga Kovaleva",
      "Saurabh Kulshreshtha",
      "Anna Rogers",
      "Anna Rumshisky"
    ],
    "abstract":"Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.",
    "venue":"Findings",
    "year":2021,
    "publication_date":"2021-05-14",
    "citation_count":106,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2105.06990",
      "DBLP":"conf\/acl\/KovalevaKRR21",
      "ACL":"2021.findings-acl.300",
      "DOI":"10.18653\/v1\/2021.findings-acl.300",
      "CorpusId":235313996
    }
  },
  {
    "paper_id":"5beadbead14fa7de0fdc7db1a16a5faba6261925",
    "title":"Analisis Sentimen Perbandingan Lange Language Model Study kasus: Gpt, Gemini dan Llama Menggunakan BERT",
    "authors":[
      "Linda Safitri"
    ],
    "abstract":"Abstrak : Perkembangan Large Language Models (LLMs) seperti GPT, Gemini, dan LLaMA telah mendorong penelitian lebih lanjut mengenai efektivitas dan sentimen yang dihasilkan oleh model-model ini dalam berbagai aplikasi. Studi ini bertujuan untuk melakukan analisis sentimen perbandingan terhadap ketiga model tersebut dengan pendekatan berbasis BERT (Bidirectional Encoder Representations from Transformers). Metode yang digunakan mencakup pengumpulan data dari keluaran masing-masing model dalam berbagai konteks, analisis sentimen menggunakan BERT, serta evaluasi perbedaan dalam distribusi sentimen positif, negatif, dan netral. Hasil penelitian ini menunjukan model GPT menunjukkan sentimen positif tertinggi (49,54%) dibandingkan dengan Gemini (38,76%) dan Llama (35,98%). Sementara itu, model Llama memiliki sentimen negatif tertinggi (43,47%), diikuti oleh Gemini (41,68%) dan GPT (35,21%). Untuk sentimen netral, Llama memiliki persentase tertinggi (20,55%), sedangkan Gemini dan GPT memiliki persentase yang lebih rendah, masing-masing 13,56% dan 15,25%.Secara keseluruhan, GPT cenderung menghasilkan lebih banyak sentimen positif dibandingkan dengan model lainnya, sedangkan Llama dan Gemini lebih banyak menghasilkan sentimen negatif.Secara umum, dapat disimpulkan bahwa GPT menerima persepsi publik yang lebih positif dibandingkan Gemini dan LLaMA dalam konteks yang dianalisis dalam studi ini. Hal ini mengindikasikan bahwa pengguna atau pihak yang memberikan sentimen cenderung memiliki pengalaman atau pandangan yang lebih baik terhadap GPT. Hasil penelitian ini diharapkan dapat memberikan wawasan mengenai keunggulan dan kelemahan masing-masing LLM dalam memahami dan menghasilkan teks yang mencerminkan sentimen secara akurat. Temuan ini juga dapat membantu dalam pemilihan model yang lebih sesuai untuk aplikasi spesifik yang memerlukan analisis sentimen tingkat lanjut.   Kata Kunci : Analisis Sentimen, Large Language Models, GPT, Gemini, LLaMA, BERT",
    "venue":"Smart Comp Jurnalnya Orang Pintar Komputer",
    "year":2025,
    "publication_date":"2025-07-20",
    "citation_count":0,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.30591\/smartcomp.v14i3.8373",
      "CorpusId":280794910
    }
  },
  {
    "paper_id":"560ee4396ef861a59950637ac1d9f897e7a12f6e",
    "title":"Transforming Sentimental Analysis: Applying Deep Learning with BERT and GPT for X Data Insights",
    "authors":[
      "Manthan S Varma",
      "Soma Ghosh"
    ],
    "abstract":"Social media platforms like X have become key channels for expressing opinions and sentiments, creating vast amounts of textual data. Analyzing this data to understand public sentiment is crucial for various applications, including market research, political analysis, and customer feedback. Traditional sentiment analysis methods often struggle with the nuances of language, especially in informal settings like X. However, recent advancements in deep learning, particularly with transformer models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have significantly improved the accuracy of sentiment analysis. These models use attention mechanisms to capture the context of words within a sentence, allowing for a more sophisticated understanding of sentiment. Unlike existing studies that predominantly focus on either BERT or GPT separately, this research provides a comparative evaluation of their effectiveness for sentiment analysis on X, highlighting their strengths and limitations in handling informal and context-dependent text. BERT, with its bidirectional approach, and GPT, known for its text generation capabilities, have achieved state-of-the-art results in numerous NLP tasks. This study leverages these models to analyze the Sentiment140 dataset, aiming to enhance sentiment analysis on X by utilizing deep learning techniques and libraries like Hugging Face Transformers, PyTorch, and TensorFlow.",
    "venue":"2025 International Conference on Visual Analytics and Data Visualization (ICVADV)",
    "year":2025,
    "publication_date":"2025-03-04",
    "citation_count":0,
    "reference_count":21,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICVADV63329.2025.10961289",
      "CorpusId":278001032
    }
  },
  {
    "paper_id":"c53eeb352cee723c9cd6418aac499572a0bf37cd",
    "title":"Multilingual and Multi-Class Sentiment Classification Using Machine Learning, BERT, and GPT-4o-mini",
    "authors":[
      "Tuğba Tosun Pataci",
      "Furkan Göz"
    ],
    "abstract":"In this study, we investigate multilingual and multiclass sentiment classification by analyzing datasets in Turkish, English, and Italian. The proposed approach consists of three main stages: sentence representation extraction, classification, and performance evaluation. First, sentence representations were extracted from these datasets using the distiluse-base-multilingual-cased-v1, sentence-transformers\/LaBSE, and Alibaba-NLP\/gte-multilingual-base models. These representations were then used as input for Logistic Regression (LR), Support Vector Machines (SVM), Random Forest (RF), and Naive Bayes (NB). Additionally, fine-tuned BERT-base-multilingual-cased and GPT-4o-mini were directly employed as end-to-end models. The classification performance of the models was evaluated using accuracy, F1-score, precision, and recall. Additionally, a confusion matrix analysis was conducted for each dataset to examine classification performance in detail. The experimental results show the influence of embedding models and learning algorithms on multilingual multi-class sentiment classification.",
    "venue":"2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)",
    "year":2025,
    "publication_date":"2025-05-23",
    "citation_count":0,
    "reference_count":17,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICHORA65333.2025.11017018",
      "CorpusId":279126461
    }
  },
  {
    "paper_id":"3599a236f285af48782fc30b1341d13ec7320735",
    "title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
    "authors":[
      "Ce Zhou",
      "Qian Li",
      "Chen Li",
      "Jun Yu",
      "Yixin Liu",
      "Guangjing Wang",
      "Kaichao Zhang",
      "Cheng Ji",
      "Qi Yan",
      "Lifang He",
      "Hao Peng",
      "Jianxin Li",
      "Jia Wu",
      "Ziwei Liu",
      "Pengtao Xie",
      "Caiming Xiong",
      "Jian Pei",
      "Philip S. Yu",
      "Lichao Sun Michigan State University",
      "B. University",
      "Lehigh University",
      "Macquarie University",
      "Nanyang Technological University",
      "University of California at San Diego",
      "Duke University",
      "U. Chicago",
      "Salesforce Research"
    ],
    "abstract":"Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-02-18",
    "citation_count":630,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2302.09419",
      "DBLP":"journals\/corr\/abs-2302-09419",
      "DOI":"10.48550\/arXiv.2302.09419",
      "CorpusId":257039063
    }
  },
  {
    "paper_id":"465aa8a632016054ddf12568fc5465fd85730082",
    "title":"CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines",
    "authors":[
      "Chao Pang",
      "Xinzhuo Jiang",
      "N. Pavinkurve",
      "Krishna S. Kalluri",
      "Elise Minto",
      "Jason Patterson",
      "Linying Zhang",
      "G. Hripcsak",
      "Noémie Elhadad",
      "Karthik Natarajan"
    ],
    "abstract":"Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-02-06",
    "citation_count":17,
    "reference_count":22,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2402.04400",
      "DBLP":"journals\/corr\/abs-2402-04400",
      "DOI":"10.48550\/arXiv.2402.04400",
      "CorpusId":267523270
    }
  },
  {
    "paper_id":"13e8def66f890c7bb65d6f13a6545b4a689bddc2",
    "title":"Natural language processing with transformers: a review",
    "authors":[
      "Georgiana Țucudean",
      "Marian Bucos",
      "Bogdan Drăgulescu",
      "C. Căleanu"
    ],
    "abstract":"Natural language processing (NLP) tasks can be addressed with several deep learning architectures, and many different approaches have proven to be efficient. This study aims to briefly summarize the use cases for NLP tasks along with the main architectures. This research presents transformer-based solutions for NLP tasks such as Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-Training (GPT) architectures. To achieve that, we conducted a step-by-step process in the review strategy: identify the recent studies that include Transformers, apply filters to extract the most consistent studies, identify and define inclusion and exclusion criteria, assess the strategy proposed in each study, and finally discuss the methods and architectures presented in the resulting articles. These steps facilitated the systematic summarization and comparative analysis of NLP applications based on Transformer architectures. The primary focus is the current state of the NLP domain, particularly regarding its applications, language models, and data set types. The results provide insights into the challenges encountered in this research domain.",
    "venue":"PeerJ Computer Science",
    "year":2024,
    "publication_date":"2024-08-07",
    "citation_count":19,
    "reference_count":48,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/peerj-cs\/TucudeanBDC24",
      "PubMedCentral":"11322986",
      "DOI":"10.7717\/peerj-cs.2222",
      "CorpusId":271781965,
      "PubMed":"39145251"
    }
  },
  {
    "paper_id":"fb75fec3d582756e2699f2ad2121eb5fc6ba97ef",
    "title":"Unleashing the transformers: NLP models detect AI writing in education",
    "authors":[
      "J. Campino"
    ],
    "abstract":"Artificial Intelligence (AI) has witnessed widespread application across diverse domains, with education being a prominent focus for enhancing learning outcomes and tailoring educational approaches. Transformer models, exemplified by BERT, have demonstrated remarkable efficacy in Natural Language Processing (NLP) tasks. This research scrutinizes the current landscape of AI in education, emphasizing the utilization of transformer models. Specifically, the research delves into the influence of AI tools facilitating text generation through input prompts, with a notable instance being the GPT-4 model developed by OpenAI. The study employs pre-trained transformer models to discern whether a given text originates from AI or human sources. Notably, BERT emerges as the most effective model, fine-tuned using a dataset comprising abstracts authored by humans and those generated by AI. The outcomes reveal a heightened accuracy in distinguishing AI-generated text. These findings bear significance for the educational realm, suggesting that while endorsing the use of such tools for learning, vigilance is warranted to identify potential misuse or instances where students should independently develop their reasoning skills. Nevertheless, ethical considerations must be paramount when employing such methodologies. We have highlighted vulnerabilities concerning the potential bias of AI models towards non-native English speakers, stemming from possible deficiencies in vocabulary and grammatical structure. Additionally, users must ensure that there is no complete reliance on these systems to ascertain students' performance. Further research is imperative to unleash the full potential of AI in education and address ethical considerations tied to its application.",
    "venue":"Journal of Computers in Education",
    "year":2024,
    "publication_date":"2024-06-13",
    "citation_count":16,
    "reference_count":37,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1007\/s40692-024-00325-y",
      "CorpusId":270500649
    }
  },
  {
    "paper_id":"a90275686e6f3542067530ef44c96c1bbd350790",
    "title":"AI-Driven Text Generation: A Novel GPT-Based Approach for Automated Content Creation",
    "authors":[
      "Professor P.Kumar",
      "S.Manikandan",
      "R.Kishore"
    ],
    "abstract":"By combining the strengths of BERT (Bidirectional Encoding Representations from Transformers) and GPT (Generative Pre-trained Transformer), this study presents a novel method for automated text synthesis. We combine BERT's bidirectional contextual awareness to improve the coherence & relevance of generated text, while utilizing the pre-trained abilities of GPT for innovative and context-aware content generation. In order to provide a more complex and contextually accurate output, our model uses a two-stage architecture, where GPT starts the content production process and BERT repeatedly refines it. We show through extensive experimentation that our approach performs better than others in a variety of text creation tasks, such as question-answering, creative writing, and summarizing. This hybrid GPT-BERT approach represents a major breakthrough in automated text creation techniques, demonstrating not just exceptional fluency & coherence but also a remarkable capacity to adapt to a variety of linguistic circumstances. The results highlight the possibility of integrating transformer-based models to produce language creation that is more complex and contextually sensitive.",
    "venue":"2024 2nd International Conference on Networking and Communications (ICNWC)",
    "year":2024,
    "publication_date":"2024-04-02",
    "citation_count":12,
    "reference_count":12,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICNWC60771.2024.10537562",
      "CorpusId":270096401
    }
  },
  {
    "paper_id":"dcd8dff20f4f490acd5f94001d34c774167f053e",
    "title":"Jump to Conclusions: Short-Cutting Transformers with Linear Transformations",
    "authors":[
      "Alexander Yom Din",
      "Taelin Karidi",
      "Leshem Choshen",
      "Mor Geva"
    ],
    "abstract":"Transformer-based language models create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, using linear transformations. This approximation far exceeds the prevailing practice of inspecting hidden representations from all layers, in the space of the final layer. Moreover, in the context of language modeling, our method produces more accurate predictions from hidden layers, across various model scales, architectures, and data distributions. This allows “peeking” into intermediate representations, showing that GPT-2 and BERT often predict the final output already in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for example, at retention of 95% accuracy, our approach saves additional 7.9% layers for GPT-2 and 5.4% layers for BERT. Last, we extend our method to linearly approximate sub-modules, finding that attention is most tolerant to this change. Our code and learned mappings are publicly available at https:\/\/github.com\/sashayd\/mat.",
    "venue":"International Conference on Language Resources and Evaluation",
    "year":2023,
    "publication_date":"2023-03-16",
    "citation_count":76,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2303-09435",
      "ACL":"2024.lrec-main.840",
      "ArXiv":"2303.09435",
      "DOI":"10.48550\/arXiv.2303.09435",
      "CorpusId":257557722
    }
  },
  {
    "paper_id":"57c2fbcf8a32fffacf79bdf9c1df12b8cd26980e",
    "title":"Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models",
    "authors":[
      "Lochan Basyal",
      "Mihir Sanghvi"
    ],
    "abstract":"Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets. The assessment of these models' effectiveness contributes valuable insights to researchers and practitioners within the NLP domain. This work serves as a resource for those interested in harnessing the potential of LLMs for text summarization and lays the foundation for the development of advanced Generative AI applications aimed at addressing a wide spectrum of business challenges.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-10-16",
    "citation_count":49,
    "reference_count":11,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2310.10449",
      "DBLP":"journals\/corr\/abs-2310-10449",
      "DOI":"10.48550\/arXiv.2310.10449",
      "CorpusId":264146201
    }
  },
  {
    "paper_id":"6c817791a48705b7cf5eeae88b686f3c15712bb2",
    "title":"A Dual Pipeline AI Framework for Real Time Detection of Phishing and Financial Fraud in Fraud GPT",
    "authors":[
      "Y. Ajitha",
      "P. Sudha",
      "G. Gowthami",
      "C. Shanthini",
      "Mohammad Ayaz Ahmad"
    ],
    "abstract":"GenAI can create original writing, music, movies, and images and collaborate with humans and other AI models. It is one of our most revolutionary creations. Firms, academics, developers, and consumers have profited from increased productivity, innovation, and efficiency. Generative AI has challenges like any disruptive technology. It increases creativity and efficiency but creates ethical, disinformation, data privacy, and prejudice concerns. Recently developed AI tool fraudGPT became popular in the dark market where hackers may automate fraud emails, SMS, manufacture fake text messages, mimic a well-known organisation, and make hacking easy. This traps many innocent people in hackers' webs. AI should be used to detect fraudGPT-related phishing, smishing, social engineering, and financial crimes, according to this study. Python, transformer-based NLP model, anomaly detection model, binary classification, BERT, PyTorch, transformers, and sci-kit-learn are used. The suggested method detects phishing emails with 92% accuracy and transaction fraud with 94% accuracy in trials. To test the model's resilience, precision (91%), recall (90%), and F1-score (90.5%) were used for phishing identification and precision (93%), recall (92%), and F1-score (92.5%) for transaction fraud detection. Real-time monitoring and AI-based detection proactively take on AI-driven cyber risks. This work improves AI-powered cybersecurity and informs financial institutions and policymakers. AI researchers, cybersecurity experts, and regulatory bodies must collaborate to combat rogue AI technologies like FraudGPT.",
    "venue":"FMDB Transactions on Sustainable Technoprise Letters",
    "year":2024,
    "publication_date":"2024-12-07",
    "citation_count":0,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.69888\/ftstpl.2024.000335",
      "CorpusId":279911441
    }
  },
  {
    "paper_id":"56a14d082a792f0dcb376735bf35a91a0419f3a2",
    "title":"Comparative Analysis of Graph Neural Networks and Transformers for Robust Fake News Detection: A Verification and Reimplementation Study",
    "authors":[
      "Soveatin Kuntur",
      "Maciej Krzywda",
      "Anna Wróblewska",
      "M. Paprzycki",
      "M. Ganzha"
    ],
    "abstract":"This study compares Transformer-based models and Graph Neural Networks (GNNs) for fake news detection across three datasets: FakeNewsNet, ISOT, and WELFake. Transformer models (BERT, RoBERTa, GPT-2) demonstrated superior performance, achieving mean accuracies above 85% on FakeNewsNet and exceeding 98% on ISOT and WELFake. Specifically, RoBERTa achieved 86.16% accuracy on FakeNewsNet and 99.99% on ISOT, while GPT-2 reached 99.72% on WELFake. In contrast, GNNs (GCN, GraphSAGE, GIN, GAT) exhibited lower performance. GCN achieved 71% accuracy on FakeNewsNet but dropped to 53.30% on ISOT and 50.28% on WELFake, with F1 scores reflecting similar trends. Other GNNs, like GraphSAGE, showed even lower results, particularly on ISOT and WELFake, where performance hovered around 50%. Our findings indicate that while Transformers provide exceptional accuracy and reliability, GNNs offer potential efficiency benefits for resource-constrained scenarios despite their lower predictive performance. This study informs model selection for fake news detection tasks and encourages the exploration of hybrid approaches to balance accuracy and computational efficiency.",
    "venue":"Electronics",
    "year":2024,
    "publication_date":"2024-12-04",
    "citation_count":8,
    "reference_count":29,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.3390\/electronics13234784",
      "CorpusId":274644903
    }
  },
  {
    "paper_id":"e0995bad59c8638ea8c319bb7220c0f0b1ed5dca",
    "title":"DeepNet: Scaling Transformers to 1,000 Layers",
    "authors":[
      "Hongyu Wang",
      "Shuming Ma",
      "Li Dong",
      "Shaohan Huang",
      "Dongdong Zhang",
      "Furu Wei"
    ],
    "abstract":"In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Extensive experiments demonstrate that DeepNet has superior performance across various benchmarks, including machine translation, language modeling (i.e., BERT, GPT) and vision pre-training (i.e., BEiT). Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.",
    "venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year":2022,
    "publication_date":"2022-03-01",
    "citation_count":188,
    "reference_count":74,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2203.00555",
      "DBLP":"journals\/pami\/WangMDHZW24",
      "DOI":"10.1109\/TPAMI.2024.3386927",
      "CorpusId":247187905
    }
  },
  {
    "paper_id":"58f3f5d875b31dc3ae7744c0368349512bdb08e3",
    "title":"GPT-4 Performance for Neurologic Localization",
    "authors":[
      "Jung-Hyun Lee",
      "Eunhee Choi",
      "Robert A. McDougal",
      "William W. Lytton"
    ],
    "abstract":"Background and Objectives In health care, large language models such as Generative Pretrained Transformers (GPTs), trained on extensive text datasets, have potential applications in reducing health care disparities across regions and populations. Previous software developed for lesion localization has been limited in scope. This study aims to evaluate the capability of GPT-4 for lesion localization based on clinical presentation. Methods GPT-4 was prompted using history and neurologic physical examination (H&P) from published cases of acute stroke followed by questions for clinical reasoning with answering for “single or multiple lesions,” “side,” and “brain region” using Zero-Shot Chain-of-Thought and Text Classification prompting. GPT-4 output on 3 separate trials for each of 46 cases was compared with imaging-based localization. Results GPT-4 successfully processed raw text from H&P to generate accurate neuroanatomical localization and detailed clinical reasoning. Performance metrics across trial-based analysis for specificity, sensitivity, precision, and F1-score were 0.87, 0.74, 0.75, and 0.74, respectively, for side; 0.94, 0.85, 0.84, and 0.85, respectively, for brain region. Class labels within the brain region were similarly high for all regions except the cerebellum and were also similar when considering all 3 trials to examine metrics by case. Errors were due to extrinsic causes—inadequate information in the published cases, and intrinsic causes—failures of logic or inadequate knowledge base. Discussion This study reveals capabilities of GPT-4 in the localization of acute stroke lesions, showing a potential future role as a clinical tool in neurology.",
    "venue":"Neurology Clinical Practice",
    "year":2024,
    "publication_date":"2024-03-27",
    "citation_count":6,
    "reference_count":32,
    "fields_of_study":[
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"11003355",
      "DOI":"10.1212\/CPJ.0000000000200293",
      "CorpusId":268755780,
      "PubMed":"38596779"
    }
  },
  {
    "paper_id":"fe81e84593cc8556ac70f1cae16715e71f6e3573",
    "title":"Transforming Worlds: The Intersection of Translation Technology and Transformers in the Digital Age",
    "authors":[
      "Zaki Muhammad Zayyanu"
    ],
    "abstract":"This study provides a comparative analysis of the transformers’ role in translation technology with focus on the advancements of machine translation in digital age using Natural Language Processing (NLP). The study traces the technological advancements from rule-based to statistical methods of machine translation. The advancement is achieved through the integration of transformer models such as BERT, GPT and T5 in improving the efficiency of machine translation such as Neural Machine Translation (NMT). The BERT, GPT and T5 transformer models are characterised by their parallel processing and self-attention mechanisms, have significantly enhanced the accuracy and efficiency of translation, thus contributing to improved global communication. The study applied comparative and interpretative approach and theory of meaning. The study’s population focused on the translation technology and transformers. The study establishes the intersection of the two systems based on the facts and results of the systems from language engineers and translation experts. The discussion revolves around the challenges inherently present in transformer-based systems, including concerns over data efficiency, the handling of rare words, context sensitivity, bias, fairness, and the overarching societal impact of such technologies. The research highlights the development of innovative tools including wearable translation devices, smartphone applications, and emotion recognition systems that aid in surpassing language barriers and fostering international collaboration and understanding. It delves into the societal ramifications of these technologies, advocating for preservation of cultural nuances and the promotion of intercultural dialogue while highlighting ethical considerations such as privacy, security, and misinformation, and their role in shaping the deployment of translation technology. The study concludes that there is a synergy between transformers and translation technology in the digital age. The study traced an evolution from rule-based machine translation to the sophisticated AI powered translation that are revolutionized using transformer models. Through analysis and comparison, it is clear that transformer models beside enhancing accuracy and efficiency of translation, can also process and interpret natural language.",
    "venue":"Computer Science &amp; Engineering: An International Journal",
    "year":2024,
    "publication_date":"2024-06-28",
    "citation_count":2,
    "reference_count":25,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.5121\/cseij.2024.14301",
      "CorpusId":270739748
    }
  },
  {
    "paper_id":"a69169bba617c0525e31bc4e7c620011087daac7",
    "title":"3D Parallelism for Transformers via Integer Programming",
    "authors":[
      "Hao Zheng",
      "Peng Liang",
      "Yu Tang",
      "Yanqi Shi",
      "Linbo Qiao",
      "Dongsheng Li"
    ],
    "abstract":"Transformer models, such as BERT, GPT, and ViT, have been applied to a wide range of areas in recent years, due to their efficacy. In order to improve the training efficiency of Transformer models, different distributed training approaches have been proposed, like Megatron-LM [8]. However, when multi-dimensional parallelism strategies are considered, due to the complexity, existing works can not harmonize the different strategies well enough to obtain a globally optimal solution. In this paper, we propose a parallelism strategy searching algorithm PTIP, which generates operator-level parallelism strategies consisting of three schemes: data parallelism, tensor parallelism, and pipeline parallelism. PTIP abstracts these three parallelism schemes simultaneously into an auxiliary graph, reformulates the searching problem into a mixed-integer programming (MIP) problem, and uses a MIP solver to obtain a high-quality multi-dimensional strategy. Experiments conducted on Transformers demonstrate that PTIP obtains 13.9% − 24.7% performance improvement compared to Megatron-LM [8].",
    "venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year":2024,
    "publication_date":"2024-04-14",
    "citation_count":2,
    "reference_count":17,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icassp\/Zheng0TSQ024",
      "DOI":"10.1109\/ICASSP48485.2024.10446916",
      "CorpusId":268605037
    }
  },
  {
    "paper_id":"82bd415d28721c84d0d8b2e731fcd57410ebe6aa",
    "title":"Cottention: Linear Transformers With Cosine Attention",
    "authors":[
      "Gabriel Mongaras",
      "Trevor Dohm",
      "Eric C. Larson"
    ],
    "abstract":"Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-09-27",
    "citation_count":2,
    "reference_count":38,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2409-18747",
      "ArXiv":"2409.18747",
      "DOI":"10.48550\/arXiv.2409.18747",
      "CorpusId":272969212
    }
  },
  {
    "paper_id":"6796b7db48bc8d617e71d5e3a1c9bce763f260dc",
    "title":"A Scalable GPT-2 Inference Hardware Architecture on FPGA",
    "authors":[
      "Anil Yemme",
      "S. Garani"
    ],
    "abstract":"Transformer-based architectures using attention mechanisms are a class of learning architectures for sequence processing tasks. These include architectures such as the generative pretrained transformer (GPT) and the bidirectional encoder representations from transformers (BERT). GPT-2 is a popular sequence learning architecture that uses transformer architecture. GPT-2 is trained on text prediction, and the network parameters obtained during this training process can be used in various other tasks like text classification and premise-hypothesis testing. Edge computing is an recent trend in which training is done on cloud or server with multiple GPUs, but inference is done on edge devices like mobile phones to reduce latency and improve privacy. This necessitates a study of GPT-2 performance and complexity to distill hardware-based architectures for their usability on edge devices. In this paper, a single layer of GPT-2 based inference architecture is implemented on Virtex-7 xc7vx485tffg1761-2 FPGA board. The inference engine has model dimensionality of 128 and latency of 1.637 ms while operating at 142.44 MHz, consuming 85.6K flip-flops and 96.8K lookup tables, achieving 1.73x speedup compared to previously reported work on transformer-based architecture. The approach proposed in this paper is scalable to models of higher dimensionality.",
    "venue":"IEEE International Joint Conference on Neural Network",
    "year":2023,
    "publication_date":"2023-06-18",
    "citation_count":5,
    "reference_count":14,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/ijcnn\/YemmeG23",
      "DOI":"10.1109\/IJCNN54540.2023.10191067",
      "CorpusId":260388254
    }
  },
  {
    "paper_id":"31ca9175ad5042a09046bf91e1bb414ef0ae0513",
    "title":"Permutation Equivariance of Transformers and its Applications",
    "authors":[
      "Hengyuan Xu",
      "Liyao Xiang",
      "Hang Ye",
      "Dixi Yao",
      "Pengzhi Chu",
      "Baochun Li"
    ],
    "abstract":"Revolutionizing the field of deep learning, Transformer-based models have achieved remarkable performance in many tasks. Recent research has recognized these models are robust to shuffling but are limited to inter-token permutation in the forward propagation. In this work, we propose our definition of permutation equivariance, a broader concept covering both inter- and intra- token per-mutation in the forward and backward propagation of neural networks. We rigorously proved that such permutation equivariance property can be satisfied on most vanilla Transformer-based models with almost no adaptation. We examine the property over a range of state-of-the-art models including ViT, Bert, GPT, and others, with experimental validations. Further, as a proof-of-concept, we explore how real-world applications including privacy-enhancing split learning, and model authorization, could exploit the permutation equivariance property, which implicates wider, intriguing application scenarios. The code is available at https:\/\/github.com\/Doby-Xu\/ST",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2023,
    "publication_date":"2023-04-16",
    "citation_count":22,
    "reference_count":39,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/XuXYYCL24",
      "ArXiv":"2304.07735",
      "DOI":"10.1109\/CVPR52733.2024.00572",
      "CorpusId":258179027
    }
  },
  {
    "paper_id":"5dc644c3c5aa2ea85f27e7102035d4fc7f606893",
    "title":"Zero-Shot Extraction of Seizure Outcomes from Clinical Notes Using Generative Pretrained Transformers",
    "authors":[
      "W. K. Ojemann",
      "Kevin Xie",
      "Kevin Liu",
      "Ellie Chang",
      "Dan Roth",
      "B. Litt",
      "Colin A. Ellis"
    ],
    "abstract":"Emerging evidence has shown that pre-trained encoder transformer models can extract information from unstructured clinic note text but require manual annotation for supervised fine-tuning. Large, Generative Pre-trained Transformer (GPT) models may streamline this process. In this study, we explore GPTs in zero- and few-shot learning scenarios to analyze clinical health records. We prompt-engineered Llama2 13B to optimize performance in extracting seizure freedom from epilepsy clinic notes and compared it against zero-shot and fine-tuned Bio + ClinicalBERT (BERT) models. Our evaluation encompasses different prompting paradigms, including one-word answers, elaboration-based responses, prompts with date formatting instructions, and prompts with dates in context. We found promising median accuracy rates in seizure freedom classification for zero-shot GPTs: one-word—62%, elaboration—50%, prompts with formatted dates—62%, and prompts with dates in context—74%. These outperform the zero-shot BERT model (25%) but fall short of the fully fine-tuned BERT model (84%). Furthermore, in sparse contexts, such as notes from general neurologists, the best performing GPT (76%) surpasses the fine-tuned BERT model (67%) in extracting seizure freedom. This study demonstrates the potential of GPTs in extracting clinically relevant information from unstructured EHR text, offering insights into population trends in seizure management, drug effects, risk factors, and healthcare disparities. Moreover, GPTs exhibit superiority over task-specific models in contexts with the potential to include less precise descriptions of epilepsy and seizures, highlighting their versatility. Additionally, simple prompt engineering techniques enhance model accuracy, presenting a framework for leveraging EHR data with zero clinical annotation.",
    "venue":"Journal of Healthcare Informatics Research",
    "year":2024,
    "publication_date":"2024-11-04",
    "citation_count":1,
    "reference_count":61,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/jhir\/OjemannXLCRLE25",
      "PubMedCentral":"12290146",
      "DOI":"10.1007\/s41666-025-00198-5",
      "CorpusId":273807962,
      "PubMed":"40726746"
    }
  },
  {
    "paper_id":"4fa5e007709a23ccc4ecbafe13e09e02d419147c",
    "title":"Understanding Transformers: A Comprehensive Review",
    "authors":[
      "Berlina Rahmadhani",
      "Purwono Purwono",
      "Safar Dwi Kurniawan"
    ],
    "abstract":"Transformers have been recognized as one of the most significant innovations in the development of deep learning technology, with widespread application to Natural Language Processing (NLP), Computer Vision (CV), and multimodal data analysis. The self-attention mechanism, which is at the core of this architecture, is designed to capture global relationships in sequential and spatial data in parallel, enabling more efficient and accurate processing than Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN)-based approaches. Models such as BERT, GPT, and Vision Transformer (ViT) have been used for a variety of tasks, including text classification, translation, object detection, and image segmentation. Although the advantages of this model are significant, the high computing power requirements and reliance on large datasets are major challenges. Efforts to overcome these limitations have been made through the development of lightweight variants, such as the MobileViT and Swin Transformer, which are designed to improve efficiency without sacrificing accuracy. Further research is also directed at the application of transformers for multimodal data and specific domains, such as medical image analysis. With its high flexibility and adaptability, transformers continue to be regarded as a key component in the development of more advanced and far-reaching artificial intelligence.",
    "venue":"Journal of Advanced Health Informatics Research",
    "year":2024,
    "publication_date":"2024-08-31",
    "citation_count":0,
    "reference_count":41,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.59247\/jahir.v2i2.292",
      "CorpusId":277776185
    }
  },
  {
    "paper_id":"fbea9004b389683f2b0c1d1c46f80db2b88b3e75",
    "title":"A Comprehensive Analysis on Sentiment Using Deep Learning and Transformers",
    "authors":[
      "Md Mohsin",
      "Jaishri Wankhede",
      "Afshan Fatima",
      "V. K. Solanki"
    ],
    "abstract":"Emotions describe the social attachment between the human that are ascendancy by cultural norms, social interactions, and Interpersonal bonds. So in this paper we are represent the application of deep learning models and transformers for emotion analysis on text based data. We are using different Word2Vec approaches for analyze the emotion from the textual data. As we know, Distributed Bag of Words (DBOW), Distributed Memory Concatenation (DMC), Distributed Memory Mean (DMM), and hybrid models DBOW+DMC and DBOW+DMM that’s all are the part of Word2Vec model and all are based on deep learning techniques. In this paper we also explore the NLP transformer such as BERT, BART, DistilBERT, RoBERTa, and GPT to analyze the emotion of data and compare the performance of deep learning based technique with transformers based techniques. The aim of the exploration is compare the effectiveness of these models using standard sentiment analysis datasets, assessing performance through accuracy metrics.",
    "venue":"2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",
    "year":2024,
    "publication_date":"2024-10-19",
    "citation_count":0,
    "reference_count":24,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICCCMLA63077.2024.10871474",
      "CorpusId":276290706
    }
  },
  {
    "paper_id":"af5a8967df4fddaf58e4f2e076071fa3aec8805e",
    "title":"Nursing-Care Text Classification and Extraction of Important Terms for Classification Using Transformers",
    "authors":[
      "Tomoya Kampu",
      "Manabu Nii",
      "Eiko Nakanishi",
      "R. Sakashita"
    ],
    "abstract":"In the medical field, improving the quality of nursing-care by nurses is important for improving the quality of medical services. Therefore, a Web-based Nursing-care Quality Improvement System has been developed by the Japan Institute for Nursing Quality Improvement (JINQI) that evaluates the quality of nursing care in each hospital ward from an independent organization. Currently, we are developing a classification system for nursing-care texts using artificial intelligence technology to support nursing-care experts and enable more hospital wards to use this system. In our former studies, we investigated classification performance by UTH-BERT and visualized the important terms and phrases for classification using attention weights. In this paper, nursing-care texts are classified using models of JmedRoBERTa (a domain-specific model of RoBERTa), rinna-GPT2 (a general-purpose model of the GPT series), and ELECTRA japanese (a general-purpose model of ELECTRA), which are derivative models based on Transformers. We also visualize important terms using SHAP values. From our experimental results, JmedRoBERTa and ELECTRA showed better performance than previous studies. Terms extracted using SHAP values also showed bases close to the judgment bases by nursing-care experts.",
    "venue":"International Conference on Machine Learning and Computing",
    "year":2024,
    "publication_date":"2024-09-20",
    "citation_count":0,
    "reference_count":15,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icmlc\/KampuNNS24",
      "DOI":"10.1109\/ICMLC63072.2024.10935220",
      "CorpusId":277382474
    }
  },
  {
    "paper_id":"78d989ee92b8e1605b459447bb54e5f9d395f111",
    "title":"Revolutionizing Content Digestion: Unleashing the Power of Bidirectional and Auto-Regressive Transformers in AI-Powered Automatic Text Summarization",
    "authors":[
      "Ms. Vaishali V. Jikar",
      "Dr.Gurudev B. Sawarkar",
      "Ms. Rupali Dasarkar",
      "Ms. Minakshi Dobale"
    ],
    "abstract":"Automatic text summarization has become increasingly essential in managing the overwhelming volume of textual information available across various domains. This paper explores the role of bidirectional and auto-regressive transformers, two prominent paradigms in natural language processing (NLP), in revolutionizing content digestion through AI-powered automatic text summarization. We discuss how bidirectional transformers, exemplified by models like BERT, and auto-regressive transformers, such as GPT, capture context and generate output tokens sequentially, respectively, contributing to the production of accurate and coherent summaries. By providing an overview of the challenges posed by the vast volume of textual data and the significance of automatic summarization, we delve into key advancements in NLP, emphasizing the development and applications of bidirectional and auto-regressive transformers in text summarization. Furthermore, we survey state-of-the-art models like BART and its derivatives, highlighting their convergence of bidirectional and auto-regressive techniques. Through a comprehensive analysis, we elucidate the transformative potential of bidirectional and auto-regressive transformers, offering valuable insights for researchers and practitioners in content digestion and NLP-driven knowledge extraction.",
    "venue":"International Journal For Multidisciplinary Research",
    "year":2024,
    "publication_date":"2024-05-13",
    "citation_count":0,
    "reference_count":13,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.36948\/ijfmr.2024.v06i03.19417",
      "CorpusId":269813542
    }
  },
  {
    "paper_id":"e7123082f57663a62d1324198ef8b6ab78d44cb8",
    "title":"Comparative Analysis of Fine-Tuned LLM, BERT and DL Models for Customer Sentiment Analysis",
    "authors":[
      "Anandan Chinnalagu"
    ],
    "abstract":"The fine-tuned Large Language Models such as Generative Pre-Trained Transformers (GPT), Google's and BERT models are leveraged for NLP tasks. The online businesses are relying on customers' online review posting and positive feedback to improve the products sales and services. To predict the accurate emotion and sentiment of the customers remains challenging. There are literatures and sentiment models' research studies show that the traditional models are having performance and accuracy issues. To overcome the sentiment prediction accuracy and model performance issues, author propose the fine-tuned Mistral LLM and BERT models. These models' experimental study results show that fine-tuned LLM outperforms traditional models, and it predicts more accurate sentiments of the customers.",
    "venue":"SMART",
    "year":2024,
    "publication_date":"2024-12-06",
    "citation_count":1,
    "reference_count":19,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/SMART63812.2024.10882546",
      "CorpusId":276508800
    }
  },
  {
    "paper_id":"ad73c9595878d826da5450685d4bdbb7a9fb2df3",
    "title":"A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers",
    "authors":[
      "Jordan Meadows",
      "Marco Valentino",
      "Damien Teney",
      "André Freitas"
    ],
    "abstract":"This paper proposes a methodology for generating and perturbing detailed derivations of equations at scale, aided by a symbolic engine, to evaluate the generalisability of Transformers to out-of-distribution mathematical reasoning problems. Instantiating the framework in the context of sequence classification tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned BERT models, exploring the relationship between specific operators and generalisation failure via the perturbation of reasoning aspects such as symmetry and variable surface forms. Surprisingly, our empirical evaluation reveals that the average in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning can reduce their performance by up to 80 F1 points. Overall, the results suggest that the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training, and highlight a shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities. We release the full codebase, constructed datasets, and fine-tuned models to encourage future progress in the field.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-05-21",
    "citation_count":12,
    "reference_count":74,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/MeadowsVTF24",
      "ACL":"2024.naacl-long.84",
      "ArXiv":"2305.12563",
      "DOI":"10.18653\/v1\/2024.naacl-long.84",
      "CorpusId":258832763
    }
  },
  {
    "paper_id":"eb6fb136d980316e6745d23ab2b4d805feb0c2b4",
    "title":"Generative Pretrained Transformers for Emotion Detection in a Code-Switching Setting",
    "authors":[
      "Andrew Nedilko"
    ],
    "abstract":"This paper describes the approach that we utilized to participate in the shared task for multi-label and multi-class emotion classification organized as part of WASSA 2023 at ACL 2023. The objective was to build mod- els that can predict 11 classes of emotions, or the lack thereof (neutral class) based on code- mixed Roman Urdu and English SMS text messages. We participated in Track 2 of this task - multi-class emotion classification (MCEC). We used generative pretrained transformers, namely ChatGPT because it has a commercially available full-scale API, for the emotion detec- tion task by leveraging the prompt engineer- ing and zero-shot \/ few-shot learning method- ologies based on multiple experiments on the dev set. Although this was the first time we used a GPT model for the purpose, this ap- proach allowed us to beat our own baseline character-based XGBClassifier, as well as the baseline model trained by the organizers (bert- base-multilingual-cased). We ranked 4th and achieved the macro F1 score of 0.7038 and the accuracy of 0.7313 on the blind test set.",
    "venue":"Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    "year":2023,
    "publication_date":null,
    "citation_count":11,
    "reference_count":5,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2023.wassa-1.61",
      "DBLP":"conf\/wassa\/Nedilko23",
      "DOI":"10.18653\/v1\/2023.wassa-1.61",
      "CorpusId":260063140
    }
  },
  {
    "paper_id":"0fea521e4d094e4e5eb3a3a290b6b8f52e8f9327",
    "title":"Automatic Question Generation Using Natural Language Processing and Transformers",
    "authors":[
      "Reyam Magdy Elshiny",
      "Abeer Hamdy"
    ],
    "abstract":"Online education’s rapid growth and the rise of E-learning tools have raised the demand for creating assessments and challenging questions for learners which require significant effort to create suitable content for testing. Consequently, automatic question generation aims to automate the creation of different types of questions in the shortest period of time possible by applying minimal human effort all using natural language processing and transformers. This paper proposes different methodologies to generate questions like true or false, fill in the blanks, matching, multiple-choice, and “Wh-” questions specified from a given context. Transformer models including GPT, T5, and BERT are used to achieve the methodologies needed to generate such questions. The system, tested through surveys, generated an 80% satisfaction rate among teacher participants and showed potential to generate questions similar to ChatGPT’s.",
    "venue":"International Conferences on Computing Advancements",
    "year":2023,
    "publication_date":"2023-11-28",
    "citation_count":5,
    "reference_count":18,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icca3\/ElshinyH23",
      "DOI":"10.1109\/ICCA59364.2023.10401848",
      "CorpusId":267204952
    }
  },
  {
    "paper_id":"179090dd84550fbdbfa264019b868d5551466a5b",
    "title":"BERTuit: Understanding Spanish language in Twitter with transformers",
    "authors":[
      "Javier Huertas-Tato",
      "Alejandro Martín",
      "David Camacho"
    ],
    "abstract":"The appearance of complex attention‐based language models such as BERT, RoBERTa or GPT‐3 has allowed to address highly complex tasks in a plethora of scenarios. However, when applied to specific domains, these models encounter considerable difficulties. This is the case of Social Networks such as Twitter, an ever‐changing stream of information written with informal and complex language, where each message requires careful evaluation to be understood even by humans given the important role that context plays. Addressing tasks in this domain through Natural Language Processing involves severe challenges. When powerful state‐of‐the‐art multilingual language models are applied to this scenario, language specific nuances get lost in translation. To face these challenges we present BERTuit, the largest transformer proposed so far for Spanish language, pre‐trained on a massive dataset of 230 M Spanish tweets using RoBERTa optimization. Our motivation is to provide a powerful resource to better understand Spanish Twitter and to be used on applications focused on this social network, with special emphasis on solutions devoted to tackle the spreading of misinformation in this platform. BERTuit is evaluated on several tasks and compared against M‐BERT, XLM‐RoBERTa and XLM‐T, very competitive multilingual transformers. The utility of our approach is shown with applications, in this case: an unsupervised methodology to visualize groups of hoaxes; and supervised profiling of authors spreading disinformation.",
    "venue":"Expert Syst. J. Knowl. Eng.",
    "year":2023,
    "publication_date":"2023-07-24",
    "citation_count":5,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/es\/HuertasTatoMC23",
      "DOI":"10.1111\/exsy.13404",
      "CorpusId":260229579
    }
  },
  {
    "paper_id":"58e11c65eb151d461569549a5aa3cf1c413430bd",
    "title":"NewAgeHealthWarriors at MEDIQA-Chat 2023 Task A: Summarizing Short Medical Conversation with Transformers",
    "authors":[
      "Prakhar Mishra",
      "Ravi Theja Desetty"
    ],
    "abstract":"This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for MEDIQA-Chat 2023 Task A: Short Dialogue2Note Summarization. Manual creation of these clinical notes requires extensive human efforts, thus making it a time-consuming and expensive process. To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants, and Rule-based systems to automatically generate clinical notes from these conversations. The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks on the test set (ranking 8th on the leaderboard for both tasks) and shows better performance compared to a baseline system using BART variants.",
    "venue":"Clinical Natural Language Processing Workshop",
    "year":2023,
    "publication_date":null,
    "citation_count":2,
    "reference_count":23,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl-clinicalnlp\/MishraD23",
      "ACL":"2023.clinicalnlp-1.44",
      "DOI":"10.18653\/v1\/2023.clinicalnlp-1.44",
      "CorpusId":259833776
    }
  },
  {
    "paper_id":"84c1102305595155feb33fa7ebdc37f3310b571e",
    "title":"ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers",
    "authors":[
      "Zhewei Yao",
      "Reza Yazdani Aminabadi",
      "Stephen Youn",
      "Xiaoxia Wu",
      "Elton Zheng",
      "Yuxiong He"
    ],
    "abstract":"Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16\/BF16 mode, enhancing accuracy.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-10-26",
    "citation_count":2,
    "reference_count":25,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2310-17723",
      "ArXiv":"2310.17723",
      "DOI":"10.48550\/arXiv.2310.17723",
      "CorpusId":264555298
    }
  },
  {
    "paper_id":"e1ec9a011a049115ce3dc45aeb738ad34d726f87",
    "title":"Decepticon: Attacking Secrets of Transformers",
    "authors":[
      "Mujahid Al Rafi",
      "Yuan Feng",
      "Fan Yao",
      "Meng Tang",
      "Hyeran Jeon"
    ],
    "abstract":"With the growing burden of training deep learning models with huge datasets, transfer learning has been widely adopted (e.g., Transformers like BERT, GPT). Transfer learning significantly reduces the time and effort of model training. However, the security impact of using shared pre-trained models has not been evaluated. In this paper, we provide in-depth characterizations of the fine-tuning process and reveal the security vulnerabilities of transfer-learned models. Then, we show a novel two-level model extraction attack; 1) identifying the pre-trained model of a victim transfer-learned model through model fingerprint collected from off-the-shelf GPUs and 2) extracting the entire weights of the victim black-box model based on the hints in the pre-trained model. The extracted model shows almost alike prediction accuracy with over 94% matching prediction outputs with the victim model. The two-level model extraction enables large model weight extraction that is considered as challenging if not impossible through significantly reduced extraction effort.",
    "venue":"IEEE International Symposium on Workload Characterization",
    "year":2023,
    "publication_date":"2023-10-01",
    "citation_count":1,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iiswc\/RafiFYTJ23",
      "DOI":"10.1109\/IISWC59245.2023.00028",
      "CorpusId":264520916
    }
  },
  {
    "paper_id":"3d7e004b8a20467937231539a805e3d5c9258c15",
    "title":"BIOFORMERS: A SCALABLE FRAMEWORK FOR EXPLORING BIOSTATES USING TRANSFORMERS",
    "authors":[
      "Siham Amara-Belgadi",
      "Orion Li",
      "D. Zhang",
      "Ashwin Gopinath"
    ],
    "abstract":"Generative pre-trained models, such as BERT and GPT, have demonstrated remarkable success in natural language processing and computer vision. Leveraging the combination of large-scale, diverse datasets, transformers, and unsupervised learning, these models have emerged as a promising method for understanding complex systems like language. Despite the apparent differences, human language and biological systems share numerous parallels. Biology, like language, is a dynamic, interconnected network where biomolecules interact to create living entities akin to words forming coherent narratives. Inspired by this analogy, we explored the potential of using transformer-based unsupervised model development for analyzing biological systems and proposed a framework that can ingest vast amounts of biological data to create a foundational model of biology using BERT or GPT. This framework focuses on the concept of a ‘biostate,’ defined as a high-dimensional vector encompassing various biological markers such as genomic, proteomic, transcriptomic, physiological, and phenotypical data. We applied this technique to a small dataset of single-cell transcriptomics to demonstrate its ability to capture meaningful biological insights into genes and cells, even without any pre-training. Furthermore, the model can be readily used for gene network inference and genetic perturbation prediction.",
    "venue":"bioRxiv",
    "year":2023,
    "publication_date":"2023-12-01",
    "citation_count":1,
    "reference_count":29,
    "fields_of_study":[
      "Biology"
    ],
    "external_ids":{
      "DOI":"10.1101\/2023.11.29.569320",
      "CorpusId":265660086
    }
  },
  {
    "paper_id":"1fccc6107980176a19e40c1d0d08273448ffe6be",
    "title":"Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights",
    "authors":[
      "Zhaiming Shen",
      "Alex Havrilla",
      "Rongjie Lai",
      "Alexander Cloninger",
      "Wenjing Liao"
    ],
    "abstract":"Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data\/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.",
    "venue":"arXiv.org",
    "year":2025,
    "publication_date":"2025-05-06",
    "citation_count":2,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "ArXiv":"2505.03205",
      "DBLP":"journals\/corr\/abs-2505-03205",
      "DOI":"10.48550\/arXiv.2505.03205",
      "CorpusId":278339200
    }
  },
  {
    "paper_id":"d8b2f619f83dfc150ca219ec4ffa14cb111ad137",
    "title":"Transformers in Sentiment Analysis: A Paradigm Shift in Deep Learning Research",
    "authors":[
      "Kumar Puttaswamy Gowda",
      "Dr. Rabins Porwal",
      "Cindhe Ramesh",
      "S. Tiwari",
      "Dr. Kriti Srivastava",
      "Dr. R. Rambabu",
      "Dr S Govinda"
    ],
    "abstract":"It was the Transformers which changed the paradigm for the sentiment analysis, sending shock waves to deep learning with its architecture and unprecedented effectiveness. An attention mechanism abstracts the significant features of the input in the self-attention layer, leading to a reconsideration of both pre-trained models, such as BERT, RoBERTa and GPT, at all levels of the sentiment analysis pipeline. These models utilize self-attention mechanisms, allowing them to capture syntactic and semantic dependencies more effectively than recurrent and convolutional networks, leading to significant improvements in various NLP tasks. A rigorous methodology was applied, including fine-tuning pre-trained transformers on heterogeneous datasets and comparing their performance against state-of-the-art methods. The numerous experimentation showed the improvements in terms of accuracy, precision, and recall in some domains such as customer reviews, social media sentiments, and financial data analysis. The study reveals key findings demonstrating the adaptability of the models to solve domain-specific challenges with transfer learning and their efficiency in handling imbalanced datasets. More than that, the paper also describes the trade-offs between computational cost, scalability, and other aspects to consider when implementing transformers in practice. With effective syntactic and semantic embeddings being learned by transformer-based models, this study demonstrates that such deep learning-based architectures redefine performance standards for sentiment analysis tasks and serve as promising basis for building even better interpretable superclass models, suggesting their tremendous potential in shaping current and future research trends in both natural language processing and beyond.",
    "venue":"Journal of Information Systems Engineering & Management",
    "year":2025,
    "publication_date":"2025-01-24",
    "citation_count":2,
    "reference_count":15,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.52783\/jisem.v10i5s.612",
      "CorpusId":276166681
    }
  },
  {
    "paper_id":"3374fea545b805729277cf23dc50854f3e09d693",
    "title":"Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support",
    "authors":[
      "Olajumoke Taiwo",
      "Baidaa Al-Bander"
    ],
    "abstract":"Mental health disorders have a global prevalence of 25%, according to the WHO, and this is exacerbated by factors such as stigma, geographical location, and a worldwide shortage of practitioners. Mental health chatbots have been developed to address these barriers, but these systems lack key features such as emotion recognition, personalisation, multilingual support, and ethical appropriateness. This paper introduces an innovative mental health support system that integrates BERT‐based emotional distress detection with a psychological first aid (PFA)‐generative pre‐trained transformer (PFA‐GPT) model, providing an emotion‐aware PFA chatbot. The methodology leverages deep learning models, utilising bidirectional encoder representations from transformers (BERT) for emotional distress detection and fine‐tuning GPT‐3.5 on therapy transcripts for PFA chatbot development. The findings demonstrate BERT's superior accuracy (93%) for emotional distress detection compared to bidirectional long short‐term memory. The multilingual PFA chatbot developed using the PFA‐GPT model demonstrated superior BERT scores (exceeding 83%) and proficiently provided ethical PFA. A proof of concept has been developed to illustrate the integration of the emotional distress detection model with the novel generative conversational agent for PFA. This integrated approach holds significant potential in overcoming existing barriers to mental health support and has the potential to transform mental health support, offering timely and accessible care through AI‐powered psychological interventions.",
    "venue":"Cognitive Computation and Systems",
    "year":2025,
    "publication_date":"2025-01-01",
    "citation_count":3,
    "reference_count":39,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/ccs\/TaiwoA25",
      "DOI":"10.1049\/ccs2.12116",
      "CorpusId":275911331
    }
  },
  {
    "paper_id":"9cbd838531b084bd402dec4dffdcb34d5d5e7f52",
    "title":"POSTER: TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in Mobile Networks using Transformers, Adversarial Learning, and Differential Privacy",
    "authors":[
      "Al Nahian Bin Emran",
      "Dhiman Goswami",
      "Md Hasan Ullah Sadi",
      "Sanchari Das"
    ],
    "abstract":"The proliferation of propaganda on mobile platforms raises critical concerns around detection accuracy and user privacy. To address this, we propose TRIDENT -a three-tier propaganda detection model implementing transformers, adversarial learning, and differential privacy which integrates syntactic obfuscation and label perturbation to mitigate privacy leakage while maintaining propaganda detection accuracy. TRIDENT leverages multilingual back-translation to introduce semantic variance, character-level noise, and entity obfuscation for differential privacy enforcement, and combines these techniques into a unified defense mechanism. Using a binary propaganda classification dataset, baseline transformer models (BERT, GPT-2) we achieved F1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a reduced but effective cumulative F1 of 0.83, demonstrating strong privacy protection across mobile ML deployments with minimal degradation.",
    "venue":"Wireless Network Security",
    "year":2025,
    "publication_date":"2025-06-05",
    "citation_count":0,
    "reference_count":5,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/wisec\/EmranGSD25",
      "ArXiv":"2506.05421",
      "DOI":"10.1145\/3734477.3736148",
      "CorpusId":279244872
    }
  },
  {
    "paper_id":"619ce5bd7d9bf895a55448283f09b03ea910de80",
    "title":"Transformers for Domain-Specific Text Classification: A Case Study in the Banking Sector",
    "authors":[
      "Samer Murrar",
      "Fatima Alhaj",
      "Fadi Almasalha",
      "Mahmoud H. Qutqut"
    ],
    "abstract":"The growing volume of unstructured text data in the banking sector has created a need for advanced classification methods to manage customer inquiries efficiently, resulting in faster response times, automated message classification, and reduced human errors. The classification results are integrated into live banking systems, enabling continuous 24\/7 message processing and instant categorization. Previously, human operators could process up to 70 messages per day during working hours. With AI integration, the system now categorizes messages automatically and in real time, ensuring that relevant department managers receive them promptly, significantly improving operational workflows. Our research offers key insights into applying transformer models for domain-specific classification in banking. This paper presents a case study on fine-tuning transformer models—BERT, GPT-2, GPT-3, and Falcon-1B for domain-specific text classification in the banking industry. The dataset used in this investigation consists of 5,447 customer messages submitted between 2023 and 2024 through Invest Bankś secure messaging portal, which serves as a direct communication channel where customers can submit their requests at any time of the day without needing to visit the bank in person. The inquiries span fifteen service categories provided by the bank. To the best of our knowledge, no prior studies have focused on automating customer secure messages in the banking sector, especially those written in colloquial and abbreviated language. The models were fine-tuned to improve classification accuracy and operational efficiency. A heterogeneous ensemble of BERT and GPT-2 achieved the best performance with an Area Under the Curve (AUC) score of 99.42% and an F1 score of 86.74%. A homogeneous ensemble of BERT models also performed well, achieving an AUC score of 98.81% and an F1 score of 84.57%. Notably, the single BERT and GPT-2 models, with AUC scores of 97.65% and 97.9%, respectively, delivered competitive performance, making them viable alternatives when computational resources are limited.",
    "venue":"IEEE Access",
    "year":2025,
    "publication_date":null,
    "citation_count":0,
    "reference_count":38,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ACCESS.2025.3585164",
      "CorpusId":279887416
    }
  },
  {
    "paper_id":"a239cda3806cb588f47c726dd444dbbc49e57381",
    "title":"Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers",
    "authors":[
      "KM Khalid Saifullah",
      "Faiaz Azmain",
      "Habiba Hye"
    ],
    "abstract":"Sentiment analysis plays a crucial role in understanding developer interactions, issue resolutions, and project dynamics within software engineering (SE). While traditional SE-specific sentiment analysis tools have made significant strides, they often fail to account for the nuanced and context-dependent language inherent to the domain. This study systematically evaluates the performance of bidirectional transformers, such as BERT, against generative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment analysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark the models' capabilities with fine-tuned and default configurations. The results reveal that fine-tuned GPT-4o-mini performs comparable to BERT and other bidirectional models on structured and balanced datasets like GitHub and Jira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively. However, on linguistically complex datasets with imbalanced sentiment distributions, such as Stack Overflow, the default GPT-4o-mini model exhibits superior generalization, achieving an accuracy of 85.3\\% compared to the fine-tuned model's 13.1\\%. These findings highlight the trade-offs between fine-tuning and leveraging pre-trained models for SE tasks. The study underscores the importance of aligning model architectures with dataset characteristics to optimize performance and proposes directions for future research in refining sentiment analysis tools tailored to the SE domain.",
    "venue":"arXiv.org",
    "year":2025,
    "publication_date":"2025-04-22",
    "citation_count":0,
    "reference_count":24,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2505.14692",
      "DBLP":"journals\/corr\/abs-2505-14692",
      "DOI":"10.48550\/arXiv.2505.14692",
      "CorpusId":278783155
    }
  },
  {
    "paper_id":"07099fe26ee8850c9ccba6fe2ee139d67289b67c",
    "title":"Foundation Transformers",
    "authors":[
      "Hongyu Wang",
      "Shuming Ma",
      "Shaohan Huang",
      "Li Dong",
      "Wenhui Wang",
      "Zhiliang Peng",
      "Yu Wu",
      "Payal Bajaj",
      "Saksham Singhal",
      "Alon Benhaim",
      "Barun Patra",
      "Zhun Liu",
      "Vishrav Chaudhary",
      "Xia Song",
      "Furu Wei"
    ],
    "abstract":"A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name\"Transformers\", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-10-12",
    "citation_count":28,
    "reference_count":38,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2210-06423",
      "ArXiv":"2210.06423",
      "DOI":"10.48550\/arXiv.2210.06423",
      "CorpusId":252846241
    }
  },
  {
    "paper_id":"7a5cd1fcdb2a67b5a7d17826b1e6d952be896575",
    "title":"XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models",
    "authors":[
      "Ariana Sahitaj",
      "Jiaao Li",
      "Pia Wenzel Neves",
      "Fedor Splitt",
      "Premtim Sahitaj",
      "Charlott Jakob",
      "Veronika Solopova",
      "Vera Schmitt"
    ],
    "abstract":"This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared task on multilingual subjectivity detection. We evaluate two approaches: (1) supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and German-BERT, on monolingual and machine-translated training data; and (2) zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and Perspective (comparative reasoning). The Annotation Approach achieves 1st place in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the baseline of 0.6461. The same model also performs reliably in the multilingual task and improves over the baseline in Greek. For German, a German-BERT model fine-tuned on translated training data from typologically related languages yields competitive performance over the baseline. In contrast, performance in the Ukrainian and Polish zero-shot settings falls slightly below the respective baselines, reflecting the challenge of generalization in low-resource cross-lingual scenarios.",
    "venue":"arXiv.org",
    "year":2025,
    "publication_date":"2025-09-15",
    "citation_count":0,
    "reference_count":31,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2509.12130",
      "DBLP":"journals\/corr\/abs-2509-12130",
      "DOI":"10.48550\/arXiv.2509.12130",
      "CorpusId":281315346
    }
  },
  {
    "paper_id":"c51a484cd639681402618e4c27a1bfb415b9767e",
    "title":"Hardware Accelerator for Approximation-Based Softmax and Layer Normalization in Transformers",
    "authors":[
      "Raehyeong Kim",
      "Dayoung Lee",
      "Jinyeol Kim",
      "Joungmin Park",
      "Seung Eun Lee"
    ],
    "abstract":"Transformer-based models have achieved remarkable success across various AI tasks, but their growing complexity has led to significant computational and memory demands. While most optimization efforts have focused on linear operations such as matrix multiplications, non-linear functions like Softmax and layer normalization (LayerNorm) are increasingly dominating inference latency, especially for long sequences and high-dimensional inputs. To address this emerging bottleneck, we present a hardware accelerator that jointly approximates these non-linear functions using piecewise linear approximation for the exponential in Softmax and Newton–Raphson iteration for the square root in LayerNorm. The proposed unified architecture dynamically switches operation modes while reusing hardware resources. The proposed accelerator was implemented on a Xilinx VU37P FPGA and evaluated with BERT and GPT-2 models. Experimental results demonstrate speedups of up to 7.6× for Softmax and 2.0× for LayerNorm, while maintaining less than 1% accuracy degradation on classification tasks with conservative approximation settings. However, generation tasks showed greater sensitivity to approximation, underscoring the need for task-specific tuning.",
    "venue":"Electronics",
    "year":2025,
    "publication_date":"2025-06-07",
    "citation_count":0,
    "reference_count":26,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.3390\/electronics14122337",
      "CorpusId":279295290
    }
  },
  {
    "paper_id":"04696a9ff4ac2bb5562a49943d9b096015b76776",
    "title":"Fine-Tuning Transformers for Sentiment Analysis",
    "authors":[
      "Raghav Dashrath",
      "Shravani Deshmukh",
      "Atharv Bhaleghare"
    ],
    "abstract":"This paper explores advanced techniques for fine-tuning\n pre-trained transformer models such as BERT and GPT for sentiment\n analysis tasks, with a particular focus on handling domain-specific\n language in customer feedback. We propose a novel adaptive transfer\n learning framework that combines contextual embedding augmen\ntation with progressive domain adaptation to improve sentiment\n classification accuracy across diverse domains. Our experimental\n results demonstrate that our proposed methods achieve state-of-the-art\n performance on benchmark datasets, with significant improvements\n in handling domain-specific terminology and contextual nuances in\n customer feedback. We also introduce a new approach to cross\ndomain generalization through contrastive domain adaptation that\n shows promising results for zero-shot adaptation to new domains.",
    "venue":"International Journal For Multidisciplinary Research",
    "year":2025,
    "publication_date":"2025-05-04",
    "citation_count":0,
    "reference_count":10,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.36948\/ijfmr.2025.v07i03.42227",
      "CorpusId":278434031
    }
  },
  {
    "paper_id":"0a08bfb87151ca27982661f6aa130eb353476359",
    "title":"Identification of DNA Coding Regions Using Transformers",
    "authors":[
      "Gustavo H. F. Cruz",
      "Aurora T. R. Pozo"
    ],
    "abstract":"Identifying coding (exon) and non-coding (intron) regions in DNA sequences is fundamental to understanding gene expression and its implications for biological processes and genetic diseases. In this work, we investigate the application of Transformer-based architectures to the task of intron and exon classification, comparing three distinct models: GPT-2, BERT, and DNABERT. These models were selected to evaluate the impact of context modeling strategies—autoregressive, bidirectional, and k-mer-based—on genomic sequence analysis. Experiments were carried out on a curated dataset comprising 100000 training sequences and 30000 test sequences, using mutually exclusive samples to ensure robust evaluation. All models were fine-tuned under uniform conditions, with a fixed batch size of 32 and learning rate constraints, and executed three times with different seeds. The results show that BERT achieved the highest classification accuracy (0.9905), outperforming DNABERT (0.9569) and GPT-2 (0.9867). While DNABERT was the fastest to train due to its k-mer tokenization and lighter computational requirements, its limited capacity to model long-range dependencies impaired its performance. In contrast, GPT-2 demonstrated competitive accuracy but at a higher computational cost, reinforcing the trade-off between generative modeling power and efficiency. This study highlights the importance of context-aware attention mechanisms in genomic sequence modeling and confirms the viability of Transformer architectures—especially bidirectional models like BERT—for high-accuracy classification of intronic and exonic regions. Future work may benefit from exploring larger models, sequence representation alternatives, and training optimization techniques to further enhance performance in genomics applications.",
    "venue":"Anais do XIII Symposium on Knowledge Discovery, Mining and Learning (KDMiLe 2025)",
    "year":2025,
    "publication_date":"2025-09-29",
    "citation_count":0,
    "reference_count":22,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.5753\/kdmile.2025.247575",
      "CorpusId":282286427
    }
  },
  {
    "paper_id":"b5ff831467be490bf94768caa055ea5995d2ab2b",
    "title":"Fit Talks: Forecasting Fitness Awareness in Saudi Arabia Using Fine-Tuned Transformers",
    "authors":[
      "Nora Alturayeif",
      "Deemah A. Alqahtani",
      "Sumayh S. Aljameel",
      "Najla Almajed",
      "Lama Alshehri",
      "Nourah Aldhuwaihi",
      "Madawi Alhadyan",
      "Nouf Aldakheel"
    ],
    "abstract":"Understanding public sentiment on health and fitness is essential for addressing regional health challenges in Saudi Arabia. This research employs sentiment analysis to assess fitness awareness by analyzing content from the X platform (formerly Twitter), using a dataset called Saudi Aware, which includes 3593 posts related to fitness awareness. Preprocessing steps such as normalization, stop-word removal, and tokenization ensured high-quality data. The findings revealed that positive sentiments about fitness and health were more prevalent than negative ones, with posts across all sentiment categories being most common in the western region. However, the eastern region exhibited the highest percentage of positive sentiment, indicating a strong interest in fitness and health. For sentiment classification, we fine-tuned two transformer architectures—BERT and GPT—utilizing three BERT-based models (AraBERT, MARBERT, CAMeLBERT) and GPT-3.5. These findings provide valuable insights into Saudi Arabian attitudes toward fitness and health, offering actionable information for public health campaigns and initiatives.",
    "venue":"Big Data and Cognitive Computing",
    "year":2025,
    "publication_date":"2025-01-23",
    "citation_count":0,
    "reference_count":21,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/bdcc\/AlturayeifAAAAAAA25",
      "DOI":"10.3390\/bdcc9020020",
      "CorpusId":275861093
    }
  },
  {
    "paper_id":"8dd49f94b213834e580eeea18098f402b1be1226",
    "title":"Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers",
    "authors":[
      "Minjia Zhang",
      "Niranjan Uma Naresh",
      "Yuxiong He"
    ],
    "abstract":"Deep and large pre-trained language models (e.g., BERT, GPT-3) are state-of-the-art for various natural language processing tasks. However, the huge size of these models brings challenges to fine-tuning and online deployment due to latency and cost constraints. Existing knowledge distillation methods reduce the model size, but they may encounter difficulties transferring knowledge from the teacher model to the student model due to the limited data from the downstream tasks. In this work, we propose AD^2, a novel and effective data augmentation approach to improving the task-specific knowledge transfer when compressing large pre-trained transformer models. Different from prior methods, AD^2 performs distillation by using an enhanced training set that contains both original inputs and adversarially perturbed samples that mimic the output distribution from the teacher. \n \nExperimental results show that this method allows better transfer of knowledge from the teacher to the student during distillation, producing student models that retain 99.6\\% accuracy of the teacher model while outperforming existing task-specific knowledge distillation baselines by 1.2 points on average over a variety of natural language understanding tasks. Moreover, compared with alternative data augmentation methods, such as text-editing-based approaches, AD^2 is up to 28 times faster while achieving comparable or higher accuracy. In addition, when AD^2 is combined with more advanced task-agnostic distillation, we can advance the state-of-the-art performance even more. On top of the encouraging performance, this paper also provides thorough ablation studies and analysis. The discovered interplay between KD and adversarial data augmentation for compressing pre-trained Transformers may further inspire more advanced KD algorithms for compressing even larger scale models.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2022,
    "publication_date":"2022-06-28",
    "citation_count":15,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/ZhangNH22",
      "DOI":"10.1609\/aaai.v36i10.21423",
      "CorpusId":250304889
    }
  },
  {
    "paper_id":"556eef77b3db096e8f16d50944bb43ba6c5829b8",
    "title":"Multilingual AI-Generated Text Detection with BERT and LSTM Model",
    "authors":[
      "Jeetendra Kumar",
      "Rashmi Gupta",
      "Suvarna Sharma",
      "Jatin Arora",
      "Dharmendra Dangi",
      "Dheeraj kumar dixit"
    ],
    "abstract":"The advent of AI-generated text, driven by advanced language models like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) has revolutionized content creation but also raised concerns about authenticity and misuse in various domains. This research proposes a system to detect whether the supplied text is written by human-being or it generated by AI algorithms, supporting both English and Hindi languages using two datasets with mixed human-authored and AI-generated content. After pre-processing steps such as tokenization, cleaning, and normalization, trained the models including BERT, LSTM (Long Short Term Memory), Logistic Regression, SVM (Support Vector Machine), Random Forest, and Naive Bayes, with BERT finetuned for contextual understanding and LSTM for sequential data, achieving higher accuracy in both languages. A userfriendly interface was developed to allow text file uploads for instant authenticity predictions, serving academic, journalistic, and digital media contexts, with the study highlighting transformer-based models' potential in multilingual detection to enhance digital content trustworthiness and mitigate AIgenerated text misuse. Using the proposed method, accuracies of 95.76 % in English text and 95.58 % in Hindi text have been obtained. The proposed system was found to be effective in detecting text generated by AI algorithms.",
    "venue":"2025 Third International Conference on Networks, Multimedia and Information Technology (NMITCON)",
    "year":2025,
    "publication_date":"2025-08-01",
    "citation_count":0,
    "reference_count":25,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/NMITCON65824.2025.11188051",
      "CorpusId":282006762
    }
  },
  {
    "paper_id":"776e38d8d1bbfe4a5b7b80b803eb3c5df0e1a1b5",
    "title":"Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers",
    "authors":[
      "Z. Yao",
      "Xiaoxia Wu",
      "Conglong Li",
      "Connor Holmes",
      "Minjia Zhang",
      "Cheng Li",
      "Yuxiong He"
    ],
    "abstract":"Large-scale transformer models have become the de-facto architectures for various machine learning applications, e.g., CV and NLP. However, those large models also introduce prohibitive training costs. To mitigate this issue, we propose a novel random and layerwise token dropping method (random-LTD), which skips the computation of a subset of the input tokens at all middle layers. Particularly, random-LTD achieves considerable speedups and comparable accuracy as the standard training baseline. Compared to other token dropping methods, random-LTD does not require (1) any importance score-based metrics, (2) any special token treatment (e.g., [CLS]), and (3) many layers in full sequence length training except the first and the last layers. Besides, a new LayerToken learning rate schedule is proposed for pretraining problems that resolve the heavy tuning requirement for our proposed training mechanism. Finally, we demonstrate that random-LTD can be applied to broader applications, including GPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results show that random-LTD can save about 33.3% theoretical compute cost and 25.6% wall-clock training time while achieving similar zero-shot evaluations on GPT-31.3B as compared to baseline.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-11-17",
    "citation_count":13,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2211.11586",
      "DBLP":"journals\/corr\/abs-2211-11586",
      "DOI":"10.48550\/arXiv.2211.11586",
      "CorpusId":253735208
    }
  },
  {
    "paper_id":"2cc0e605470d3ac20aad82c73560b888ecc449cd",
    "title":"Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
    "authors":[
      "Andrew Silva",
      "Pradyumna Tambwekar",
      "M. Gombolay"
    ],
    "abstract":"The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-06-01",
    "citation_count":64,
    "reference_count":30,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.naacl-main.189",
      "MAG":"3166961312",
      "DBLP":"conf\/naacl\/SilvaTG21",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.189",
      "CorpusId":235097394
    }
  },
  {
    "paper_id":"1b055049c568be70d6a762679cdb93f630d5d6e6",
    "title":"Tabular Transformers for Modeling Multivariate Time Series",
    "authors":[
      "Inkit Padhi",
      "Yair Schiff",
      "Igor Melnyk",
      "Mattia Rigotti",
      "Youssef Mroueh",
      "Pierre L. Dognin",
      "Jerret Ross",
      "Ravi Nair",
      "Erik Altman"
    ],
    "abstract":"Tabular datasets are ubiquitous in data science applications. Given their importance, it seems natural to apply state-of-the-art deep learning algorithms in order to fully unlock their potential. Here we propose neural network models that represent tabular time series that can optionally leverage their hierarchical structure. This results in two architectures for tabular time series: one for learning representations that is analogous to BERT and can be pre-trained end-to-end and used in downstream tasks, and one that is akin to GPT and can be used for generation of realistic synthetic tabular sequences. We demonstrate our models on two datasets: a synthetic credit card transaction dataset, where the learned representations are used for fraud detection and synthetic data generation, and on a real pollution dataset, where the learned encodings are used to predict atmospheric pollutant concentrations. Code and data are available at https:\/\/github.com\/IBM\/TabFormer.",
    "venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year":2020,
    "publication_date":"2020-11-03",
    "citation_count":108,
    "reference_count":23,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2011.01843",
      "DBLP":"journals\/corr\/abs-2011-01843",
      "MAG":"3095004927",
      "DOI":"10.1109\/ICASSP39728.2021.9414142",
      "CorpusId":226237049
    }
  },
  {
    "paper_id":"e7f86fcea0851451eb1fee3763072a00581ea3f9",
    "title":"Analysing similarities between legal court documents using natural language processing approaches based on transformers",
    "authors":[
      "R. S. Oliveira",
      "E. G. Sperandio Nascimento"
    ],
    "abstract":"Recent advancements in Artificial Intelligence have yielded promising results in addressing complex challenges within Natural Language Processing (NLP), serving as a vital tool for expediting judicial proceedings in the legal domain. This study focuses on the detection of similarity among judicial documents within an inference group, employing eight NLP techniques grounded in transformer architecture, specifically applied to a case study of legal proceedings in the Brazilian judicial system. The transformer-based models utilised — BERT, GPT-2, RoBERTa, and LlaMA — were pre-trained on general-purpose corpora of Brazilian Portuguese and subsequently fine-tuned for the legal sector using a dataset of 210,000 legal cases. Vector representations of each legal document were generated based on their embeddings, facilitating the clustering of lawsuits and enabling an evaluation of each model’s performance through the cosine distance between group elements and their centroid. The results demonstrated that transformer-based models outperformed traditional NLP techniques, with the LlaMA model, specifically fine-tuned for the Brazilian legal domain, achieving the highest accuracy. This research presents a methodology employed in a real case involving substantial documentary content that can be adapted for various applications. It conducts a comparative analysis of existing techniques focused on a non-English language to quantitatively explain the results obtained with various NLP transformers-based models. This approach advances the current state of the art in NLP applications within the legal sector and contributes to the achievement of Sustainable Development Goals.",
    "venue":"PLoS ONE",
    "year":2022,
    "publication_date":"2022-04-14",
    "citation_count":2,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "ArXiv":"2204.07182",
      "PubMedCentral":"11978053",
      "DOI":"10.1371\/journal.pone.0320244",
      "CorpusId":258615134,
      "PubMed":"40198739"
    }
  },
  {
    "paper_id":"89ca2f9d748db0b6d1092431627a74b9f1260e0a",
    "title":"Explain to me like I am five - Sentence Simplification Using Transformers",
    "authors":[
      "Aman Agarwal"
    ],
    "abstract":"Sentence simplification aims at making the structure of text easier to read and understand while maintaining its original meaning. This can be helpful for people with disabilities, new language learners, or those with low literacy. Simplification often involves removing difficult words and rephrasing the sentence. Previous research have focused on tackling this task by either using external linguistic databases for simplification or by using control tokens for desired fine-tuning of sentences. However, in this paper we purely use pre-trained transformer models. We experiment with a combination of GPT-2 and BERT models, achieving the best SARI score of 46.80 on the Mechanical Turk dataset, which is significantly better than previous state-of-the-art results. The code can be found at https:\/\/github.com\/amanbasu\/sentence-simplification.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-12-08",
    "citation_count":2,
    "reference_count":14,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2212.04595",
      "DBLP":"journals\/corr\/abs-2212-04595",
      "DOI":"10.48550\/arXiv.2212.04595",
      "CorpusId":254535553
    }
  },
  {
    "paper_id":"8b8c29c0cbb6cbae26b930840396596dd5806f33",
    "title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers",
    "authors":[
      "Anne Lauscher",
      "Olga Majewska",
      "Leonardo F. R. Ribeiro",
      "Iryna Gurevych",
      "N. Rozanov",
      "Goran Glavavs"
    ],
    "abstract":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, joint pre-training (i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective) may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https:\/\/github.com\/wluper\/retrograph.",
    "venue":"Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
    "year":2020,
    "publication_date":"2020-05-24",
    "citation_count":87,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2005-11787",
      "MAG":"3026990524",
      "ACL":"2020.deelio-1.5",
      "ArXiv":"2005.11787",
      "DOI":"10.18653\/v1\/2020.deelio-1.5",
      "CorpusId":218870140
    }
  },
  {
    "paper_id":"8cd4054b41936ba0889edc26be8969c3dc8491d8",
    "title":"Language Models with Transformers",
    "authors":[
      "Chenguang Wang",
      "Mu Li",
      "Alex Smola"
    ],
    "abstract":"The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling. \nIn this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",
    "venue":"arXiv.org",
    "year":2019,
    "publication_date":"2019-04-20",
    "citation_count":129,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1904.09408",
      "MAG":"2936497627",
      "DBLP":"journals\/corr\/abs-1904-09408",
      "CorpusId":127980590
    }
  },
  {
    "paper_id":"939b4b1ff5a21108bb2f8c81117f1d5b230180a9",
    "title":"Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers",
    "authors":[
      "Adebayo Oshingbesan",
      "Courage Ekoh",
      "Germann Atakpa",
      "Yonah Byaruagaba"
    ],
    "abstract":"Text-to-text transformers have shown remarkable success in the task of multi-task transfer learning, especially in natural language processing (NLP). However, while there have been several attempts to train transformers on different domains, there is usually a clear relationship between these domains, e.g.,, code summarization, where the natural language summary describes the code. There have been very few attempts to study how multi-task transfer learning works on tasks in significantly different domains. In this project, we investigated the behavior of multi-domain, multi-task learning using multi-domain text-to-text transfer transformers (MD-T5) on four tasks across two domains - Python Code and Chess. We carried out extensive experiments using three popular training strategies: Bert-style joint pretraining + successive finetuning, GPT-style joint pretraining + successive finetuning, and GPT-style joint pretraining + joint finetuning. Also, we evaluate the model on four metrics - Play Score, Eval Score, BLEU Score, and Multi-Domain Learning Score (MDLS). These metrics measure performance across the various tasks and multi-domain learning. We show that while negative knowledge transfer and catastrophic forgetting are still considerable challenges for all the models, the GPT-style joint pretraining + joint finetuning strategy showed the most promise in multi-domain, multi-task learning as it performs well across all four tasks while still keeping its multi-domain knowledge.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-09-21",
    "citation_count":0,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2209-10106",
      "ArXiv":"2209.10106",
      "DOI":"10.48550\/arXiv.2209.10106",
      "CorpusId":252407491
    }
  },
  {
    "paper_id":"92add61e7c3dd745321fc8a6e113dae8199668ac",
    "title":"Improving Text-to-Code Generation with Features of Code Graph on GPT-2",
    "authors":[
      "Incheon Paik",
      "Jun-Wei Wang"
    ],
    "abstract":"Code generation, as a very hot application area of deep learning models for text, consists of two different fields: code-to-code and text-to-code. A recent approach, GraphCodeBERT uses code graph, which is called data flow, and showed good performance improvement. The base model architecture of it is bidirectional encoder representations from transformers (BERT), which uses the encoder part of a transformer. On the other hand, generative pre-trained transformer (GPT)—another multiple transformer architecture—uses the decoder part and shows great performance in the multilayer perceptron model. In this study, we investigate the improvement of code graphs with several variances on GPT-2 to refer to the abstract semantic tree used to collect the features of variables in the code. Here, we mainly focus on GPT-2 with additional features of code graphs that allow the model to learn the effect of the data stream. The experimental phase is divided into two parts: fine-tuning of the existing GPT-2 model, and pre-training from scratch using code data. When we pre-train a new model from scratch, the model produces an outperformed result compared with using the code graph with enough data.",
    "venue":"Electronics",
    "year":2021,
    "publication_date":"2021-11-05",
    "citation_count":14,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.3390\/electronics10212706",
      "CorpusId":243820661
    }
  },
  {
    "paper_id":"0acc3ee64c34b738a89cbc3790d4e5e1c9deb943",
    "title":"PipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers",
    "authors":[
      "Chaoyang He",
      "Shen Li",
      "M. Soltanolkotabi",
      "S. Avestimehr"
    ],
    "abstract":"The size of Transformer models is growing at an unprecedented pace. It has only taken less than one year to reach trillion-level parameters after the release of GPT-3 (175B). Training such models requires both substantial engineering efforts and enormous computing resources, which are luxuries most research teams cannot afford. In this paper, we propose PipeTransformer, which leverages automated and elastic pipelining and data parallelism for efficient distributed training of Transformer models. PipeTransformer automatically adjusts the pipelining and data parallelism by identifying and freezing some layers during the training, and instead allocates resources for training of the remaining active layers. More specifically, PipeTransformer dynamically excludes converged layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on GLUE and SQuAD datasets. Our results show that PipeTransformer attains a 2.4 fold speedup compared to the state-of-the-art baseline. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design. We also develop open-sourced flexible APIs for PipeTransformer, which offer a clean separation among the freeze algorithm, model definitions, and training accelerations, hence allowing it to be applied to other algorithms that require similar freezing strategies.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-02-05",
    "citation_count":33,
    "reference_count":25,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2102-03161",
      "ArXiv":"2102.03161",
      "CorpusId":231839901
    }
  },
  {
    "paper_id":"8146ca19f80b781a8407594038369014580f2256",
    "title":"CoVShorts: News Summarization Application Based on Deep NLP Transformers for SARS-CoV-2",
    "authors":[
      "Hunar Batra",
      "Akansha Jain",
      "Gargi Bisht",
      "Khushi Srivastava",
      "Meenakshi Bharadwaj",
      "Deepali Bajaj",
      "Urmil Bharti"
    ],
    "abstract":"Amidst the grueling SARS-CoV-2 pandemic which has affected the lives of people across the world, the accelerating growth in COVID-19 related news articles is making it difficult for the general public to stay up-to-date with all the information. News articles are a crucial medium to convey coronavirus-related information across the world to the public. Short summaries of news articles can assist the public in grasping a gist of an entire article without having to read it fully. With the evolution of Deep Learning in Natural Language Processing (NLP), we exploited the power of recent advances in pre-trained and transformer-based NLP models to perform text summarization over the COVID-19 Public Media Dataset. For this, we analyzed and compared the results of BERT, GPT-2, XLNet, BART, and T5. The first three models are among the most popular extractive summarization models and the last two are abstractive summarization models. We evaluated the results of our experiments using ROUGE scores (ROUGE-2 and ROUGE-L) and found that BERT, a transformer autoencoder, outperforms the other models under consideration in SARS-CoV-2 news summarization. Thus, we leveraged BERT in our web application “CoVShorts” to summarize COVID-19 articles. Further, we visually analyzed the dataset to depict the most used words in COVID-19 news articles using Word Cloud to validate the accuracy of the summarization task. CoVShorts will serve the public by helping them in gaining brief, concise, and to-the-point summaries quickly.",
    "venue":"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",
    "year":2021,
    "publication_date":"2021-09-03",
    "citation_count":19,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/icrito51393.2021.9596520",
      "CorpusId":244136960
    }
  },
  {
    "paper_id":"b4f30496a8fa212a40461ca1bdef32169e998902",
    "title":"Efficient pre-training objectives for Transformers",
    "authors":[
      "Luca Di Liello",
      "Matteo Gabburo",
      "Alessandro Moschitti"
    ],
    "abstract":"The Transformer architecture deeply changed the natural language processing, outperforming all previous state-of-the-art models. However, well-known Transformer models like BERT, RoBERTa, and GPT-2 require a huge compute budget to create a high quality contextualised representation. In this paper, we study several efficient pre-training objectives for Transformers-based models. By testing these objectives on different tasks, we determine which of the ELECTRA model's new features is the most relevant. We confirm that Transformers pre-training is improved when the input does not contain masked tokens and that the usage of the whole output to compute the loss reduces training time. Moreover, inspired by ELECTRA, we study a model composed of two blocks; a discriminator and a simple generator based on a statistical model with no impact on the computational performances. Besides, we prove that eliminating the MASK token and considering the whole output during the loss computation are essential choices to improve performance. Furthermore, we show that it is possible to efficiently train BERT-like models using a discriminative approach as in ELECTRA but without a complex generator, which is expensive. Finally, we show that ELECTRA benefits heavily from a state-of-the-art hyper-parameters search.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-04-20",
    "citation_count":16,
    "reference_count":23,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2104.09694",
      "DBLP":"journals\/corr\/abs-2104-09694",
      "CorpusId":233306960
    }
  },
  {
    "paper_id":"7887c02ce61250287cf036cc16a57c2841b8741d",
    "title":"Investigation of Pre-Trained Bidirectional Encoder Representations from Transformers Checkpoints for Indonesian Abstractive Text Summarization",
    "authors":[
      "Henry Lucky",
      "Derwin Suhartono"
    ],
    "abstract":"Text summarization aims to reduce text by removing less useful information to obtain information quickly and precisely. In Indonesian abstractive text summarization, the research mostly focuses on multi-document summarization which methods will not work optimally in single-document summarization. As the public summarization datasets and works in English are focusing on single-document summarization, this study emphasized on Indonesian single-document summarization. Abstractive text summarization studies in English frequently use Bidirectional Encoder Representations from Transformers (BERT), and since Indonesian BERT checkpoint is available, it was employed in this study. This study investigated the use of Indonesian BERT in abstractive text summarization on the IndoSum dataset using the BERTSum model. The investigation proceeded by using various combinations of model encoders, model embedding sizes, and model decoders. Evaluation results showed that models with more embedding size and used Generative Pre-Training (GPT)-like decoder could improve the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score and BERTScore of the model results.",
    "venue":"Journal of Information and Communication Technologies",
    "year":2021,
    "publication_date":"2021-11-11",
    "citation_count":6,
    "reference_count":47,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.32890\/jict2022.21.1.4",
      "CorpusId":244121721
    }
  },
  {
    "paper_id":"f068c591f69af4a836b89624c6a8d8e2790ef37d",
    "title":"A Comparative Study of Transformers on Word Sense Disambiguation",
    "authors":[
      "Avi Chawla",
      "Nidhi Mulay",
      "Vikas Bishnoi",
      "Gaurav Dhama",
      "D. A. K. Singh"
    ],
    "abstract":"Recent years of research in Natural Language Processing (NLP) have witnessed dramatic growth in training large models for generating context-aware language representations. In this regard, numerous NLP systems have leveraged the power of neural network-based architectures to incorporate sense information in embeddings, resulting in Contextualized Word Embeddings (CWEs). Despite this progress, the NLP community has not witnessed any significant work performing a comparative study on the contextualization power of such architectures. This paper presents a comparative study and an extensive analysis of nine widely adopted Transformer models. These models are BERT, CTRL, DistilBERT, OpenAI-GPT, OpenAI-GPT2, Transformer-XL, XLNet, ELECTRA, and ALBERT. We evaluate their contextualization power using two lexical sample Word Sense Disambiguation (WSD) tasks, SensEval-2 and SensEval-3. We adopt a simple yet effective approach to WSD that uses a k-Nearest Neighbor (kNN) classification on CWEs. Experimental results show that the proposed techniques also achieve superior results over the current state-of-the-art on both the WSD tasks",
    "venue":"International Conference on Neural Information Processing",
    "year":2021,
    "publication_date":"2021-11-30",
    "citation_count":6,
    "reference_count":20,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2111-15417",
      "ArXiv":"2111.15417",
      "DOI":"10.1007\/978-3-030-92307-5_87",
      "CorpusId":244729149
    }
  },
  {
    "paper_id":"b26f2037f769d5ffc5f7bdcec2de8da28ec14bee",
    "title":"Dense Passage Retrieval for Open-Domain Question Answering",
    "authors":[
      "Vladimir Karpukhin",
      "Barlas Oğuz",
      "Sewon Min",
      "Patrick Lewis",
      "Ledell Yu Wu",
      "Sergey Edunov",
      "Danqi Chen",
      "Wen-tau Yih"
    ],
    "abstract":"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-04-10",
    "citation_count":4537,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3015883388",
      "ArXiv":"2004.04906",
      "DBLP":"conf\/emnlp\/KarpukhinOMLWEC20",
      "ACL":"2020.emnlp-main.550",
      "DOI":"10.18653\/v1\/2020.emnlp-main.550",
      "CorpusId":215737187
    }
  },
  {
    "paper_id":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
    "title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
    "authors":[
      "Peter Anderson",
      "Xiaodong He",
      "Chris Buehler",
      "Damien Teney",
      "Mark Johnson",
      "Stephen Gould",
      "Lei Zhang"
    ],
    "abstract":"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr \/ SPICE \/ BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
    "venue":"2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition",
    "year":2017,
    "publication_date":"2017-07-25",
    "citation_count":4464,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2745461083",
      "DBLP":"conf\/cvpr\/00010BT0GZ18",
      "ArXiv":"1707.07998",
      "DOI":"10.1109\/CVPR.2018.00636",
      "CorpusId":3753452
    }
  },
  {
    "paper_id":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
    "title":"VQA: Visual Question Answering",
    "authors":[
      "Aishwarya Agrawal",
      "Jiasen Lu",
      "Stanislaw Antol",
      "Margaret Mitchell",
      "C. L. Zitnick",
      "Devi Parikh",
      "Dhruv Batra"
    ],
    "abstract":"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sim $$\\end{document}0.25 M images, ∼\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sim $$\\end{document}0.76 M questions, and ∼\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sim $$\\end{document}10 M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http:\/\/cloudcv.org\/vqa).",
    "venue":"International Journal of Computer Vision",
    "year":2015,
    "publication_date":"2015-05-03",
    "citation_count":5969,
    "reference_count":73,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/AntolALMBZP15",
      "MAG":"1933349210",
      "ArXiv":"1505.00468",
      "DOI":"10.1007\/s11263-016-0966-6",
      "CorpusId":3180429
    }
  },
  {
    "paper_id":"22655979df781d222eaf812b0d325fa9adf11594",
    "title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    "authors":[
      "Zhilin Yang",
      "Peng Qi",
      "Saizheng Zhang",
      "Yoshua Bengio",
      "William W. Cohen",
      "R. Salakhutdinov",
      "Christopher D. Manning"
    ],
    "abstract":"Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-09-25",
    "citation_count":3405,
    "reference_count":20,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"D18-1259",
      "DBLP":"journals\/corr\/abs-1809-09600",
      "MAG":"2952862139",
      "ArXiv":"1809.09600",
      "DOI":"10.18653\/v1\/D18-1259",
      "CorpusId":52822214
    }
  },
  {
    "paper_id":"17dbd7b72029181327732e4d11b52a08ed4630d0",
    "title":"Natural Questions: A Benchmark for Question Answering Research",
    "authors":[
      "T. Kwiatkowski",
      "J. Palomaki",
      "Olivia Redfield",
      "Michael Collins",
      "Ankur P. Parikh",
      "Chris Alberti",
      "D. Epstein",
      "I. Polosukhin",
      "Jacob Devlin",
      "Kenton Lee",
      "Kristina Toutanova",
      "Llion Jones",
      "Matthew Kelcey",
      "Ming-Wei Chang",
      "Andrew M. Dai",
      "Jakob Uszkoreit",
      "Quoc V. Le",
      "Slav Petrov"
    ],
    "abstract":"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long\/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-08-01",
    "citation_count":3927,
    "reference_count":38,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"Q19-1026",
      "MAG":"2912924812",
      "DBLP":"journals\/tacl\/KwiatkowskiPRCP19",
      "DOI":"10.1162\/tacl_a_00276",
      "CorpusId":86611921
    }
  },
  {
    "paper_id":"88bb0a28bb58d847183ec505dda89b63771bb495",
    "title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
    "authors":[
      "Peter Clark",
      "Isaac Cowhey",
      "Oren Etzioni",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Carissa Schoenick",
      "Oyvind Tafjord"
    ],
    "abstract":"We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",
    "venue":"arXiv.org",
    "year":2018,
    "publication_date":"2018-03-14",
    "citation_count":3481,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1803.05457",
      "DBLP":"journals\/corr\/abs-1803-05457",
      "MAG":"2794325560",
      "CorpusId":3922816
    }
  },
  {
    "paper_id":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
    "title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
    "authors":[
      "Yash Goyal",
      "Tejas Khot",
      "D. Summers-Stay",
      "Dhruv Batra",
      "Devi Parikh"
    ],
    "abstract":"The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http:\/\/visualqa.org\/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",
    "venue":"International Journal of Computer Vision",
    "year":2016,
    "publication_date":"2016-12-02",
    "citation_count":3678,
    "reference_count":154,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3016211260",
      "DBLP":"conf\/cvpr\/GoyalKSBP17",
      "ArXiv":"1612.00837",
      "DOI":"10.1007\/s11263-018-1116-0",
      "CorpusId":8081284
    }
  },
  {
    "paper_id":"d3135733aa39dec20ce72aa138589dda27c8406d",
    "title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
    "authors":[
      "Pan Lu",
      "Swaroop Mishra",
      "Tony Xia",
      "Liang Qiu",
      "Kai-Wei Chang",
      "Song-Chun Zhu",
      "Oyvind Tafjord",
      "Peter Clark",
      "A. Kalyan"
    ],
    "abstract":"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https:\/\/scienceqa.github.io.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-09-20",
    "citation_count":1722,
    "reference_count":76,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2209.09513",
      "DBLP":"journals\/corr\/abs-2209-09513",
      "DOI":"10.48550\/arXiv.2209.09513",
      "CorpusId":252383606
    }
  },
  {
    "paper_id":"a7ac99d7cf3f568ab1a741392144b646b856ae0c",
    "title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
    "authors":[
      "Drew A. Hudson",
      "Christopher D. Manning"
    ],
    "abstract":"We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2019,
    "publication_date":"2019-02-25",
    "citation_count":2511,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2950104027",
      "DBLP":"conf\/cvpr\/HudsonM19",
      "DOI":"10.1109\/CVPR.2019.00686",
      "CorpusId":152282269
    }
  },
  {
    "paper_id":"ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb",
    "title":"Towards Expert-Level Medical Question Answering with Large Language Models",
    "authors":[
      "K. Singhal",
      "Tao Tu",
      "Juraj Gottweis",
      "R. Sayres",
      "Ellery Wulczyn",
      "Le Hou",
      "Kevin Clark",
      "S. Pfohl",
      "H. Cole-Lewis",
      "Darlene Neal",
      "Mike Schaekermann",
      "Amy Wang",
      "Mohamed Amin",
      "S. Lachgar",
      "P. A. Mansfield",
      "Sushant Prakash",
      "Bradley Green",
      "Ewa Dominowska",
      "B. A. Y. Arcas",
      "Nenad Tomašev",
      "Yun Liu",
      "Renee C Wong",
      "Christopher Semturs",
      "S. S. Mahdavi",
      "J. Barral",
      "D. Webster",
      "G. Corrado",
      "Yossi Matias",
      "Shekoofeh Azizi",
      "A. Karthikesalingam",
      "Vivek Natarajan"
    ],
    "abstract":"Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-05-16",
    "citation_count":649,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-09617",
      "ArXiv":"2305.09617",
      "DOI":"10.48550\/arXiv.2305.09617",
      "CorpusId":258715226
    }
  },
  {
    "paper_id":"1f02ba1c6fae779ec3d003340e72eaf82351cfb9",
    "title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
    "authors":[
      "Yushi Hu",
      "Benlin Liu",
      "Jungo Kasai",
      "Yizhong Wang",
      "Mari Ostendorf",
      "Ranjay Krishna",
      "Noah A. Smith"
    ],
    "abstract":"Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research. 1",
    "venue":"IEEE International Conference on Computer Vision",
    "year":2023,
    "publication_date":"2023-03-21",
    "citation_count":316,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iccv\/HuLKWOKS23",
      "ArXiv":"2303.11897",
      "DOI":"10.1109\/ICCV51070.2023.01866",
      "CorpusId":257636562
    }
  },
  {
    "paper_id":"b611c501269224702d1a9942c8600a31ec66ab28",
    "title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
    "authors":[
      "Ahmed Masry",
      "Do Xuan Long",
      "J. Tan",
      "Shafiq R. Joty",
      "Enamul Hoque"
    ],
    "abstract":"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
    "venue":"Findings",
    "year":2022,
    "publication_date":"2022-03-19",
    "citation_count":1009,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2203.10244",
      "DBLP":"conf\/acl\/MasryLTJH22",
      "ACL":"2022.findings-acl.177",
      "DOI":"10.48550\/arXiv.2203.10244",
      "CorpusId":247593713
    }
  },
  {
    "paper_id":"3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
    "title":"DriveLM: Driving with Graph Visual Question Answering",
    "authors":[
      "Chonghao Sima",
      "Katrin Renz",
      "Kashyap Chitta",
      "Li Chen",
      "Hanxue Zhang",
      "Chengen Xie",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li"
    ],
    "abstract":"We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.",
    "venue":"European Conference on Computer Vision",
    "year":2023,
    "publication_date":"2023-12-21",
    "citation_count":303,
    "reference_count":101,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2312.14150",
      "DBLP":"conf\/eccv\/SimaRCCZXBLGL24",
      "DOI":"10.48550\/arXiv.2312.14150",
      "CorpusId":266435584
    }
  },
  {
    "paper_id":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4",
    "title":"Self-Chained Image-Language Model for Video Localization and Question Answering",
    "authors":[
      "Shoubin Yu",
      "Jaemin Cho",
      "Prateek Yadav",
      "Mohit Bansal"
    ],
    "abstract":"Recent studies have shown promising results on utilizing large pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and Answerer, where both are parameter-efficiently fine-tuned from BLIP-2. We propose two ways of chaining these modules for cascaded inference and self-refinement. First, in the forward chain, the Localizer finds multiple language-aware keyframes in a video, which the Answerer uses to predict the answer. Second, in the reverse chain, the Answerer generates keyframe pseudo-labels to refine the Localizer, alleviating the need for expensive video moment localization annotations. Our SeViLA framework outperforms several strong baselines on 5 challenging video QA and event prediction benchmarks, and achieves the state-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We also analyze the impact of Localizer, comparisons of Localizer with other temporal localization models, pre-training\/self-refinement of Localizer, and varying the number of keyframes.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-11",
    "citation_count":192,
    "reference_count":96,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.06988",
      "DBLP":"journals\/corr\/abs-2305-06988",
      "DOI":"10.48550\/arXiv.2305.06988",
      "CorpusId":258615748
    }
  },
  {
    "paper_id":"ea8c46e193d5121e440daf96edfd15a47151c293",
    "title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    "authors":[
      "Gautier Izacard",
      "Edouard Grave"
    ],
    "abstract":"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
    "venue":"Conference of the European Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-07-02",
    "citation_count":1379,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.eacl-main.74",
      "ArXiv":"2007.01282",
      "MAG":"3039017601",
      "DBLP":"conf\/eacl\/IzacardG21",
      "DOI":"10.18653\/v1\/2021.eacl-main.74",
      "CorpusId":220302360
    }
  },
  {
    "paper_id":"2f3efe44083af91cef562c1a3451eee2f8601d22",
    "title":"WebGPT: Browser-assisted question-answering with human feedback",
    "authors":[
      "Reiichiro Nakano",
      "Jacob Hilton",
      "S. Balaji",
      "Jeff Wu",
      "Ouyang Long",
      "Christina Kim",
      "Christopher Hesse",
      "Shantanu Jain",
      "Vineet Kosaraju",
      "W. Saunders",
      "Xu Jiang",
      "K. Cobbe",
      "Tyna Eloundou",
      "Gretchen Krueger",
      "Kevin Button",
      "Matthew Knight",
      "Benjamin Chess",
      "John Schulman"
    ],
    "abstract":"We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-12-17",
    "citation_count":1541,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2112-09332",
      "ArXiv":"2112.09332",
      "CorpusId":245329531
    }
  },
  {
    "paper_id":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
    "title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
    "authors":[
      "Alon Talmor",
      "Jonathan Herzig",
      "Nicholas Lourie",
      "Jonathan Berant"
    ],
    "abstract":"When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":null,
    "citation_count":2048,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2952331680",
      "ACL":"N19-1421",
      "DBLP":"journals\/corr\/abs-1811-00937",
      "ArXiv":"1811.00937",
      "DOI":"10.18653\/v1\/N19-1421",
      "CorpusId":53296520
    }
  },
  {
    "paper_id":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",
    "title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
    "authors":[
      "Xiaoman Zhang",
      "Chaoyi Wu",
      "Ziheng Zhao",
      "Weixiong Lin",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ],
    "abstract":"Medical Visual Question Answering (MedVQA) presents a significant opportunity to enhance diagnostic accuracy and healthcare delivery by leveraging artificial intelligence to interpret and answer questions based on medical images. In this study, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction and propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. We establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. We train the proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD, SLAKE, and Image-Clef-2019, significantly outperforming existing MedVQA models in generating relevant, accurate free-form answers. In addition, we propose a test set that has undergone manual verification, which is significantly more challenging, serving to better monitor the development of generative MedVQA methods. To facilitate comprehensive evaluation and comparison, we have maintained a leaderboard at https:\/\/paperswithcode.com\/paper\/pmc-vqa-visual-instruction-tuning-for-medical, offering a centralized resource for tracking progress and benchmarking state-of-the-art approaches. The PMC-VQA dataset emerges as a vital resource for the field of research, and the MedVInT presents a significant breakthrough in the area of MedVQA.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-05-17",
    "citation_count":247,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.10415",
      "DBLP":"journals\/corr\/abs-2305-10415",
      "DOI":"10.48550\/arXiv.2305.10415",
      "CorpusId":258741360
    }
  },
  {
    "paper_id":"a52dd1e900200e0733eea927edc7d6c27aeba187",
    "title":"TheoremQA: A Theorem-driven Question Answering dataset",
    "authors":[
      "Wenhu Chen",
      "Ming Yin",
      "Max W.F. Ku",
      "Yixin Wan",
      "Xueguang Ma",
      "Jianyu Xu",
      "Tony Xia",
      "Xinyi Wang",
      "Pan Lu"
    ],
    "abstract":"The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems. The data and code are released in https:\/\/github.com\/wenhuchen\/TheoremQA.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-21",
    "citation_count":176,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-12524",
      "ArXiv":"2305.12524",
      "DOI":"10.48550\/arXiv.2305.12524",
      "CorpusId":258833200
    }
  },
  {
    "paper_id":"346081161bdc8f18e2a4c4af7f51d35452b5cb01",
    "title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
    "authors":[
      "Mor Geva",
      "Daniel Khashabi",
      "Elad Segal",
      "Tushar Khot",
      "D. Roth",
      "Jonathan Berant"
    ],
    "abstract":"Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-01-06",
    "citation_count":884,
    "reference_count":31,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2101.02235",
      "DBLP":"journals\/tacl\/GevaKSKRB21",
      "DOI":"10.1162\/tacl_a_00370",
      "CorpusId":230799347
    }
  },
  {
    "paper_id":"47a67e76ed84260ff19f7a948d764005d1edf1c9",
    "title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
    "authors":[
      "Dustin Schwenk",
      "Apoorv Khandelwal",
      "Christopher Clark",
      "Kenneth Marino",
      "Roozbeh Mottaghi"
    ],
    "abstract":"The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models. Project page: http:\/\/a-okvqa.allenai.org\/",
    "venue":"European Conference on Computer Vision",
    "year":2022,
    "publication_date":"2022-06-03",
    "citation_count":700,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-01718",
      "ArXiv":"2206.01718",
      "DOI":"10.48550\/arXiv.2206.01718",
      "CorpusId":249375629
    }
  },
  {
    "paper_id":"c12b80b44d9acfe6cd92fdf965264c4b706c367c",
    "title":"ToolQA: A Dataset for LLM Question Answering with External Tools",
    "authors":[
      "Yuchen Zhuang",
      "Yue Yu",
      "Kuan Wang",
      "Haotian Sun",
      "Chao Zhang"
    ],
    "abstract":"Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-06-23",
    "citation_count":323,
    "reference_count":103,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/ZhuangYWSZ23",
      "ArXiv":"2306.13304",
      "DOI":"10.48550\/arXiv.2306.13304",
      "CorpusId":259243960
    }
  },
  {
    "paper_id":"ee156428803c5bd6e7372f6b27d74bcf88390db3",
    "title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario",
    "authors":[
      "Tianwen Qian",
      "Jingjing Chen",
      "Linhai Zhuo",
      "Yang Jiao",
      "Yu-Gang Jiang"
    ],
    "abstract":"We introduce a novel visual question answering (VQA) task in the context of autonomous driving, aiming to answer natural language questions based on street-view clues. Compared to traditional VQA tasks, VQA in autonomous driving scenario presents more challenges. Firstly, the raw visual data are multi-modal, including images and point clouds captured by camera and LiDAR, respectively. Secondly, the data are multi-frame due to the continuous, real-time acquisition. Thirdly, the outdoor scenes exhibit both moving foreground and static background. Existing VQA benchmarks fail to adequately address these complexities. To bridge this gap, we propose NuScenes-QA, the first benchmark for VQA in the autonomous driving scenario, encompassing 34K visual scenes and 460K question-answer pairs. Specifically, we leverage existing 3D detection annotations to generate scene graphs and design question templates manually. Subsequently, the question-answer pairs are generated programmatically based on these templates. Comprehensive statistics prove that our NuScenes-QA is a balanced large-scale benchmark with diverse question formats. Built upon it, we develop a series of baselines that employ advanced 3D detection and VQA techniques. Our extensive experiments highlight the challenges posed by this new task. Codes and dataset are available at https:\/\/github.com\/qiantianwen\/NuScenes-QA.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2023,
    "publication_date":"2023-05-24",
    "citation_count":229,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.14836",
      "DBLP":"conf\/aaai\/QianCZJJ24",
      "DOI":"10.48550\/arXiv.2305.14836",
      "CorpusId":258866014
    }
  },
  {
    "paper_id":"629f44f5fb78ec390ef66633dc627f1d04f3eb85",
    "title":"Knowledge Graph Prompting for Multi-Document Question Answering",
    "authors":[
      "Yu Wang",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Alexa F. Siu",
      "Ruiyi Zhang",
      "Tyler Derr"
    ],
    "abstract":"The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages\/tables), and edges denoting the semantic\/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https:\/\/github.com\/YuWVandy\/KG-LLM-MDQA.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2023,
    "publication_date":"2023-08-22",
    "citation_count":208,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/0160LRSZD24",
      "ArXiv":"2308.11730",
      "DOI":"10.48550\/arXiv.2308.11730",
      "CorpusId":261076072
    }
  },
  {
    "paper_id":"741776172685b9717159a9fcd21841461bb33b14",
    "title":"MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering",
    "authors":[
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ],
    "abstract":"This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \\&NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \\&topics. A detailed explanation of the solution, along with the above information, is provided in this study.",
    "venue":"ACM Conference on Health, Inference, and Learning",
    "year":2022,
    "publication_date":"2022-03-27",
    "citation_count":469,
    "reference_count":29,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2203-14371",
      "ArXiv":"2203.14371",
      "DOI":"10.48550\/arXiv.2203.14371",
      "CorpusId":247763070
    }
  },
  {
    "paper_id":"a970c8fadef8497576660b288c52c0ec8eebdc12",
    "title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
    "authors":[
      "Antoine Yang",
      "Antoine Miech",
      "Josef Sivic",
      "I. Laptev",
      "C. Schmid"
    ],
    "abstract":"Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https:\/\/github.com\/antoyang\/FrozenBiLM.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-06-16",
    "citation_count":271,
    "reference_count":140,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2206.08155",
      "DBLP":"conf\/nips\/YangMSLS22",
      "DOI":"10.48550\/arXiv.2206.08155",
      "CorpusId":249712016
    }
  },
  {
    "paper_id":"c1a4fb211cf995b1af1247a152fcc145594ec13b",
    "title":"SQA3D: Situated Question Answering in 3D Scenes",
    "authors":[
      "Xiaojian Ma",
      "Silong Yong",
      "Zilong Zheng",
      "Qing Li",
      "Yitao Liang",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ],
    "abstract":"We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-10-14",
    "citation_count":214,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2210.07474",
      "DBLP":"journals\/corr\/abs-2210-07474",
      "DOI":"10.48550\/arXiv.2210.07474",
      "CorpusId":252907411
    }
  },
  {
    "paper_id":"4ab41d9780f1d1ac34d39fa7e527e73652507fcc",
    "title":"GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
    "authors":[
      "Xikun Zhang",
      "Antoine Bosselut",
      "Michihiro Yasunaga",
      "Hongyu Ren",
      "Percy Liang",
      "Christopher D. Manning",
      "J. Leskovec"
    ],
    "abstract":"Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-01-21",
    "citation_count":254,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2201-08860",
      "ArXiv":"2201.08860",
      "CorpusId":246240437
    }
  },
  {
    "paper_id":"fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7",
    "title":"What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams",
    "authors":[
      "Di Jin",
      "Eileen Pan",
      "Nassim Oufattole",
      "W. Weng",
      "Hanyi Fang",
      "Peter Szolovits"
    ],
    "abstract":"Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7%, 42.0%, and 70.1% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.",
    "venue":"Applied Sciences",
    "year":2020,
    "publication_date":"2020-09-28",
    "citation_count":1126,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3088056511",
      "DBLP":"journals\/corr\/abs-2009-13081",
      "ArXiv":"2009.13081",
      "DOI":"10.20944\/PREPRINTS202105.0498.V1",
      "CorpusId":221970190
    }
  },
  {
    "paper_id":"c8a145ecdf84015a8a38ef21c3baa954fa84368e",
    "title":"The multi-modal fusion in visual question answering: a review of attention mechanisms",
    "authors":[
      "Siyu Lu",
      "Mingzhe Liu",
      "Lirong Yin",
      "Zhengtong Yin",
      "Xuan Liu",
      "Wenfeng Zheng"
    ],
    "abstract":"Visual Question Answering (VQA) is a significant cross-disciplinary issue in the fields of computer vision and natural language processing that requires a computer to output a natural language answer based on pictures and questions posed based on the pictures. This requires simultaneous processing of multimodal fusion of text features and visual features, and the key task that can ensure its success is the attention mechanism. Bringing in attention mechanisms makes it better to integrate text features and image features into a compact multi-modal representation. Therefore, it is necessary to clarify the development status of attention mechanism, understand the most advanced attention mechanism methods, and look forward to its future development direction. In this article, we first conduct a bibliometric analysis of the correlation through CiteSpace, then we find and reasonably speculate that the attention mechanism has great development potential in cross-modal retrieval. Secondly, we discuss the classification and application of existing attention mechanisms in VQA tasks, analysis their shortcomings, and summarize current improvement methods. Finally, through the continuous exploration of attention mechanisms, we believe that VQA will evolve in a smarter and more human direction.",
    "venue":"PeerJ Computer Science",
    "year":2023,
    "publication_date":"2023-05-30",
    "citation_count":255,
    "reference_count":112,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"10280591",
      "DBLP":"journals\/peerj-cs\/LuLYYLZ23",
      "DOI":"10.7717\/peerj-cs.1400",
      "CorpusId":259209374,
      "PubMed":"37346665"
    }
  },
  {
    "paper_id":"1b937ff4b05e2b56c2c2fcdfa5baa3085cd5a08c",
    "title":"NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions",
    "authors":[
      "Junbin Xiao",
      "Xindi Shang",
      "Angela Yao",
      "Tat-seng Chua"
    ],
    "abstract":"We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial description towards a deeper understanding of videos. (The dataset and related resources are available at https:\/\/github.com\/doc-doc\/NExT-QA.git).",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2021,
    "publication_date":"2021-05-18",
    "citation_count":672,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/XiaoSYC21",
      "ArXiv":"2105.08276",
      "DOI":"10.1109\/CVPR46437.2021.00965",
      "CorpusId":234763093
    }
  },
  {
    "paper_id":"7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
    "title":"BBQ: A hand-built bias benchmark for question answering",
    "authors":[
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Sam Bowman"
    ],
    "abstract":"It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
    "venue":"Findings",
    "year":2021,
    "publication_date":"2021-10-15",
    "citation_count":568,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2110-08193",
      "ArXiv":"2110.08193",
      "ACL":"2022.findings-acl.165",
      "DOI":"10.18653\/v1\/2022.findings-acl.165",
      "CorpusId":239010011
    }
  },
  {
    "paper_id":"0899c8fecd47b2843db8dc28e53ee06c3115fb3d",
    "title":"A Critical Evaluation of Evaluations for Long-form Question Answering",
    "authors":[
      "Fangyuan Xu",
      "Yixiao Song",
      "Mohit Iyyer",
      "Eunsol Choi"
    ],
    "abstract":"Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts’ evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single “overall score” of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-05-29",
    "citation_count":126,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2023.acl-long.181",
      "DBLP":"journals\/corr\/abs-2305-18201",
      "ArXiv":"2305.18201",
      "DOI":"10.48550\/arXiv.2305.18201",
      "CorpusId":258960565
    }
  },
  {
    "paper_id":"89ed7fd00319d45269906a9b05e10c8680bf9cec",
    "title":"FinanceBench: A New Benchmark for Financial Question Answering",
    "authors":[
      "Pranab Islam",
      "Anand Kannappan",
      "Douwe Kiela",
      "Rebecca Qian",
      "Nino Scherrer",
      "Bertie Vidgen"
    ],
    "abstract":"FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-11-20",
    "citation_count":133,
    "reference_count":38,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2311-11944",
      "ArXiv":"2311.11944",
      "DOI":"10.48550\/arXiv.2311.11944",
      "CorpusId":265294665
    }
  },
  {
    "paper_id":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6",
    "title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering",
    "authors":[
      "Bo Liu",
      "Li-Ming Zhan",
      "Li Xu",
      "Lin Ma",
      "Y. Yang",
      "Xiao-Ming Wu"
    ],
    "abstract":"Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http:\/\/www.med-vqa.com\/slake.",
    "venue":"IEEE International Symposium on Biomedical Imaging",
    "year":2021,
    "publication_date":"2021-02-18",
    "citation_count":400,
    "reference_count":15,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2102-09542",
      "ArXiv":"2102.09542",
      "DOI":"10.1109\/ISBI48211.2021.9434010",
      "CorpusId":231951663
    }
  },
  {
    "paper_id":"b3213c84a6ff7a2f11099de783c93166e4fc02a4",
    "title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    "authors":[
      "Fengbin Zhu",
      "Wenqiang Lei",
      "Youcheng Huang",
      "Chao Wang",
      "Shuo Zhang",
      "Jiancheng Lv",
      "Fuli Feng",
      "Tat-seng Chua"
    ],
    "abstract":"Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison\/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-05-17",
    "citation_count":392,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2105.07624",
      "ACL":"2021.acl-long.254",
      "DBLP":"conf\/acl\/ZhuLHWZLFC20",
      "DOI":"10.18653\/v1\/2021.acl-long.254",
      "CorpusId":234741852
    }
  },
  {
    "paper_id":"8f6c652a392995bd047a2f7b94474ab1e6e23ff0",
    "title":"ScanQA: 3D Question Answering for Spatial Scene Understanding",
    "authors":[
      "Daich Azuma",
      "Taiki Miyanishi",
      "Shuhei Kurita",
      "M. Kawanabe"
    ],
    "abstract":"We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https:\/\/github.com\/ATR-DBI\/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2021,
    "publication_date":"2021-12-20",
    "citation_count":281,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2112.10482",
      "DBLP":"journals\/corr\/abs-2112-10482",
      "DOI":"10.1109\/CVPR52688.2022.01854",
      "CorpusId":245334889
    }
  },
  {
    "paper_id":"28ad018c39d1578bea84e7cedf94459e3dbe1e70",
    "title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
    "authors":[
      "Kenneth Marino",
      "Mohammad Rastegari",
      "Ali Farhadi",
      "Roozbeh Mottaghi"
    ],
    "abstract":"Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2019,
    "publication_date":"2019-05-31",
    "citation_count":1308,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/MarinoRFM19",
      "ArXiv":"1906.00067",
      "MAG":"2947312908",
      "DOI":"10.1109\/CVPR.2019.00331",
      "CorpusId":173991173
    }
  },
  {
    "paper_id":"1536e8958697c5364f68b2e2448905dbbeb3a0ca",
    "title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    "authors":[
      "Todor Mihaylov",
      "Peter Clark",
      "Tushar Khot",
      "Ashish Sabharwal"
    ],
    "abstract":"We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-08-13",
    "citation_count":1927,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1809-02789",
      "MAG":"2952396187",
      "ACL":"D18-1260",
      "ArXiv":"1809.02789",
      "DOI":"10.18653\/v1\/D18-1260",
      "CorpusId":52183757
    }
  },
  {
    "paper_id":"3950df97ea527009a32569cb7016bc3df1383dca",
    "title":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
    "authors":[
      "Michihiro Yasunaga",
      "Hongyu Ren",
      "Antoine Bosselut",
      "Percy Liang",
      "J. Leskovec"
    ],
    "abstract":"The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-13",
    "citation_count":685,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.naacl-main.45",
      "ArXiv":"2104.06378",
      "MAG":"3172335055",
      "DBLP":"journals\/corr\/abs-2104-06378",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.45",
      "CorpusId":233219869
    }
  },
  {
    "paper_id":"f36c96827e629c86762bb96db70fab8f2660fc76",
    "title":"Can I Trust Your Answer? Visually Grounded Video Question Answering",
    "authors":[
      "Junbin Xiao",
      "Angela Yao",
      "Yicong Li",
      "Tat-Seng Chua"
    ],
    "abstract":"We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA - an extension of NExT-QA with 10.5K temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a grounded-QA method via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both grounding and QA. With these efforts, we aim to push towards trustworthy VLMs in VQA systems. Our dataset and code are available at https:\/\/github.com\/doc-doc\/NExT-GQA.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2023,
    "publication_date":"2023-09-04",
    "citation_count":96,
    "reference_count":79,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/XiaoY0C24",
      "ArXiv":"2309.01327",
      "DOI":"10.1109\/CVPR52733.2024.01254",
      "CorpusId":261531601
    }
  },
  {
    "paper_id":"238deab37e201c57505a4a47bb854e462af79bd7",
    "title":"Entity-Based Knowledge Conflicts in Question Answering",
    "authors":[
      "S. Longpre",
      "Kartik Perisetla",
      "Anthony Chen",
      "Nikhil Ramesh",
      "Chris DuBois",
      "Sameer Singh"
    ],
    "abstract":"Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4% - 7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-09-10",
    "citation_count":303,
    "reference_count":37,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/LongprePCRD021",
      "ACL":"2021.emnlp-main.565",
      "ArXiv":"2109.05052",
      "DOI":"10.18653\/v1\/2021.emnlp-main.565",
      "CorpusId":237491581
    }
  },
  {
    "paper_id":"990a7b4eceedb6e053e6386269481bdfc42a1094",
    "title":"CoQA: A Conversational Question Answering Challenge",
    "authors":[
      "Siva Reddy",
      "Danqi Chen",
      "Christopher D. Manning"
    ],
    "abstract":"Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https:\/\/stanfordnlp.github.io\/coqa.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2018,
    "publication_date":"2018-08-21",
    "citation_count":1296,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2888296173",
      "DBLP":"journals\/tacl\/ReddyCM19",
      "ACL":"Q19-1016",
      "ArXiv":"1808.07042",
      "DOI":"10.1162\/tacl_a_00266",
      "CorpusId":52055325
    }
  },
  {
    "paper_id":"291133a657498920451481d3bf784ebbafda8d6e",
    "title":"GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning",
    "authors":[
      "Jiaqi Chen",
      "Jianheng Tang",
      "Jinghui Qin",
      "Xiaodan Liang",
      "Lingbo Liu",
      "E. Xing",
      "Liang Lin"
    ],
    "abstract":"Automatic math problem solving has recently attracted increasing attention as a long-standing AI benchmark. In this paper, we focus on solving geometric problems, which requires a comprehensive understanding of textual descriptions, visual diagrams, and theorem knowledge. However, the existing methods were highly dependent on handcraft rules and were merely evaluated on small-scale datasets. Therefore, we propose a Geometric Question Answering dataset GeoQA, containing 4,998 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems. Compared with another publicly available dataset GeoS, GeoQA is 25 times larger, in which the program annotations can provide a practical testbed for future research on explicit and explainable numerical reasoning. Moreover, we introduce a Neural Geometric Solver (NGS) to address geometric problems by comprehensively parsing multimodal information and generating interpretable programs. We further add multiple self-supervised auxiliary tasks on NGS to enhance cross-modal semantic representation. Extensive experiments on GeoQA validate the effectiveness of our proposed NGS and auxiliary tasks. However, the results are still significantly lower than human performance, which leaves large room for future research. Our benchmark and code are released at https:\/\/github.com\/chen-judge\/GeoQA .",
    "venue":"Findings",
    "year":2021,
    "publication_date":"2021-05-30",
    "citation_count":237,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.findings-acl.46",
      "ArXiv":"2105.14517",
      "DBLP":"conf\/acl\/ChenTQLLXL21",
      "DOI":"10.18653\/v1\/2021.findings-acl.46",
      "CorpusId":235253782
    }
  },
  {
    "paper_id":"f2c1ab4e850ad89bd6b6b9908e12a4151d152070",
    "title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
    "authors":[
      "Yingqi Qu",
      "Yuchen Ding",
      "Jing Liu",
      "Kai Liu",
      "Ruiyang Ren",
      "Xin Zhao",
      "Daxiang Dong",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract":"In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-16",
    "citation_count":663,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3092683697",
      "DBLP":"journals\/corr\/abs-2010-08191",
      "ArXiv":"2010.08191",
      "ACL":"2021.naacl-main.466",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.466",
      "CorpusId":231815627
    }
  },
  {
    "paper_id":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6",
    "title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    "authors":[
      "J. Clark",
      "Eunsol Choi",
      "Michael Collins",
      "Dan Garrette",
      "T. Kwiatkowski",
      "Vitaly Nikolaev",
      "J. Palomaki"
    ],
    "abstract":"Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-03-10",
    "citation_count":678,
    "reference_count":84,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2003.05002",
      "MAG":"3045462440",
      "DBLP":"journals\/tacl\/ClarkPNCGCK20",
      "DOI":"10.1162\/tacl_a_00317",
      "CorpusId":212657414
    }
  },
  {
    "paper_id":"b34c0b0169925a6c7f14e3de9764ed9505f30de3",
    "title":"Toward expert-level medical question answering with large language models",
    "authors":[
      "Karan Singhal",
      "Tao Tu",
      "Juraj Gottweis",
      "R. Sayres",
      "Ellery Wulczyn",
      "Mohamed Amin",
      "Le Hou",
      "Kevin Clark",
      "Stephen R. Pfohl",
      "Heather Cole-Lewis",
      "Darlene Neal",
      "Q. Rashid",
      "Mike Schaekermann",
      "Amy Wang",
      "Dev Dash",
      "Jonathan H. Chen",
      "Nigam H. Shah",
      "Sami Lachgar",
      "P. Mansfield",
      "Sushant Prakash",
      "Bradley Green",
      "Ewa Dominowska",
      "Blaise Agüera y Arcas",
      "Nenad Tomašev",
      "Yun Liu",
      "Renee Wong",
      "Christopher Semturs",
      "S. Mahdavi",
      "Joelle K. Barral",
      "Dale R. Webster",
      "G. Corrado",
      "Yossi Matias",
      "Shekoofeh Azizi",
      "A. Karthikesalingam",
      "Vivek Natarajan"
    ],
    "abstract":"Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P < 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications. With an improved framework for model development and evaluation, a large language model is shown to provide answers to medical questions that are comparable or preferred with respect to those provided by human physicians.",
    "venue":"Nature Network Boston",
    "year":2025,
    "publication_date":"2025-01-08",
    "citation_count":407,
    "reference_count":58,
    "fields_of_study":[
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"11922739",
      "DOI":"10.1038\/s41591-024-03423-7",
      "CorpusId":275427710,
      "PubMed":"39779926"
    }
  },
  {
    "paper_id":"a942cc84412d6a52adab325909b52296c017df48",
    "title":"Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
    "authors":[
      "Jian Liu",
      "Leyang Cui",
      "Hanmeng Liu",
      "Dandan Huang",
      "Yile Wang",
      "Yue Zhang"
    ],
    "abstract":"A compelling approach to complex question answering is to convert the question to a sequence of actions, which can then be executed on the knowledge base to yield the answer, aka the programmer-interpreter approach. Use similar training questions to the test question, meta-learning enables the programmer to adapt to unseen questions to tackle potential distributional biases quickly. However, this comes at the cost of manually labeling similar questions to learn a retrieval model, which is tedious and expensive. In this paper, we present a novel method that automatically learns a retrieval model alternately with the programmer from weak supervision, i.e., the system’s performance with respect to the produced answers. To the best of our knowledge, this is the first attempt to train the retrieval model with the programmer jointly. Our system leads to state-of-the-art performance on a large-scale task for complex question answering over knowledge bases. We have released our code at https:\/\/github.com\/DevinJake\/MARL.",
    "venue":"International Joint Conference on Artificial Intelligence",
    "year":2020,
    "publication_date":"2020-07-01",
    "citation_count":442,
    "reference_count":26,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3096835037",
      "DBLP":"journals\/corr\/abs-2010-15875",
      "ArXiv":"2010.15875",
      "DOI":"10.24963\/ijcai.2020\/505",
      "CorpusId":220483148
    }
  },
  {
    "paper_id":"27db72a2f643f9dfebc0cc2e8b98a9db307f0f07",
    "title":"HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
    "authors":[
      "Wenhu Chen",
      "Hanwen Zha",
      "Zhiyu Chen",
      "Wenhan Xiong",
      "Hong Wang",
      "W. Wang"
    ],
    "abstract":"Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB\/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20%, while the hybrid model can achieve an EM over 40%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model’s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-04-15",
    "citation_count":350,
    "reference_count":28,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3101082165",
      "DBLP":"conf\/emnlp\/ChenZCXWW20",
      "ACL":"2020.findings-emnlp.91",
      "ArXiv":"2004.07347",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.91",
      "CorpusId":215785913
    }
  },
  {
    "paper_id":"0c06bd20403a204bcd95dae1f176c10894fe7139",
    "title":"Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
    "authors":[
      "Apoorv Saxena",
      "Aditay Tripathi",
      "P. Talukdar"
    ],
    "abstract":"Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-07-01",
    "citation_count":563,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3034862985",
      "ACL":"2020.acl-main.412",
      "DBLP":"conf\/acl\/SaxenaTT20",
      "DOI":"10.18653\/v1\/2020.acl-main.412",
      "CorpusId":220047862
    }
  },
  {
    "paper_id":"11b6d1fee0f47a8f9f892ab0d86f370c449097aa",
    "title":"FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
    "authors":[
      "Esin Durmus",
      "He He",
      "Mona T. Diab"
    ],
    "abstract":"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-05-07",
    "citation_count":427,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3099766584",
      "DBLP":"journals\/corr\/abs-2005-03754",
      "ACL":"2020.acl-main.454",
      "ArXiv":"2005.03754",
      "DOI":"10.18653\/V1\/2020.ACL-MAIN.454",
      "CorpusId":218571335
    }
  },
  {
    "paper_id":"ff79d5d1282aea51195afcb7de898dca2c879a97",
    "title":"The Process of Question Answering",
    "authors":[
      "W. Lehnert"
    ],
    "abstract":"Abstract : Problems in computational question answering assume a new perspective when question answering is viewed as a problem in natural language processing. A theory of question answering has been proposed which relies on ideas in conceptual information processing and theories of human memory organization. This theory of question answering has been implemented in a computer program, QUALM, currently being used by two story understanding systems to complete a natural language processing system which reads stories and answers questions about what was read. The processes in QUALM are divided into 4 phases: (1) Conceptual categorization which guides subsequent processing by dictating which specific inference mechanisms and memory retrieval strategies should be invoked in the course of answering a question; (2) Inferential analysis which is responsible for understanding what the questioner really meant when a question should not be taken literally; (3) Content specification which determines how much of an answer should be returned in terms of detail and elaborations, and (4) Retrieval heuristics which do the actual digging to extract an answer from memory.",
    "venue":"",
    "year":2022,
    "publication_date":"2022-08-02",
    "citation_count":412,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"1495683013",
      "DOI":"10.4324\/9781003316817",
      "CorpusId":57370597
    }
  },
  {
    "paper_id":"0c3c4c88c7b07596221ac640c7b7102686e3eae3",
    "title":"PubMedQA: A Dataset for Biomedical Research Question Answering",
    "authors":[
      "Qiao Jin",
      "Bhuwan Dhingra",
      "Zhengping Liu",
      "William W. Cohen",
      "Xinghua Lu"
    ],
    "abstract":"We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes\/no\/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes\/no\/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https:\/\/pubmedqa.github.io.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-09-01",
    "citation_count":1185,
    "reference_count":23,
    "fields_of_study":[
      "Computer Science",
      "Biology"
    ],
    "external_ids":{
      "MAG":"2972522091",
      "DBLP":"conf\/emnlp\/JinDLCL19",
      "ArXiv":"1909.06146",
      "ACL":"D19-1259",
      "DOI":"10.18653\/v1\/D19-1259",
      "CorpusId":202572622
    }
  },
  {
    "paper_id":"ed38c6b157c11476939c426ec6871c926f2f3524",
    "title":"Leveraging Large Language Models for Multiple Choice Question Answering",
    "authors":[
      "Joshua Robinson",
      "Christopher Rytting",
      "D. Wingate"
    ],
    "abstract":"While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,\"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-10-22",
    "citation_count":230,
    "reference_count":74,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2210.12353",
      "DBLP":"journals\/corr\/abs-2210-12353",
      "DOI":"10.48550\/arXiv.2210.12353",
      "CorpusId":253098700
    }
  },
  {
    "paper_id":"fc0b46a0f3720e6c29c1a913aaa3de4a0699f713",
    "title":"PathVQA: 30000+ Questions for Medical Visual Question Answering",
    "authors":[
      "Xuehai He",
      "Yichen Zhang",
      "Luntian Mou",
      "E. Xing",
      "Pengtao Xie"
    ],
    "abstract":"Is it possible to develop an \"AI Pathologist\" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-03-07",
    "citation_count":357,
    "reference_count":34,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2003.10286",
      "MAG":"3012608737",
      "DBLP":"journals\/corr\/abs-2003-10286",
      "CorpusId":214612106
    }
  },
  {
    "paper_id":"dc08d90005b12f66a12798fd79959a8f7f8c4885",
    "title":"Counterfactual Samples Synthesizing for Robust Visual Question Answering",
    "authors":[
      "Long Chen",
      "Xin Yan",
      "Jun Xiao",
      "Hanwang Zhang",
      "Shiliang Pu",
      "Yueting Zhuang"
    ],
    "abstract":"Despite Visual Question Answering (VQA) has realized impressive progress over the last few years, today's VQA models tend to capture superficial linguistic correlations in the train set and fail to generalize to the test set with different QA distributions. To reduce the language biases, several recent works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on VQA-CP. However, since the complexity of design, current methods are unable to equip the ensemble-based models with two indispensable characteristics of an ideal VQA model: 1) visual-explainable: the model should rely on the right visual regions when making decisions. 2) question-sensitive: the model should be sensitive to the linguistic variations in question. To this end, we propose a model-agnostic Counterfactual Samples Synthesizing (CSS) training scheme. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (ie, the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. In return, the performance of these models is further boosted. Extensive ablations have shown the effectiveness of CSS. Particularly, by building on top of the model LMH, we achieve a record-breaking performance of 58.95% on VQA-CP v2, with 6.5% gains.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2020,
    "publication_date":"2020-03-14",
    "citation_count":316,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2003.06576",
      "DBLP":"journals\/corr\/abs-2003-06576",
      "MAG":"3035517717",
      "DOI":"10.1109\/cvpr42600.2020.01081",
      "CorpusId":212725353
    }
  },
  {
    "paper_id":"c3afcd7e57c3e04b03b0b5001a3854482fa39441",
    "title":"In Defense of Grid Features for Visual Question Answering",
    "authors":[
      "Huaizu Jiang",
      "Ishan Misra",
      "Marcus Rohrbach",
      "E. Learned-Miller",
      "Xinlei Chen"
    ],
    "abstract":"Popularized as `bottom-up' attention, bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (e.g. better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well -- running more than an order of magnitude faster with the same accuracy (e.g. if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2020,
    "publication_date":"2020-01-10",
    "citation_count":349,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2001-03615",
      "MAG":"3000433013",
      "ArXiv":"2001.03615",
      "DOI":"10.1109\/cvpr42600.2020.01028",
      "CorpusId":210156985
    }
  },
  {
    "paper_id":"2c1890864c1c2b750f48316dc8b650ba4772adc5",
    "title":"Stacked Attention Networks for Image Question Answering",
    "authors":[
      "Zichao Yang",
      "Xiaodong He",
      "Jianfeng Gao",
      "L. Deng",
      "Alex Smola"
    ],
    "abstract":"This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2015,
    "publication_date":"2015-11-07",
    "citation_count":1958,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "MAG":"2950318722",
      "DBLP":"conf\/cvpr\/YangHGDS16",
      "ArXiv":"1511.02274",
      "DOI":"10.1109\/CVPR.2016.10",
      "CorpusId":8849206
    }
  },
  {
    "paper_id":"2b4bc49a3b23229a060609380752666b24b435fb",
    "title":"Distilling Knowledge from Reader to Retriever for Question Answering",
    "authors":[
      "Gautier Izacard",
      "Edouard Grave"
    ],
    "abstract":"The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.",
    "venue":"International Conference on Learning Representations",
    "year":2020,
    "publication_date":"2020-12-08",
    "citation_count":294,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/IzacardG21",
      "ArXiv":"2012.04584",
      "MAG":"3111425437",
      "CorpusId":227746078
    }
  },
  {
    "paper_id":"12f7de07f9b00315418e381b2bd797d21f12b419",
    "title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
    "authors":[
      "Akira Fukui",
      "Dong Huk Park",
      "Daylen Yang",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Marcus Rohrbach"
    ],
    "abstract":"Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2016,
    "publication_date":"2016-06-06",
    "citation_count":1528,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/FukuiPYRDR16",
      "ACL":"D16-1044",
      "MAG":"2412400526",
      "ArXiv":"1606.01847",
      "DOI":"10.18653\/v1\/D16-1044",
      "CorpusId":2840197
    }
  },
  {
    "paper_id":"db9296eaa252231e24d066e8413bf29fb058ee45",
    "title":"Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering",
    "authors":[
      "Fengbin Zhu",
      "Wenqiang Lei",
      "Chao Wang",
      "Jianming Zheng",
      "Soujanya Poria",
      "Tat-seng Chua"
    ],
    "abstract":"Open-domain Question Answering (OpenQA) is an important task in Natural Language Processing (NLP), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on OpenQA, particularly on techniques that integrate with neural Machine Reading Comprehension (MRC). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on QA systems. In this work, we review the latest research trends in OpenQA, with particular attention to systems that incorporate neural MRC techniques. Specifically, we begin with revisiting the origin and development of OpenQA systems. We then introduce modern OpenQA architecture named\"Retriever-Reader\"and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing OpenQA systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in OpenQA research, so as to stimulate further progress in this field.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-01-04",
    "citation_count":283,
    "reference_count":163,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2101-00774",
      "ArXiv":"2101.00774",
      "CorpusId":230433817
    }
  },
  {
    "paper_id":"a81874b4a651a740fffbfc47ef96515e8c7f782f",
    "title":"Latent Retrieval for Weakly Supervised Open Domain Question Answering",
    "authors":[
      "Kenton Lee",
      "Ming-Wei Chang",
      "Kristina Toutanova"
    ],
    "abstract":"Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and\/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-06-01",
    "citation_count":1097,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/LeeCT19",
      "ACL":"P19-1612",
      "ArXiv":"1906.00300",
      "MAG":"2947497897",
      "DOI":"10.18653\/v1\/P19-1612",
      "CorpusId":173990818
    }
  },
  {
    "paper_id":"8a1744da011375d711ed75fc2d160c6fdca2cf89",
    "title":"Deep Modular Co-Attention Networks for Visual Question Answering",
    "authors":[
      "Zhou Yu",
      "Jun Yu",
      "Yuhao Cui",
      "D. Tao",
      "Q. Tian"
    ],
    "abstract":"Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective `co-attention' model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63% overall accuracy on the test-dev set.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2019,
    "publication_date":"2019-06-01",
    "citation_count":902,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2954861308",
      "DBLP":"journals\/corr\/abs-1906-10770",
      "ArXiv":"1906.10770",
      "DOI":"10.1109\/CVPR.2019.00644",
      "CorpusId":195657908
    }
  },
  {
    "paper_id":"53de96cf981c9d58a86697d812484808945b47f5",
    "title":"Hierarchical Conditional Relation Networks for Video Question Answering",
    "authors":[
      "T. Le",
      "Vuong Le",
      "S. Venkatesh",
      "T. Tran"
    ],
    "abstract":"Video question answering (VideoQA) is challenging as it requires modeling capacity to distill dynamic visual artifacts and distant relations and to associate them with linguistic concepts. We introduce a general-purpose reusable neural unit called Conditional Relation Network (CRN) that serves as a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning. The resulting architecture for VideoQA is a CRN hierarchy whose branches represent sub-videos or clips, all sharing the same question as the contextual condition. Our evaluations on well-known datasets achieved new SoTA results, demonstrating the impact of building a general-purpose reasoning unit on complex domains such as VideoQA.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2020,
    "publication_date":"2020-02-25",
    "citation_count":281,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3034730770",
      "DBLP":"journals\/corr\/abs-2002-10698",
      "ArXiv":"2002.10698",
      "DOI":"10.1109\/cvpr42600.2020.00999",
      "CorpusId":211296465
    }
  },
  {
    "paper_id":"47fe46e561e3b270407b7bffa176e35291f165a7",
    "title":"Question Answering Over Temporal Knowledge Graphs",
    "authors":[
      "Apoorv Saxena",
      "Soumen Chakrabarti",
      "Partha P. Talukdar"
    ],
    "abstract":"Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-06-03",
    "citation_count":165,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2106.01515",
      "DBLP":"conf\/acl\/SaxenaCT20",
      "ACL":"2021.acl-long.520",
      "DOI":"10.18653\/v1\/2021.acl-long.520",
      "CorpusId":235313508
    }
  },
  {
    "paper_id":"d365978adf0a5c9c6028820857e015617856256b",
    "title":"MultiModalQA: Complex Question Answering over Text, Tables and Images",
    "authors":[
      "Alon Talmor",
      "Ori Yoran",
      "Amnon Catav",
      "Dan Lahav",
      "Yizhong Wang",
      "Akari Asai",
      "Gabriel Ilharco",
      "Hannaneh Hajishirzi",
      "Jonathan Berant"
    ],
    "abstract":"When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities. In this paper, we present MultiModalQA(MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically-generated questions and rephrase them into more fluent language. We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1of 51.7 over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1",
    "venue":"International Conference on Learning Representations",
    "year":2021,
    "publication_date":"2021-04-13",
    "citation_count":195,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2104.06039",
      "DBLP":"conf\/iclr\/TalmorYCLWAIHB21",
      "CorpusId":233219849
    }
  },
  {
    "paper_id":"6d40db49cec2a543e01a4ef651f053ae935274fc",
    "title":"Sequence-to-Sequence Knowledge Graph Completion and Question Answering",
    "authors":[
      "Apoorv Saxena",
      "Adrian Kochsiek",
      "Rainer Gemulla"
    ],
    "abstract":"Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors. These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA). KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities. For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility. We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering. We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding. Such a simple but powerful method reduces the model size up to 98% compared to conventional KGE models while keeping inference time tractable. After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-03-19",
    "citation_count":152,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/SaxenaKG22",
      "ArXiv":"2203.10321",
      "ACL":"2022.acl-long.201",
      "DOI":"10.48550\/arXiv.2203.10321",
      "CorpusId":247595094
    }
  },
  {
    "paper_id":"d21c5a9e34a6290212c68f6ec64f65ad5c5e46f4",
    "title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases",
    "authors":[
      "Yu Gu",
      "Sue E. Kase",
      "M. Vanni",
      "Brian M. Sadler",
      "Percy Liang",
      "Xifeng Yan",
      "Yu Su"
    ],
    "abstract":"Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1",
    "venue":"The Web Conference",
    "year":2020,
    "publication_date":"2020-11-16",
    "citation_count":252,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3100058503",
      "DBLP":"conf\/www\/GuKVSLY021",
      "ArXiv":"2011.07743",
      "DOI":"10.1145\/3442381.3449992",
      "CorpusId":226965153
    }
  },
  {
    "paper_id":"20b919412b474fdaafc9cc57a4b759798fda21cc",
    "title":"ScienceQA: a novel resource for question answering on scholarly articles",
    "authors":[
      "Tanik Saikh",
      "Tirthankar Ghosal",
      "Amish Mittal",
      "Asif Ekbal",
      "P. Bhattacharyya"
    ],
    "abstract":"Machine Reading Comprehension (MRC) of a document is a challenging problem that requires discourse-level understanding. Information extraction from scholarly articles nowadays is a critical use case for researchers to understand the underlying research quickly and move forward, especially in this age of infodemic. MRC on research articles can also provide helpful information to the reviewers and editors. However, the main bottleneck in building such models is the availability of human-annotated data. In this paper, firstly, we introduce a dataset to facilitate question answering (QA) on scientific articles. We prepare the dataset in a semi-automated fashion having more than 100k human-annotated context–question–answer triples. Secondly, we implement one baseline QA model based on Bidirectional Encoder Representations from Transformers (BERT). Additionally, we implement two models: the first one is based on Science BERT (SciBERT), and the second is the combination of SciBERT and Bi-Directional Attention Flow (Bi-DAF). The best model (i.e., SciBERT) obtains an F1 score of 75.46%. Our dataset is novel, and our work opens up a new avenue for scholarly document processing research by providing a benchmark QA dataset and standard baseline. We make our dataset and codes available here at https:\/\/github.com\/TanikSaikh\/Scientific-Question-Answering.",
    "venue":"International Journal on Digital Libraries",
    "year":2022,
    "publication_date":"2022-07-20",
    "citation_count":129,
    "reference_count":69,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "PubMedCentral":"9297303",
      "DBLP":"journals\/jodl\/SaikhGMEB22",
      "DOI":"10.1007\/s00799-022-00329-y",
      "CorpusId":250729995,
      "PubMed":"35873651"
    }
  },
  {
    "paper_id":"1015e77ecc84237515cccf9fc583796c38584c28",
    "title":"RSVQA: Visual Question Answering for Remote Sensing Data",
    "authors":[
      "Sylvain Lobry",
      "Diego Marcos",
      "J. Murray",
      "D. Tuia"
    ],
    "abstract":"This article introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information, which can be useful for a wide range of tasks, including land cover classification, object counting, or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high-level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two data sets (using low- and high-resolution data) of image\/question\/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The data sets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on convolutional neural networks (CNNs) for the visual part and a recurrent neural network (RNN) for the natural language part of this task. The model is trained on the two data sets, yielding promising results in both cases.",
    "venue":"IEEE Transactions on Geoscience and Remote Sensing",
    "year":2020,
    "publication_date":"2020-03-16",
    "citation_count":274,
    "reference_count":42,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3022533163",
      "ArXiv":"2003.07333",
      "DBLP":"journals\/tgrs\/LobryMMT20",
      "DOI":"10.1109\/TGRS.2020.2988782",
      "CorpusId":212725281
    }
  },
  {
    "paper_id":"9e979667aa81c294062c02ab3a48e87e47c54987",
    "title":"Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
    "authors":[
      "Yanlin Feng",
      "Xinyue Chen",
      "Bill Yuchen Lin",
      "Peifeng Wang",
      "Jun Yan",
      "Xiang Ren"
    ],
    "abstract":"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-05-01",
    "citation_count":261,
    "reference_count":42,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2005-00646",
      "MAG":"3097986428",
      "ACL":"2020.emnlp-main.99",
      "ArXiv":"2005.00646",
      "DOI":"10.18653\/v1\/2020.emnlp-main.99",
      "CorpusId":218486837
    }
  },
  {
    "paper_id":"77937788fdbdefe4dd775075b5768c03eaaac008",
    "title":"AVQA: A Dataset for Audio-Visual Question Answering on Videos",
    "authors":[
      "Pinci Yang",
      "Xin Wang",
      "Xuguang Duan",
      "Hong Chen",
      "Runze Hou",
      "Cong Jin",
      "Wenwu Zhu"
    ],
    "abstract":"Audio-visual question answering aims to answer questions regarding both audio and visual modalities in a given video, and has drawn increasing research interest in recent years. However, there have been no appropriate datasets for this challenging task on videos in real-life scenarios so far. They are either designed with questions containing only visual clues without taking any audio information into account, or considering audio with restrictions to specific scenarios, such as panoramic videos and videos about music performances. In this paper, to overcome the limitations of existing datasets, we introduce AVQA, a new audio-visual question answering dataset on videos in real-life scenarios. We collect 57,015 videos from daily audio-visual activities and 57,335 specially-designed question-answer pairs relying on clues from both modalities, where information contained in a single modality is insufficient or ambiguous. Furthermore, we propose a Hierarchical Audio-Visual Fusing module to model multiple semantic correlations among audio, visual, and text modalities and conduct ablation studies to analyze the role of different modalities on our datasets. Experimental results show that our proposed method significantly improves the audio-visual question answering performance over various question types. Therefore, AVQA can provide an adequate testbed for the generation of models with a deeper understanding of multimodal information on audio-visual question answering in real-life scenarios. (The dataset is available at https:\/\/mn.cs.tsinghua.edu.cn\/avqa)",
    "venue":"ACM Multimedia",
    "year":2022,
    "publication_date":"2022-10-10",
    "citation_count":115,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/mm\/Yang0DCHJ022",
      "DOI":"10.1145\/3503161.3548291",
      "CorpusId":252783126
    }
  },
  {
    "paper_id":"388513e8e09ad60f619054361f4d2cdf5a146bc8",
    "title":"FeTaQA: Free-form Table Question Answering",
    "authors":[
      "Linyong Nan",
      "Chia-Hsuan Hsieh",
      "Ziming Mao",
      "Xi Victoria Lin",
      "Neha Verma",
      "Rui Zhang",
      "Wojciech Kryscinski",
      "Nick Schoelkopf",
      "Riley Kong",
      "Xiangru Tang",
      "Murori Mutuma",
      "Benjamin Rosand",
      "Isabel Trindade",
      "Renusree Bandaru",
      "Jacob Cunningham",
      "Caiming Xiong",
      "Dragomir R. Radev"
    ],
    "abstract":"Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system’s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question–answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-01",
    "citation_count":208,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tacl\/NanHMLVZKSKTMRT22",
      "ArXiv":"2104.00369",
      "ACL":"2022.tacl-1.3",
      "DOI":"10.1162\/tacl_a_00446",
      "CorpusId":232478685
    }
  },
  {
    "paper_id":"f964563dbc1067a4965fda6b7dbd23c4f5d56018",
    "title":"Reasoning with Heterogeneous Graph Alignment for Video Question Answering",
    "authors":[
      "Pin Jiang",
      "Yahong Han"
    ],
    "abstract":"The dominant video question answering methods are based on fine-grained representation or model-specific attention mechanism. They usually process video and question separately, then feed the representations of different modalities into following late fusion networks. Although these methods use information of one modality to boost the other, they neglect to integrate correlations of both inter- and intra-modality in an uniform module. We propose a deep heterogeneous graph alignment network over the video shots and question words. Furthermore, we explore the network architecture from four steps: representation, fusion, alignment, and reasoning. Within our network, the inter- and intra-modality information can be aligned and interacted simultaneously over the heterogeneous graph and used for cross-modal reasoning. We evaluate our method on three benchmark datasets and conduct extensive ablation study to the effectiveness of the network architecture. Experiments show the network to be superior in quality.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2020,
    "publication_date":"2020-04-03",
    "citation_count":204,
    "reference_count":28,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/JiangH20",
      "MAG":"2998166190",
      "DOI":"10.1609\/AAAI.V34I07.6767",
      "CorpusId":213506079
    }
  },
  {
    "paper_id":"32cb26814b320beb0c4a8fdea86a0065ad873ef3",
    "title":"Open-Domain Question Answering Goes Conversational via Question Rewriting",
    "authors":[
      "R. Anantha",
      "Svitlana Vakulenko",
      "Zhucheng Tu",
      "S. Longpre",
      "S. Pulman",
      "Srinivas Chappidi"
    ],
    "abstract":"We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-10",
    "citation_count":198,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3171244865",
      "ACL":"2021.naacl-main.44",
      "DBLP":"conf\/naacl\/AnanthaVTLPC21",
      "ArXiv":"2010.04898",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.44",
      "CorpusId":222290679
    }
  },
  {
    "paper_id":"a5e7061780a8f96eb5dbb7844ebd95c901010027",
    "title":"Invariant Grounding for Video Question Answering",
    "authors":[
      "Yicong Li",
      "Xiang Wang",
      "Junbin Xiao",
      "Wei Ji",
      "Tat-seng Chua"
    ],
    "abstract":"Video Question Answering (VideoQA) is the task of an-swering questions about a video. At its core is understanding the alignments between visual scenes in video and linguistic semantics in question to yield the answer. In leading VideoQA models, the typical learning objective, empirical risk minimization (ERM), latches on superficial correlations between video-question pairs and answers as the alignments. However, ERM can be problematic, because it tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspecting the causal effect of question-critical scenes. As a result, the VideoQA models suffer from unreliable reasoning. In this work, we first take a causal look at VideoQA and argue that invariant grounding is the key to ruling out the spurious correlations. Towards this end, we propose a new learning framework, Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different interventions on the complement. With IGV, the VideoQA mod-els are forced to shield the answering process from the negative influence of spurious correlations, which significantly improves the reasoning ability. Experiments on three benchmark datasets validate the superiority of IGV in terms of accuracy, visual explainability, and generalization ability over the leading baselines. Our code is available at https:\/\/github.com\/y13800\/IGV.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2022,
    "publication_date":"2022-06-01",
    "citation_count":109,
    "reference_count":39,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-02349",
      "ArXiv":"2206.02349",
      "DOI":"10.1109\/CVPR52688.2022.00294",
      "CorpusId":249395473
    }
  },
  {
    "paper_id":"33422275fbb9958f55419620697faf531482699b",
    "title":"How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
    "authors":[
      "Zhengbao Jiang",
      "J. Araki",
      "Haibo Ding",
      "Graham Neubig"
    ],
    "abstract":"Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https:\/\/github.com\/jzbjyb\/lm-calibration.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-12-02",
    "citation_count":496,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tacl\/JiangADN21",
      "ArXiv":"2012.00955",
      "DOI":"10.1162\/tacl_a_00407",
      "CorpusId":235078802
    }
  },
  {
    "paper_id":"6f85ec89d9c07a8db4545e64888ced820370a21b",
    "title":"Retrieval Augmented Visual Question Answering with Outside Knowledge",
    "authors":[
      "Weizhe Lin",
      "B. Byrne"
    ],
    "abstract":"Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR trained separately from answer generation, introducing a potential limit on the overall system performance.Instead, we propose a joint training scheme which includes differentiable DPR integrated with answer generation so that the system can be trained in an end-to-end fashion. Our experiments show that our scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also introduce new diagnostic metrics to analyze how retrieval and generation interact. The strong retrieval ability of our model significantly reduces the number of retrieved documents needed in training, yielding significant benefits in answer quality and computation required for training.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-10-07",
    "citation_count":103,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2210.03809",
      "DBLP":"journals\/corr\/abs-2210-03809",
      "ACL":"2022.emnlp-main.772",
      "DOI":"10.48550\/arXiv.2210.03809",
      "CorpusId":252780775
    }
  },
  {
    "paper_id":"39e734da43eb8c72e9549b42e96760545036f8e5",
    "title":"QuAC: Question Answering in Context",
    "authors":[
      "Eunsol Choi",
      "He He",
      "Mohit Iyyer",
      "Mark Yatskar",
      "Wen-tau Yih",
      "Yejin Choi",
      "Percy Liang",
      "Luke Zettlemoyer"
    ],
    "abstract":"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http:\/\/quac.ai.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-08-21",
    "citation_count":870,
    "reference_count":31,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1808-07036",
      "ACL":"D18-1241",
      "MAG":"2951831170",
      "ArXiv":"1808.07036",
      "DOI":"10.18653\/v1\/D18-1241",
      "CorpusId":52057510
    }
  },
  {
    "paper_id":"ebf59587f8f170ff4241c42263bbfb9da5bd2135",
    "title":"ELI5: Long Form Question Answering",
    "authors":[
      "Angela Fan",
      "Yacine Jernite",
      "Ethan Perez",
      "David Grangier",
      "J. Weston",
      "Michael Auli"
    ],
    "abstract":"We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum “Explain Like I’m Five” (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-07-01",
    "citation_count":712,
    "reference_count":28,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2964040452",
      "DBLP":"conf\/acl\/FanJPGWA19",
      "ACL":"P19-1346",
      "ArXiv":"1907.09190",
      "DOI":"10.18653\/v1\/P19-1346",
      "CorpusId":196170479
    }
  },
  {
    "paper_id":"4cb65c1541acde658a95bb8a038cefdade703da5",
    "title":"Video Question Answering: Datasets, Algorithms and Challenges",
    "authors":[
      "Yaoyao Zhong",
      "Wei Ji",
      "Junbin Xiao",
      "Yicong Li",
      "Wei Deng",
      "Tat-seng Chua"
    ],
    "abstract":"This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions. We firstly categorize the datasets into 1) normal VideoQA, multi-modal VideoQA and knowledge-based VideoQA, according to the modalities invoked in the question-answer pairs, or 2) factoid VideoQA and inference VideoQA, according to the technical challenges in comprehending the questions and deriving the correct answers. We then summarize the VideoQA techniques, including those mainly designed for Factoid QA (e.g., the early spatio-temporal attention-based methods and the recently Transformer-based ones) and those targeted at explicit relation and logic inference (e.g., neural modular networks, neural symbolic methods, and graph-structured methods). Aside from the backbone techniques, we delve into the specific models and find out some common and useful insights either for video modeling, question answering, or for cross-modal correspondence learning. Finally, we point out the research trend of studying beyond factoid VideoQA to inference VideoQA, as well as towards the robustness and interpretability. Additionally, we maintain a repository, https:\/\/github.com\/VRU-NExT\/VideoQA, to keep trace of the latest VideoQA papers, datasets, and their open-source implementations if available. With these efforts, we strongly hope this survey could shed light on the follow-up VideoQA research.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-03-02",
    "citation_count":110,
    "reference_count":128,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2203-01225",
      "ACL":"2022.emnlp-main.432",
      "ArXiv":"2203.01225",
      "DOI":"10.48550\/arXiv.2203.01225",
      "CorpusId":247218478
    }
  },
  {
    "paper_id":"faa5f1130ff9d05a8ae5b9ba79d675bd4d917c5d",
    "title":"MLQA: Evaluating Cross-lingual Extractive Question Answering",
    "authors":[
      "Patrick Lewis",
      "Barlas Oğuz",
      "Ruty Rinott",
      "Sebastian Riedel",
      "Holger Schwenk"
    ],
    "abstract":"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-10-16",
    "citation_count":551,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2020.acl-main.653",
      "MAG":"3035497479",
      "ArXiv":"1910.07475",
      "DBLP":"conf\/acl\/LewisORRS20",
      "DOI":"10.18653\/v1\/2020.acl-main.653",
      "CorpusId":204734128
    }
  },
  {
    "paper_id":"fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
    "title":"Hierarchical Question-Image Co-Attention for Visual Question Answering",
    "authors":[
      "Jiasen Lu",
      "Jianwei Yang",
      "Dhruv Batra",
      "Devi Parikh"
    ],
    "abstract":"A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.",
    "venue":"Neural Information Processing Systems",
    "year":2016,
    "publication_date":"2016-05-31",
    "citation_count":1691,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2963668159",
      "ArXiv":"1606.00061",
      "DBLP":"journals\/corr\/LuYBP16",
      "CorpusId":868693
    }
  },
  {
    "paper_id":"63e939b0606207941673def2b69c6240d549d198",
    "title":"Multi-hop Question Answering",
    "authors":[
      "Vaibhav Mavi",
      "Anubhav Jangra",
      "A. Jatowt"
    ],
    "abstract":"The task of Question Answering (QA) has attracted significant research interest for long. Its relevance to language understanding and knowledge retrieval tasks, along with the simple setting makes the task of QA crucial for strong AI systems. Recent success on simple QA tasks has shifted the focus to more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most researched tasks over the recent years. In broad terms, MHQA is the task of answering natural language questions that involve extracting and combining multiple pieces of information and doing multiple steps of reasoning. An example of a multi-hop question would be\"The Argentine PGA Championship record holder has won how many tournaments worldwide?\". Answering the question would need two pieces of information:\"Who is the record holder for Argentine PGA Championship tournaments?\"and\"How many tournaments did [Answer of Sub Q1] win?\". The ability to answer multi-hop questions and perform multi step reasoning can significantly improve the utility of NLP systems. Consequently, the field has seen a surge with high quality datasets, models and evaluation strategies. The notion of 'multiple hops' is somewhat abstract which results in a large variety of tasks that require multi-hop reasoning. This leads to different datasets and models that differ significantly from each other and makes the field challenging to generalize and survey. We aim to provide a general and formal definition of the MHQA task, and organize and summarize existing MHQA frameworks. We also outline some best practices for building MHQA datasets. This book provides a systematic and thorough introduction as well as the structuring of the existing attempts to this highly interesting, yet quite challenging task.",
    "venue":"Foundations and Trends in Information Retrieval",
    "year":2022,
    "publication_date":"2022-04-19",
    "citation_count":56,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/ftir\/MaviJJ24",
      "ArXiv":"2204.09140",
      "DOI":"10.1561\/1500000102",
      "CorpusId":248266450
    }
  },
  {
    "paper_id":"93d3e45395117e21214d404c8753b578c29266d1",
    "title":"Open Question Answering over Tables and Text",
    "authors":[
      "Wenhu Chen",
      "Ming-Wei Chang",
      "Eva Schlinger",
      "W. Wang",
      "William W. Cohen"
    ],
    "abstract":"In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over both tabular and textual data and present a new large-scale dataset Open Table-and-Text Question Answering (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging -- our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use \"early fusion\" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27%.",
    "venue":"International Conference on Learning Representations",
    "year":2020,
    "publication_date":"2020-10-20",
    "citation_count":222,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2010-10439",
      "MAG":"3094282562",
      "ArXiv":"2010.10439",
      "CorpusId":224803601
    }
  },
  {
    "paper_id":"750274c61a0936c9c9db32da659cddbcb227cf32",
    "title":"MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
    "authors":[
      "S. Longpre",
      "Yi Lu",
      "Joachim Daiber"
    ],
    "abstract":"Abstract Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-07-30",
    "citation_count":189,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2007-15207",
      "MAG":"3045958725",
      "ArXiv":"2007.15207",
      "DOI":"10.1162\/tacl_a_00433",
      "CorpusId":220871404
    }
  },
  {
    "paper_id":"476ff888fe3917f92b221c522ffb7bfaa4e1861b",
    "title":"Open-Retrieval Conversational Question Answering",
    "authors":[
      "Chen Qu",
      "Liu Yang",
      "Cen Chen",
      "Minghui Qiu",
      "W. Bruce Croft",
      "Mohit Iyyer"
    ],
    "abstract":"Conversational search is one of the ultimate goals of information retrieval. Recent research approaches conversational search by simplified settings of response ranking and conversational question answering, where an answer is either selected from a given candidate set or extracted from a given passage. These simplifications neglect the fundamental role of retrieval in conversational search. To address this limitation, we introduce an open-retrieval conversational question answering (ORConvQA) setting, where we learn to retrieve evidence from a large collection before extracting answers, as a further step towards building functional conversational search systems. We create a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an end-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader that are all based on Transformers. Our extensive experiments on OR-QuAC demonstrate that a learnable retriever is crucial for ORConvQA. We further show that our system can make a substantial improvement when we enable history modeling in all system components. Moreover, we show that the reranker component contributes to the model performance by providing a regularization effect. Finally, further in-depth analyses are performed to provide new insights into ORConvQA.",
    "venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year":2020,
    "publication_date":"2020-05-22",
    "citation_count":191,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3028226890",
      "DBLP":"conf\/sigir\/Qu0CQCI20",
      "ArXiv":"2005.11364",
      "DOI":"10.1145\/3397271.3401110",
      "CorpusId":218869571
    }
  },
  {
    "paper_id":"e7e1313061b0d56364bd2c41f017deb954bb05db",
    "title":"TVQA: Localized, Compositional Video Question Answering",
    "authors":[
      "Jie Lei",
      "Licheng Yu",
      "Mohit Bansal",
      "Tamara L. Berg"
    ],
    "abstract":"Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http:\/\/tvqa.cs.unc.edu.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-09-05",
    "citation_count":699,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1809-01696",
      "ACL":"D18-1167",
      "ArXiv":"1809.01696",
      "MAG":"2951721547",
      "DOI":"10.18653\/v1\/D18-1167",
      "CorpusId":52171684
    }
  },
  {
    "paper_id":"3122a2d7799ba585b993e432b3deb47659b3f3c1",
    "title":"Hurdles to Progress in Long-form Question Answering",
    "authors":[
      "Kalpesh Krishna",
      "Aurko Roy",
      "Mohit Iyyer"
    ],
    "abstract":"The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system’s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train \/ validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-03-10",
    "citation_count":220,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2103.06332",
      "MAG":"3169841173",
      "DBLP":"conf\/naacl\/KrishnaRI21",
      "ACL":"2021.naacl-main.393",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.393",
      "CorpusId":232185275
    }
  },
  {
    "paper_id":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
    "title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
    "authors":[
      "Anna Rogers",
      "Matt Gardner",
      "Isabelle Augenstein"
    ],
    "abstract":"Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with more than 80 new datasets appearing in the past 2 years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of “skills” that question answering\/reading comprehension systems are supposed to acquire and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of overfocusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data and at researchers working on new resources.",
    "venue":"ACM Computing Surveys",
    "year":2021,
    "publication_date":"2021-07-27",
    "citation_count":182,
    "reference_count":348,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2107.12708",
      "DBLP":"journals\/corr\/abs-2107-12708",
      "DOI":"10.1145\/3560260",
      "CorpusId":236447339
    }
  },
  {
    "paper_id":"f655b61b0929d02fa74392fe4fc2aedc801b4f47",
    "title":"A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions",
    "authors":[
      "Yunshi Lan",
      "Gaole He",
      "Jinhao Jiang",
      "Jing Jiang",
      "Wayne Xin Zhao",
      "Ji-rong Wen"
    ],
    "abstract":"Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Recently, a large number of studies focus on semantically or syntactically complicated questions. In this paper, we elaborately summarize the typical challenges and solutions for complex KBQA. We begin with introducing the background about the KBQA task. Next, we present the two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. We then review the advanced methods comprehensively from the perspective of the two categories. Specifically, we explicate their solutions to the typical challenges. Finally, we conclude and discuss some promising directions for future research.",
    "venue":"International Joint Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-05-25",
    "citation_count":185,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2105.11644",
      "DBLP":"journals\/corr\/abs-2105-11644",
      "DOI":"10.24963\/ijcai.2021\/611",
      "CorpusId":235187102
    }
  },
  {
    "paper_id":"cb693ce346f44e6b89ee814c6bb9f0e5cc2fa9d2",
    "title":"Selective Question Answering under Domain Shift",
    "authors":[
      "Amita Kamath",
      "Robin Jia",
      "Percy Liang"
    ],
    "abstract":"To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model’s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model’s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model’s probabilities only answers 48% at 80% accuracy.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-06-16",
    "citation_count":237,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2006-09462",
      "MAG":"3035847612",
      "ArXiv":"2006.09462",
      "ACL":"2020.acl-main.503",
      "DOI":"10.18653\/v1\/2020.acl-main.503",
      "CorpusId":219721462
    }
  },
  {
    "paper_id":"ab9cc2c6a8d35d7a145cf608ff9dd7af87213253",
    "title":"RUBi: Reducing Unimodal Biases in Visual Question Answering",
    "authors":[
      "Rémi Cadène",
      "Corentin Dancette",
      "H. Ben-younes",
      "M. Cord",
      "Devi Parikh"
    ],
    "abstract":"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. \nWe propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. \nOur code is available: this http URL",
    "venue":"Neural Information Processing Systems",
    "year":2019,
    "publication_date":"2019-06-01",
    "citation_count":396,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/CadeneDBCP19",
      "ArXiv":"1906.10169",
      "MAG":"2970017794",
      "CorpusId":195584122
    }
  },
  {
    "paper_id":"1097cf8cf5961589ff693b069002e7181e24e631",
    "title":"OCR-VQA: Visual Question Answering by Reading Text in Images",
    "authors":[
      "Anand Mishra",
      "Shashank Shekhar",
      "A. Singh",
      "Anirban Chakraborty"
    ],
    "abstract":"The problem of answering questions about an image is popularly known as visual question answering (or VQA in short). It is a well-established problem in computer vision. However, none of the VQA methods currently utilize the text often present in the image. These \"texts in images\" provide additional useful cues and facilitate better understanding of the visual content. In this paper, we introduce a novel task of visual question answering by reading text in images, i.e., by optical character recognition or OCR. We refer to this problem as OCR-VQA. To facilitate a systematic way of studying this new problem, we introduce a large-scale dataset, namely OCRVQA-200K. This dataset comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs about these images. We judiciously combine well-established techniques from OCR and VQA domains to present a novel baseline for OCR-VQA-200K. The experimental results and rigorous analysis demonstrate various challenges present in this dataset leaving ample scope for the future research. We are optimistic that this new task along with compiled dataset will open-up many exciting research avenues both for the document image analysis and the VQA communities.",
    "venue":"IEEE International Conference on Document Analysis and Recognition",
    "year":2019,
    "publication_date":"2019-09-01",
    "citation_count":556,
    "reference_count":26,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icdar\/0001SSC19",
      "MAG":"3004268082",
      "DOI":"10.1109\/ICDAR.2019.00156",
      "CorpusId":209413409
    }
  },
  {
    "paper_id":"4f2c1af57c056102806a184517313804f66e7447",
    "title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
    "authors":[
      "Zhou Yu",
      "D. Xu",
      "Jun Yu",
      "Ting Yu",
      "Zhou Zhao",
      "Yueting Zhuang",
      "D. Tao"
    ],
    "abstract":"Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2019,
    "publication_date":"2019-06-06",
    "citation_count":581,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1906.02467",
      "DBLP":"conf\/aaai\/YuXYYZZT19",
      "MAG":"2964220823",
      "DOI":"10.1609\/aaai.v33i01.33019127",
      "CorpusId":69645185
    }
  },
  {
    "paper_id":"71c7104eaed93497824cf197949c77e7d6cb36d3",
    "title":"PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
    "authors":[
      "Haitian Sun",
      "Tania Bedrax-Weiss",
      "William W. Cohen"
    ],
    "abstract":"We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., “multi-hop”) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or “pull”) operations on the corpus and\/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-04-01",
    "citation_count":408,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1904.09537",
      "DBLP":"journals\/corr\/abs-1904-09537",
      "ACL":"D19-1242",
      "MAG":"2971155257",
      "DOI":"10.18653\/v1\/D19-1242",
      "CorpusId":128345225
    }
  },
  {
    "paper_id":"3a84c4dcdcc59d418e41fd8279626e25f87e1e6f",
    "title":"XOR QA: Cross-lingual Open-Retrieval Question Answering",
    "authors":[
      "Akari Asai",
      "Jungo Kasai",
      "J. Clark",
      "Kenton Lee",
      "Eunsol Choi",
      "Hannaneh Hajishirzi"
    ],
    "abstract":"Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity—where languages have few reference articles—and information asymmetry—where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https:\/\/nlp.cs.washington.edu\/xorqa\/.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-22",
    "citation_count":167,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/AsaiKCLCH21",
      "ArXiv":"2010.11856",
      "MAG":"3093681547",
      "ACL":"2021.naacl-main.46",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.46",
      "CorpusId":225040672
    }
  },
  {
    "paper_id":"a4e996a6a3df1ac76a28b1e0513bbcb5449e4227",
    "title":"A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
    "authors":[
      "Hanlei Jin",
      "Yang Zhang",
      "Dan Meng",
      "Jun Wang",
      "Jinghua Tan"
    ],
    "abstract":"Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema''perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-03-05",
    "citation_count":153,
    "reference_count":427,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2403.02901",
      "DBLP":"journals\/corr\/abs-2403-02901",
      "DOI":"10.48550\/arXiv.2403.02901",
      "CorpusId":268248096
    }
  },
  {
    "paper_id":"c181d9e2e488d3e4a78f67e9e8884565f3c7051e",
    "title":"Abstractive Text Summarization Using GAN",
    "authors":[
      "Tanushree Bharti",
      "Satyam Kumar Sinha",
      "Harshit Singhal",
      "Rohit Saini",
      "Dipesh Parihar"
    ],
    "abstract":"In the field of natural language processing, the task of writing long concepts into short expressions has attracted attention due to its ability to simplify the processing and understanding of information. While traditional transcription techniques are effective to some extent, they often fail to capture the essence and nuances of the original texts. This article explores a new approach to collecting abstract data using artificial neural networks (GANs), a class of deep learning models known for their ability to create patterns of real information. We describe the fundamentals of text collection through a comprehensive review of existing literature and methods and highlight the complexity of GAN-based text. Our goal is to transform complex text into context and meaning by combining the power of GANs with natural language understanding. We detail the design and training of an adaptive GAN model for the text recognition task. We also conduct various experiments and evaluations using established metrics such as ROUGE and BLEU scores to evaluate the effectiveness and efficiency of our approach. The results show that GANs can be used to improve the quality and consistency of generated content, data storage, data analysis paper, etc. It shows its promise in paving the way for advanced applications in fields. Through this research, we aim to contribute to the continued evolution of writing technology, providing insights and innovations that support the field to a new level of well-done.",
    "venue":"International Journal of Innovative Science and Research Technology",
    "year":2024,
    "publication_date":"2024-08-30",
    "citation_count":703,
    "reference_count":46,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.38124\/ijisrt\/ijisrt24aug334",
      "CorpusId":272301706
    }
  },
  {
    "paper_id":"512e1cbb4987c036f44f429ba5320011daec1b6e",
    "title":"A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models",
    "authors":[
      "Haopeng Zhang",
      "Philip S. Yu",
      "Jiawei Zhang"
    ],
    "abstract":"Text summarization research has undergone several significant transformations with the advent of deep neural networks, pre-trained language models (PLMs), and recent large language models (LLMs). This survey thus provides a comprehensive review of the research progress and evolution in text summarization through the lens of these paradigm shifts. It is organized into two main parts: (1) a detailed overview of datasets, evaluation metrics, and summarization methods before the LLM era, encompassing traditional statistical methods, deep learning approaches, and PLM fine-tuning techniques, and (2) the first detailed examination of recent advancements in benchmarking, modeling, and evaluating summarization in the LLM era. By synthesizing existing literature and presenting a cohesive overview, this survey also discusses research trends, open challenges, and proposes promising research directions in summarization, aiming to guide researchers through the evolving landscape of summarization research.",
    "venue":"ACM Computing Surveys",
    "year":2024,
    "publication_date":"2024-06-17",
    "citation_count":74,
    "reference_count":285,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2406-11289",
      "ArXiv":"2406.11289",
      "DOI":"10.1145\/3731445",
      "CorpusId":270560254
    }
  },
  {
    "paper_id":"f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
    "title":"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond",
    "authors":[
      "Ramesh Nallapati",
      "Bowen Zhou",
      "C. D. Santos",
      "Çaglar Gülçehre",
      "Bing Xiang"
    ],
    "abstract":"In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",
    "venue":"Conference on Computational Natural Language Learning",
    "year":2016,
    "publication_date":"2016-02-19",
    "citation_count":2689,
    "reference_count":34,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1602.06023",
      "DBLP":"conf\/conll\/NallapatiZSGX16",
      "MAG":"2341401723",
      "ACL":"K16-1028",
      "DOI":"10.18653\/v1\/K16-1028",
      "CorpusId":8928715
    }
  },
  {
    "paper_id":"63748e59f4e106cbda6b65939b77589f40e48fcb",
    "title":"Text Summarization with Pretrained Encoders",
    "authors":[
      "Yang Liu",
      "Mirella Lapata"
    ],
    "abstract":"Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-08-01",
    "citation_count":1538,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"D19-1387",
      "ArXiv":"1908.08345",
      "DBLP":"conf\/emnlp\/LiuL19",
      "MAG":"2970419734",
      "DOI":"10.18653\/v1\/D19-1387",
      "CorpusId":201304248
    }
  },
  {
    "paper_id":"26bdde27ff70384ef7c43f2670d8ec6559c71f45",
    "title":"Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
    "authors":[
      "Shichao Sun",
      "Ruifeng Yuan",
      "Ziqiang Cao",
      "Wenjie Li",
      "Pengfei Liu"
    ],
    "abstract":"Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2024,
    "publication_date":"2024-06-01",
    "citation_count":26,
    "reference_count":17,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2406-00507",
      "ArXiv":"2406.00507",
      "DOI":"10.48550\/arXiv.2406.00507",
      "CorpusId":270218703
    }
  },
  {
    "paper_id":"306c0576750d8ac1298f70474560aa951490b2a1",
    "title":"Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
    "authors":[
      "Xianjun Yang",
      "Yan Li",
      "Xinlu Zhang",
      "Haifeng Chen",
      "Wei Cheng"
    ],
    "abstract":"Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies \\cite{goyal2022news, zhang2023benchmarking} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-02-16",
    "citation_count":200,
    "reference_count":18,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-08081",
      "ArXiv":"2302.08081",
      "DOI":"10.48550\/arXiv.2302.08081",
      "CorpusId":256901227
    }
  },
  {
    "paper_id":"007c3d9b8dab341d2c77c4ee764fd921f7f14956",
    "title":"Adapted large language models can outperform medical experts in clinical text summarization",
    "authors":[
      "Dave Van Veen",
      "Cara Van Uden",
      "Louis Blankemeier",
      "Jean-Benoit Delbrouck",
      "Asad Aali",
      "Christian Blüthgen",
      "A. Pareek",
      "Malgorzata Polacin",
      "William Collins",
      "Neera Ahuja",
      "C. Langlotz",
      "Jason Hom",
      "S. Gatidis",
      "John M. Pauly",
      "Akshay S. Chaudhari"
    ],
    "abstract":"Analyzing vast textual data and summarizing key information from electronic health records imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown promise in natural language processing (NLP) tasks, their effectiveness on a diverse range of clinical summarization tasks remains unproven. Here we applied adaptation methods to eight LLMs, spanning four distinct clinical summarization tasks: radiology reports, patient questions, progress notes and doctor–patient dialogue. Quantitative assessments with syntactic, semantic and conceptual NLP metrics reveal trade-offs between models and adaptation methods. A clinical reader study with 10 physicians evaluated summary completeness, correctness and conciseness; in most cases, summaries from our best-adapted LLMs were deemed either equivalent (45%) or superior (36%) compared with summaries from medical experts. The ensuing safety analysis highlights challenges faced by both LLMs and medical experts, as we connect errors to potential medical harm and categorize types of fabricated information. Our research provides evidence of LLMs outperforming medical experts in clinical text summarization across multiple tasks. This suggests that integrating LLMs into clinical workflows could alleviate documentation burden, allowing clinicians to focus more on patient care. Comparative performance assessment of large language models identified ChatGPT-4 as the best-adapted model across a diverse set of clinical text summarization tasks, and it outperformed 10 medical experts in a reader study.",
    "venue":"Nature Network Boston",
    "year":2023,
    "publication_date":"2023-09-14",
    "citation_count":502,
    "reference_count":103,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "ArXiv":"2309.07430",
      "DOI":"10.1038\/s41591-024-02855-5",
      "CorpusId":261822526,
      "PubMed":"38413730"
    }
  },
  {
    "paper_id":"6c4b76232bb72897685d19b3d264c6ee3005bc2b",
    "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors":[
      "Colin Raffel",
      "Noam M. Shazeer",
      "Adam Roberts",
      "Katherine Lee",
      "Sharan Narang",
      "Michael Matena",
      "Yanqi Zhou",
      "Wei Li",
      "Peter J. Liu"
    ],
    "abstract":"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
    "venue":"Journal of machine learning research",
    "year":2019,
    "publication_date":"2019-10-23",
    "citation_count":22976,
    "reference_count":134,
    "fields_of_study":[
      "Mathematics",
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2981852735",
      "DBLP":"journals\/corr\/abs-1910-10683",
      "ArXiv":"1910.10683",
      "CorpusId":204838007
    }
  },
  {
    "paper_id":"668db48c6a79826456341680ee1175dfc4cced71",
    "title":"Get To The Point: Summarization with Pointer-Generator Networks",
    "authors":[
      "A. See",
      "Peter J. Liu",
      "Christopher D. Manning"
    ],
    "abstract":"Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN \/ Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2017,
    "publication_date":"2017-04-01",
    "citation_count":4213,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2952913664",
      "DBLP":"journals\/corr\/SeeLM17",
      "ArXiv":"1704.04368",
      "ACL":"P17-1099",
      "DOI":"10.18653\/v1\/P17-1099",
      "CorpusId":8314118
    }
  },
  {
    "paper_id":"039f82bdfac8aef61f64c3dfa4dc54ac75e418b2",
    "title":"ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
    "authors":[
      "Zheheng Luo",
      "Qianqian Xie",
      "S. Ananiadou"
    ],
    "abstract":"The performance of text summarization has been greatly boosted by pre-trained language models. A main concern of existing methods is that most generated summaries are not factually inconsistent with their source documents. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference, question answering, and syntactic dependency et al. However, these approaches are limited by either their high computational complexity or the uncertainty introduced by multi-component pipelines, resulting in only partial agreement with human judgement. Most recently, large language models(LLMs) have shown excellent performance in not only text generation but also language comprehension. In this paper, we particularly explore ChatGPT's ability to evaluate factual inconsistency under a zero-shot setting by examining it on both coarse-grained and fine-grained evaluation tasks including binary entailment inference, summary ranking, and consistency rating. Experimental results indicate that ChatGPT generally outperforms previous evaluation metrics across the three tasks, indicating its great potential for factual inconsistency evaluation. However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.",
    "venue":"",
    "year":2023,
    "publication_date":"2023-03-27",
    "citation_count":94,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2303.15621",
      "CorpusId":258108400
    }
  },
  {
    "paper_id":"daaa7d4ffb9265226e4baadd2db9a01aa7b2f6fb",
    "title":"Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts",
    "authors":[
      "Dave Van Veen",
      "Cara Van Uden",
      "Louis Blankemeier",
      "Jean-Benoit Delbrouck",
      "Asad Aali",
      "Christian Bluethgen",
      "Anuj Pareek",
      "Malgorzata Polacin",
      "William Collins",
      "Neera Ahuja",
      "C. Langlotz",
      "Jason Hom",
      "S. Gatidis",
      "John M. Pauly",
      "Akshay S. Chaudhari"
    ],
    "abstract":"Sifting through vast textual data and summarizing key information from electronic health records (EHR) imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy on a diverse range of clinical summarization tasks has not yet been rigorously demonstrated. In this work, we apply domain adaptation methods to eight LLMs, spanning six datasets and four distinct clinical summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not improve results. Further, in a clinical reader study with ten physicians, we show that summaries from our best-adapted LLMs are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis highlights challenges faced by both LLMs and human experts. Lastly, we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences. Our research marks the first evidence of LLMs outperforming human experts in clinical text summarization across multiple tasks. This implies that integrating LLMs into clinical workflows could alleviate documentation burden, empowering clinicians to focus more on personalized patient care and the inherently human aspects of medicine.",
    "venue":"Research Square",
    "year":2023,
    "publication_date":"2023-10-30",
    "citation_count":135,
    "reference_count":94,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"10635391",
      "DBLP":"journals\/corr\/abs-2309-07430",
      "DOI":"10.21203\/rs.3.rs-3483777\/v1",
      "CorpusId":268419938,
      "PubMed":"37961377"
    }
  },
  {
    "paper_id":"de17a66e6fb2bd2725c71ae1640f4a0c8faaea73",
    "title":"SummIt: Iterative Text Summarization via ChatGPT",
    "authors":[
      "Haopeng Zhang",
      "Xiao Liu",
      "Jiawei Zhang"
    ],
    "abstract":"Text summarization systems have made significant progress in recent years, but typically generate summaries in one single step. However, the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests. This paper addresses this limitation by proposing SummIt, an iterative text summarization framework based on large language models like ChatGPT. Our framework enables the model to refine the generated summary iteratively through self-evaluation and feedback, resembling humans' iterative process when drafting and revising summaries. Furthermore, we explore the potential benefits of integrating knowledge and topic extractors into the framework to enhance summary faithfulness and controllability. We automatically evaluate the performance of our framework on three benchmark summarization datasets. We also conduct a human evaluation to validate the effectiveness of the iterative refinements and identify a potential issue of over-correction.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-24",
    "citation_count":84,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/0011LZ23a",
      "ArXiv":"2305.14835",
      "DOI":"10.48550\/arXiv.2305.14835",
      "CorpusId":258865306
    }
  },
  {
    "paper_id":"4d74a5048b884e8bb3842240abf98915c619c8f8",
    "title":"Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
    "authors":[
      "Sameer Jain",
      "Vaishakh Keshava",
      "Swarnashree Mysore Sathyendra",
      "Patrick Fernandes",
      "Pengfei Liu",
      "Graham Neubig",
      "Chunting Zhou"
    ],
    "abstract":"Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-06-01",
    "citation_count":50,
    "reference_count":20,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2306.01200",
      "DBLP":"conf\/acl\/JainKSF0NZ23",
      "DOI":"10.18653\/v1\/2023.findings-acl.537",
      "CorpusId":259064002
    }
  },
  {
    "paper_id":"fe50667e1bea4c6f63909b90986231240818c1d6",
    "title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer",
    "authors":[
      "Dongqi Liu",
      "Vera Demberg"
    ],
    "abstract":"ChatGPT Analysis",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-06-13",
    "citation_count":71,
    "reference_count":86,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2306-07799",
      "ArXiv":"2306.07799",
      "ACL":"2023.acl-srw.1",
      "DOI":"10.48550\/arXiv.2306.07799",
      "CorpusId":259145296
    }
  },
  {
    "paper_id":"a7a0d43496589eb74e276888900cbdcbaa790e8a",
    "title":"Exploring the Landscape of Automatic Text Summarization: A Comprehensive Survey",
    "authors":[
      "Bilal Khan",
      "Zohaib Ali Shah",
      "Muhammad Usman",
      "Inayat Khan",
      "Badam Niazi"
    ],
    "abstract":"The discipline of Automatic Text Summarization (ATS), which is expanding quickly, intends to automatically create summaries of enormous amounts of text so that readers can save time and effort. ATS is a rapidly growing field that aims to save readers time and effort by automatically generating summaries of large volumes of text. In recent years, significant advancements have been witnessed in this area, accompanied by challenges that have spurred extensive research. The proliferation of textual data has sparked substantial interest in ATS, which is thoroughly examined in this survey study. Researchers have been refining ATS techniques since the 1950s, primarily categorized as extractive, abstractive, or hybrid approaches. In the extractive approach, key sentences are extracted from the source document(s) and combined to form the summary, while the abstractive approach employs an intermediary representation of the input document(s) to generate a summary that may differ from the original text. Hybrid approaches combine elements of both extractive and abstractive methods. Despite various recommended methodologies, the generated summaries still exhibit noticeable differences compared to those created by humans. This research survey offers an inclusive exploration of ATS, covering its challenges, types, classifications, approaches, applications, methods, implementations, processing and preprocessing techniques, linguistic analysis, datasets, and evaluation measures, catering to the needs of researchers in the field.",
    "venue":"IEEE Access",
    "year":2023,
    "publication_date":null,
    "citation_count":24,
    "reference_count":122,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/KhanSUKN23",
      "DOI":"10.1109\/ACCESS.2023.3322188",
      "CorpusId":263713879
    }
  },
  {
    "paper_id":"7cf06701bb513e8f2a31fcea0cec6daa9926398b",
    "title":"Long Text and Multi-Table Summarization: Dataset and Method",
    "authors":[
      "Shuaiqi Liu",
      "Jiannong Cao",
      "Ruosong Yang",
      "Zhiyuan Wen"
    ],
    "abstract":"Automatic document summarization aims to produce a concise summary covering the input document's salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries' informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and multiple tables in each report. To deal with the scarcity of available data, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, it has two subsets for summarizing each company's results of operations and liquidity. To summarize the long text and dozens of tables in each report, we present three types of summarization methods. Besides, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Dataset analyses and experimental results indicate the importance of jointly considering input textual and tabular data when summarizing report documents.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-02-08",
    "citation_count":27,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2302.03815",
      "DBLP":"conf\/emnlp\/00020YW22",
      "DOI":"10.48550\/arXiv.2302.03815",
      "CorpusId":256631057
    }
  },
  {
    "paper_id":"44fca068eecce2203d111213e3691647914a3945",
    "title":"LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
    "authors":[
      "Günes Erkan",
      "Dragomir R. Radev"
    ],
    "abstract":"We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
    "venue":"Journal of Artificial Intelligence Research",
    "year":2004,
    "publication_date":"2004-07-01",
    "citation_count":3165,
    "reference_count":41,
    "fields_of_study":[
      "Mathematics",
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1109.2128",
      "DBLP":"journals\/jair\/ErkanR04",
      "MAG":"2110693578",
      "DOI":"10.1613\/jair.1523",
      "CorpusId":506350
    }
  },
  {
    "paper_id":"c1799bf28d1ae93e1631be5b59196ee1e568f538",
    "title":"From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
    "authors":[
      "Darren Edge",
      "Ha Trinh",
      "Newman Cheng",
      "Joshua Bradley",
      "Alex Chao",
      "Apurva N. Mody",
      "Steven Truitt",
      "Jonathan Larson"
    ],
    "abstract":"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and\/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as\"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-04-24",
    "citation_count":811,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2404.16130",
      "DBLP":"journals\/corr\/abs-2404-16130",
      "CorpusId":269363075
    }
  },
  {
    "paper_id":"0c5598424cc96d8fb500eb553cb7969f86a0ede0",
    "title":"Evaluating the Factual Consistency of Abstractive Text Summarization",
    "authors":[
      "Wojciech Kryscinski",
      "Bryan McCann",
      "Caiming Xiong",
      "R. Socher"
    ],
    "abstract":"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-10-28",
    "citation_count":817,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2981456979",
      "ArXiv":"1910.12840",
      "DBLP":"journals\/corr\/abs-1910-12840",
      "ACL":"2020.emnlp-main.750",
      "DOI":"10.18653\/v1\/2020.emnlp-main.750",
      "CorpusId":204976362
    }
  },
  {
    "paper_id":"929b4775b6896634e11a8feb0ca4ca64ef7b3e24",
    "title":"Extractive Summarization as Text Matching",
    "authors":[
      "Ming Zhong",
      "Pengfei Liu",
      "Yiran Chen",
      "Danqing Wang",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "abstract":"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN\/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https:\/\/github.com\/maszhongming\/MatchSum.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-04-19",
    "citation_count":483,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2004-08795",
      "ACL":"2020.acl-main.552",
      "ArXiv":"2004.08795",
      "MAG":"3017113204",
      "DOI":"10.18653\/v1\/2020.acl-main.552",
      "CorpusId":215828313
    }
  },
  {
    "paper_id":"93d70149a47b78b0fb1468c8d75565d5e2db58f9",
    "title":"T-BERTSum: Topic-Aware Text Summarization Based on BERT",
    "authors":[
      "Tinghuai Ma",
      "Qian Pan",
      "Huan Rong",
      "Yurong Qian",
      "Y. Tian",
      "N. Al-Nabhan"
    ],
    "abstract":"In the era of social networks, the rapid growth of data mining in information retrieval and natural language processing makes automatic text summarization necessary. Currently, pretrained word embedding and sequence to sequence models can be effectively adapted in social network summarization to extract significant information with strong encoding capability. However, how to tackle the long text dependence and utilize the latent topic mapping has become an increasingly crucial challenge for these models. In this article, we propose a topic-aware extractive and abstractive summarization model named T-BERTSum, based on Bidirectional Encoder Representations from Transformers (BERTs). This is an improvement over previous models, in which the proposed approach can simultaneously infer topics and generate summarization from social texts. First, the encoded latent topic representation, through the neural topic model (NTM), is matched with the embedded representation of BERT, to guide the generation with the topic. Second, the long-term dependencies are learned through the transformer network to jointly explore topic inference and text summarization in an end-to-end manner. Third, the long short-term memory (LSTM) network layers are stacked on the extractive model to capture sequence timing information, and the effective information is further filtered on the abstractive model through a gated network. In addition, a two-stage extractive–abstractive model is constructed to share the information. Compared with the previous work, the proposed model T-BERTSum focuses on pretrained external knowledge and topic mining to capture more accurate contextual representations. Experimental results on the CNN\/Daily mail and XSum datasets demonstrate that our proposed model achieves new state-of-the-art results while generating consistent topics compared with the most advanced method.",
    "venue":"IEEE Transactions on Computational Social Systems",
    "year":2022,
    "publication_date":"2022-06-01",
    "citation_count":80,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tcss\/MaPRQTA22",
      "MAG":"3175875420",
      "DOI":"10.1109\/tcss.2021.3088506",
      "CorpusId":237976609
    }
  },
  {
    "paper_id":"b7564f9d6c8ba0b6a2fa52218b0be0e9f1ee4a7f",
    "title":"Automatic Text Summarization Methods: A Comprehensive Review",
    "authors":[
      "Divakar Yadav",
      "Jalpa J Desai",
      "A. Yadav"
    ],
    "abstract":"One of the most pressing issues that have arisen due to the rapid growth of the Internet is known as information overloading. Simplifying the relevant information in the form of a summary will assist many people because the material on any topic is plentiful on the Internet. Manually summarising massive amounts of text is quite challenging for humans. So, it has increased the need for more complex and powerful summarizers. Researchers have been trying to improve approaches for creating summaries since the 1950s, such that the machine-generated summary matches the human-created summary. This study provides a detailed state-of-the-art analysis of text summarization concepts such as summarization approaches, techniques used, standard datasets, evaluation metrics and future scopes for research. The most commonly accepted approaches are extractive and abstractive, studied in detail in this work. Evaluating the summary and increasing the development of reusable resources and infrastructure aids in comparing and replicating findings, adding competition to improve the outcomes. Different evaluation methods of generated summaries are also discussed in this study. Finally, at the end of this study, several challenges and research opportunities related to text summarization research are mentioned that may be useful for potential researchers working in this area.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-03-03",
    "citation_count":67,
    "reference_count":102,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2204-01849",
      "ArXiv":"2204.01849",
      "DOI":"10.48550\/arXiv.2204.01849",
      "CorpusId":247958026
    }
  },
  {
    "paper_id":"e58edbeb41f3d2d24832e6e3abb94baac754e3f7",
    "title":"Re-evaluating Evaluation in Text Summarization",
    "authors":[
      "Manik Bhandari",
      "Pranav Narayan Gour",
      "Atabak Ashfaq",
      "Pengfei Liu",
      "Graham Neubig"
    ],
    "abstract":"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-10-14",
    "citation_count":193,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2010-07100",
      "ArXiv":"2010.07100",
      "MAG":"3101017384",
      "ACL":"2020.emnlp-main.751",
      "DOI":"10.18653\/v1\/2020.emnlp-main.751",
      "CorpusId":222341867
    }
  },
  {
    "paper_id":"aa4c05b46d5813b5e46bdea3c1531c02a3302afd",
    "title":"BART-IT: An Efficient Sequence-to-Sequence Model for Italian Text Summarization",
    "authors":[
      "Moreno La Quatra",
      "Luca Cagliero"
    ],
    "abstract":"The emergence of attention-based architectures has led to significant improvements in the performance of neural sequence-to-sequence models for text summarization. Although these models have proved to be effective in summarizing English-written documents, their portability to other languages is limited thus leaving plenty of room for improvement. In this paper, we present BART-IT, a sequence-to-sequence model, based on the BART architecture that is specifically tailored to the Italian language. The model is pre-trained on a large corpus of Italian-written pieces of text to learn language-specific features and then fine-tuned on several benchmark datasets established for abstractive summarization. The experimental results show that BART-IT outperforms other state-of-the-art models in terms of ROUGE scores in spite of a significantly smaller number of parameters. The use of BART-IT can foster the development of interesting NLP applications for the Italian language. Beyond releasing the model to the research community to foster further research and applications, we also discuss the ethical implications behind the use of abstractive summarization models.",
    "venue":"Future Internet",
    "year":2022,
    "publication_date":"2022-12-27",
    "citation_count":65,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/fi\/QuatraC23",
      "DOI":"10.3390\/fi15010015",
      "CorpusId":255251442
    }
  },
  {
    "paper_id":"81b147612bef52109f1dd3208d9c27ebdee1b406",
    "title":"TEXT SUMMARIZATION",
    "authors":[
      "M. D",
      "R. V",
      "Dhanush Kannan A",
      "Kavya B",
      "S. S",
      "Swetha Srinivasan"
    ],
    "abstract":"n the last few years, a huge amount of text data from different sources has been created every day. The enormous data which needs to be processed contains valuable detail which needs to be efficiently summarized so that it serves a purpose. It is very tedious to summarize and classify large amounts of documents when done manually. It becomes cumbersome to develop a summary taking every semantics into consideration. Therefore, automatic text summarization acts as a solution. Text summarization can help in understanding the huge corpus by providing a gist of the corpus enabling comprehension in a timely manner. This paper studies the development of a web application which summarizes the given input text using different models and its deployment. Keywords: Text summarization, NLP, AWS, Text mining",
    "venue":"YMER Digital",
    "year":2022,
    "publication_date":"2022-07-07",
    "citation_count":63,
    "reference_count":17,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.31979\/etd.7zwf-n6w5",
      "CorpusId":19735683
    }
  },
  {
    "paper_id":"fa61061e13b087f151f1e85e1565f8b0a0fc8b97",
    "title":"A Comprehensive Review of Arabic Text Summarization",
    "authors":[
      "Asmaa H. Elsaid",
      "Ammar Mohammed",
      "L. F. Ibrahim",
      "M. Sakre"
    ],
    "abstract":"The explosion of online and offline data has changed how we gather, evaluate, and understand data. It is frequently difficult and time-consuming to comprehend large text documents and extract crucial information from them. Text summarization techniques address the mentioned problems by compressing long texts while retaining their essential contents. These techniques rely on the fast delivery of filtered, high-quality content to their users. Due to the massive amounts of data generated by technology and various sources, automated text summarization of large-scale data is challenging. There are three types of automatic text summarization techniques: extractive, abstractive, and hybrid. Regardless of these previous techniques, the generated summaries are a long way from the summarization produced by human experts. Although Arabic is a widely spoken language that is frequently used for content sharing on the web, Arabic text summarization of Arabic content is limited and still immature because of several problems, including the Arabic language’s morphological structure, the variety of dialects, and the lack of adequate data sources. This paper reviews text summarization approaches and recent deep learning models for this approach. Additionally, it focuses on existing datasets for these approaches, which are also reviewed, along with their characteristics and limitations. The most often used metrics for summarization quality evaluation are ROUGE1, ROUGE2, ROUGE L, and Bleu. The challenges that are encountered during Arabic text summarizing methods and approaches and the solutions proposed in each approach are analyzed. Many Arabic text summarization methods have problems, such as the lack of golden tokens during testing, being out of vocabulary (OOV) words, repeating summary sentences, lack of standard systematic methodologies and architectures, and the complexity of the Arabic language. Finally, providing the required corpora, improving evaluation using semantic representations, the lack of using rouge metrics in abstractive text summarization, and using recent deep learning models to adopt them in Arabic summarization studies is an essential demand.",
    "venue":"IEEE Access",
    "year":2022,
    "publication_date":null,
    "citation_count":59,
    "reference_count":92,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/ElsaidMIS22",
      "DOI":"10.1109\/ACCESS.2022.3163292",
      "CorpusId":247874914
    }
  },
  {
    "paper_id":"24b951275a7a42ef36aca8352caaf6f4cd6238d2",
    "title":"HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information",
    "authors":[
      "Qianqian Ruan",
      "Malte Ostendorff",
      "Georg Rehm"
    ],
    "abstract":"Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN\/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model’s SOTA performance.",
    "venue":"Findings",
    "year":2022,
    "publication_date":"2022-03-17",
    "citation_count":59,
    "reference_count":28,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2203.09629",
      "ACL":"2022.findings-acl.102",
      "DBLP":"conf\/acl\/RuanOR22",
      "DOI":"10.48550\/arXiv.2203.09629",
      "CorpusId":247594288
    }
  },
  {
    "paper_id":"a9f12a5ec4c579dfac52c794ea380473290fea8e",
    "title":"Abstractive Arabic Text Summarization Based on Deep Learning",
    "authors":[
      "Y. Wazery",
      "Marwa E. Saleh",
      "Abdullah I. Alharbi",
      "Abdelmgeid A. Ali"
    ],
    "abstract":"Text summarization (TS) is considered one of the most difficult tasks in natural language processing (NLP). It is one of the most important challenges that stand against the modern computer system's capabilities with all its new improvement. Many papers and research studies address this task in literature but are being carried out in extractive summarization, and few of them are being carried out in abstractive summarization, especially in the Arabic language due to its complexity. In this paper, an abstractive Arabic text summarization system is proposed, based on a sequence-to-sequence model. This model works through two components, encoder and decoder. Our aim is to develop the sequence-to-sequence model using several deep artificial neural networks to investigate which of them achieves the best performance. Different layers of Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM), and Bidirectional Long Short-Term Memory (BiLSTM) have been used to develop the encoder and the decoder. In addition, the global attention mechanism has been used because it provides better results than the local attention mechanism. Furthermore, AraBERT preprocess has been applied in the data preprocessing stage that helps the model to understand the Arabic words and achieves state-of-the-art results. Moreover, a comparison between the skip-gram and the continuous bag of words (CBOW) word2Vec word embedding models has been made. We have built these models using the Keras library and run-on Google Colab Jupiter notebook to run seamlessly. Finally, the proposed system is evaluated through ROUGE-1, ROUGE-2, ROUGE-L, and BLEU evaluation metrics. The experimental results show that three layers of BiLSTM hidden states at the encoder achieve the best performance. In addition, our proposed system outperforms the other latest research studies. Also, the results show that abstractive summarization models that use the skip-gram word2Vec model outperform the models that use the CBOW word2Vec model.",
    "venue":"Computational Intelligence and Neuroscience",
    "year":2022,
    "publication_date":"2022-01-11",
    "citation_count":61,
    "reference_count":35,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "PubMedCentral":"8767398",
      "DBLP":"journals\/cin\/WazerySAA22",
      "DOI":"10.1155\/2022\/1566890",
      "CorpusId":246063308,
      "PubMed":"35069714"
    }
  },
  {
    "paper_id":"ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
    "title":"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
    "authors":[
      "Shankar Kanthara",
      "Rixie Tiffany Ko Leong",
      "Xiang Lin",
      "Ahmed Masry",
      "Megh Thakkar",
      "Enamul Hoque",
      "Shafiq R. Joty"
    ],
    "abstract":"Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-03-12",
    "citation_count":191,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2203-06486",
      "ACL":"2022.acl-long.277",
      "ArXiv":"2203.06486",
      "DOI":"10.48550\/arXiv.2203.06486",
      "CorpusId":247446806
    }
  },
  {
    "paper_id":"d4388cd51d7e6bbe987e4d7b494fbae01ae0c30c",
    "title":"A Comprehensive Survey of Abstractive Text Summarization Based on Deep Learning",
    "authors":[
      "Mengli Zhang",
      "Gang Zhou",
      "Wanting Yu",
      "Ningbo Huang",
      "Wenfen Liu"
    ],
    "abstract":"With the rapid development of the Internet, the massive amount of web textual data has grown exponentially, which has brought considerable challenges to downstream tasks, such as document management, text classification, and information retrieval. Automatic text summarization (ATS) is becoming an extremely important means to solve this problem. The  core of ATS is to mine the gist of the original text and automatically generate a concise and readable summary. Recently, to better balance and develop these two aspects, deep learning (DL)-based abstractive summarization models have been developed. At present, for ATS tasks, almost all state-of-the-art (SOTA) models are based on DL architecture. However, a comprehensive literature survey is still lacking in the field of DL-based abstractive text summarization. To fill this gap, this paper provides researchers with a comprehensive survey of DL-based abstractive summarization. We first give an overview of abstractive summarization and DL. Then, we summarize several typical frameworks of abstractive summarization. After that, we also give a comparison of several popular datasets that are commonly used for training, validation, and testing. We further analyze the performance of several typical abstractive summarization systems on common datasets. Finally, we highlight some open challenges in the abstractive summarization task and outline some future research trends. We hope that these explorations will provide researchers with new insights into DL-based abstractive summarization.",
    "venue":"Computational Intelligence and Neuroscience",
    "year":2022,
    "publication_date":"2022-08-01",
    "citation_count":47,
    "reference_count":148,
    "fields_of_study":[
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"9359827",
      "DOI":"10.1155\/2022\/7132226",
      "CorpusId":251270540,
      "PubMed":"35958768"
    }
  },
  {
    "paper_id":"8f4eaabaecc7ab39e127f2de30103b5a08dd6041",
    "title":"Performance Study on Extractive Text Summarization Using BERT Models",
    "authors":[
      "Shehab Abdel-Salam",
      "Ahmed Rafea"
    ],
    "abstract":"The task of summarization can be categorized into two methods, extractive and abstractive. Extractive summarization selects the salient sentences from the original document to form a summary while abstractive summarization interprets the original document and generates the summary in its own words. The task of generating a summary, whether extractive or abstractive, has been studied with different approaches in the literature, including statistical-, graph-, and deep learning-based approaches. Deep learning has achieved promising performances in comparison to the classical approaches, and with the advancement of different neural architectures such as the attention network (commonly known as the transformer), there are potential areas of improvement for the summarization task. The introduction of transformer architecture and its encoder model “BERT” produced an improved performance in downstream tasks in NLP. BERT is a bidirectional encoder representation from a transformer modeled as a stack of encoders. There are different sizes for BERT, such as BERT-base with 12 encoders and BERT-larger with 24 encoders, but we focus on the BERT-base for the purpose of this study. The objective of this paper is to produce a study on the performance of variants of BERT-based models on text summarization through a series of experiments, and propose “SqueezeBERTSum”, a trained summarization model fine-tuned with the SqueezeBERT encoder variant, which achieved competitive ROUGE scores retaining the BERTSum baseline model performance by 98%, with 49% fewer trainable parameters.",
    "venue":"Inf.",
    "year":2022,
    "publication_date":"2022-01-28",
    "citation_count":66,
    "reference_count":29,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/information\/Abdel-SalamR22",
      "DOI":"10.3390\/info13020067",
      "CorpusId":246419424
    }
  },
  {
    "paper_id":"903ce14de7226276a66e1bd3e3d3d97934d2dfd2",
    "title":"Mapping the Design Space of Human-AI Interaction in Text Summarization",
    "authors":[
      "Ruijia Cheng",
      "Alison Smith-Renner",
      "Kecheng Zhang",
      "Joel R. Tetreault",
      "A. Jaimes"
    ],
    "abstract":"Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-06-29",
    "citation_count":39,
    "reference_count":78,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-14863",
      "ACL":"2022.naacl-main.33",
      "ArXiv":"2206.14863",
      "DOI":"10.48550\/arXiv.2206.14863",
      "CorpusId":250144771
    }
  },
  {
    "paper_id":"3b511b45dbc7b3cea4553527029e0bc0ddecacb6",
    "title":"Automatic Text Summarization of Biomedical Text Data: A Systematic Review",
    "authors":[
      "Andrea Chaves",
      "C. Kesiku",
      "B. Garcia-Zapirain"
    ],
    "abstract":"In recent years, the evolution of technology has led to an increase in text data obtained from many sources. In the biomedical domain, text information has also evidenced this accelerated growth, and automatic text summarization systems play an essential role in optimizing physicians’ time resources and identifying relevant information. In this paper, we present a systematic review in recent research of text summarization for biomedical textual data, focusing mainly on the methods employed, type of input data text, areas of application, and evaluation metrics used to assess systems. The survey was limited to the period between 1st January 2014 and 15th March 2022. The data collected was obtained from WoS, IEEE, and ACM digital libraries, while the search strategies were developed with the help of experts in NLP techniques and previous systematic reviews. The four phases of a systematic review by PRISMA methodology were conducted, and five summarization factors were determined to assess the studies included: Input, Purpose, Output, Method, and Evaluation metric. Results showed that 3.5% of 801 studies met the inclusion criteria. Moreover, Single-document, Biomedical Literature, Generic, and Extractive summarization proved to be the most common approaches employed, while techniques based on Machine Learning were performed in 16 studies and Rouge (Recall-Oriented Understudy for Gisting Evaluation) was reported as the evaluation metric in 26 studies. This review found that in recent years, more transformer-based methodologies for summarization purposes have been implemented compared to a previous survey. Additionally, there are still some challenges in text summarization in different domains, especially in the biomedical field in terms of demand for further research.",
    "venue":"Inf.",
    "year":2022,
    "publication_date":"2022-08-19",
    "citation_count":34,
    "reference_count":85,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/information\/ChavesKG22",
      "DOI":"10.3390\/info13080393",
      "CorpusId":251757366
    }
  },
  {
    "paper_id":"06f6869f8eb90a35148f1dde9a4baff03460d699",
    "title":"Entity-level Factual Consistency of Abstractive Text Summarization",
    "authors":[
      "Feng Nan",
      "Ramesh Nallapati",
      "Zhiguo Wang",
      "C. D. Santos",
      "Henghui Zhu",
      "Dejiao Zhang",
      "K. McKeown",
      "Bing Xiang"
    ],
    "abstract":"A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.",
    "venue":"Conference of the European Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-02-18",
    "citation_count":172,
    "reference_count":20,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2102-09130",
      "MAG":"3130250071",
      "ArXiv":"2102.09130",
      "ACL":"2021.eacl-main.235",
      "DOI":"10.18653\/v1\/2021.eacl-main.235",
      "CorpusId":231951460
    }
  },
  {
    "paper_id":"85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
    "title":"Leveraging Locality in Abstractive Text Summarization",
    "authors":[
      "Yixin Liu",
      "Ansong Ni",
      "Linyong Nan",
      "Budhaditya Deb",
      "Chenguang Zhu",
      "A. Awadallah",
      "Dragomir R. Radev"
    ],
    "abstract":"Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-05-25",
    "citation_count":20,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-12476",
      "ACL":"2022.emnlp-main.408",
      "ArXiv":"2205.12476",
      "DOI":"10.48550\/arXiv.2205.12476",
      "CorpusId":249063064
    }
  },
  {
    "paper_id":"8509f1545c3c2b67582dadc3de5b6fb2f2efca42",
    "title":"Abstractive Text Summarization Using BART",
    "authors":[
      "Attada Venkataramana",
      "K. Srividya",
      "R. Cristin"
    ],
    "abstract":"In the last recent years, there's a huge amount of data available on the internet, and is generated very rapidly. It is very difficult for human beings to analyze and extract useful information from huge data especially when the text is large in size and longer documents which increases the time to process and analyze the data, further it also increases the time taken to summarize it. To address this issue automatic text summarization is used. Text summarization is defined as creating a short, accurate, and fluent summary of a longer document. It summarizes the larger text without any human intervention. This paper will provide a mechanism where it does the text summarization quickly and effectively even for large data. We can use this model to summarize and extract important information from a large document or text based on our input. Here a concept of the Deep Learning model is used for text summarization which is called BART (Bidirectional and Auto- Regressive Transformer). it consists of both an encoder and decoder. The encoder and decoder are merged to form the BART algorithm. BERT is trained with a huge amount of unlabeled data to achieve the state of art results. The use of an attention mechanism in each layer of BERT makes the model much more popular as it highlights the important features of the input data. BERT performs the masked language modeling with the help of its several bidirectional transformer layers and predicts the missing values. On the other hand, decoder is used to predict the next token in a sentence. Merging both the encoder and decoder will form the BART model. Here we will compare the BART model with BERT, T5, and Roberta.",
    "venue":"2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)",
    "year":2022,
    "publication_date":"2022-10-16",
    "citation_count":23,
    "reference_count":14,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/MysuruCon55714.2022.9972639",
      "CorpusId":254640790
    }
  },
  {
    "paper_id":"4b2af8e5da894a72f6236ab9347753760cfea7fd",
    "title":"Sequence Level Contrastive Learning for Text Summarization",
    "authors":[
      "Shusheng Xu",
      "Xingxing Zhang",
      "Yi Wu",
      "Furu Wei"
    ],
    "abstract":"Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives. We release our code at https:\/\/github.com\/xssstory\/SeqCo.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-09-08",
    "citation_count":107,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2109.03481",
      "DBLP":"journals\/corr\/abs-2109-03481",
      "DOI":"10.1609\/aaai.v36i10.21409",
      "CorpusId":237439568
    }
  },
  {
    "paper_id":"c2d140865e25bbb5a76e167baf48083cb38066dd",
    "title":"The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey",
    "authors":[
      "Yi-Chong Huang",
      "Xiachong Feng",
      "Xiaocheng Feng",
      "Bing Qin"
    ],
    "abstract":"Recently, various neural encoder-decoder models pioneered by Seq2Seq framework have been proposed to achieve the goal of generating more abstractive summaries by learning to map input text to output text. At a high level, such neural models can freely generate summaries without any constraint on the words or phrases used. Moreover, their format is closer to human-edited summaries and output is more readable and fluent. However, the neural model's abstraction ability is a double-edged sword. A commonly observed problem with the generated summaries is the distortion or fabrication of factual information in the article. This inconsistency between the original text and the summary has caused various concerns over its applicability, and the previous evaluation methods of text summarization are not suitable for this issue. In response to the above problems, the current research direction is predominantly divided into two categories, one is to design fact-aware evaluation metrics to select outputs without factual inconsistency errors, and the other is to develop new summarization systems towards factual consistency. In this survey, we focus on presenting a comprehensive review of these fact-specific evaluation methods and text summarization models.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-04-30",
    "citation_count":116,
    "reference_count":50,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2104-14839",
      "ArXiv":"2104.14839",
      "CorpusId":233476302
    }
  },
  {
    "paper_id":"2f99573bab4db9b8b877a8d8dbfbdab9c02408c0",
    "title":"Text Summarization using Transformer Model",
    "authors":[
      "Jaishree Ranganathan",
      "Gloria Abuka"
    ],
    "abstract":"The increased availability of online feedback or review tools, and the enormous amount of information on these platforms, have made text summarization a vital research area in natural language processing. Instead of potential consumers going through thousands of reviews to get needed information, summarization will enable them to see a concise form of a chunk of reviews with relevant information. News and scientific articles have been used in text summarization models. This study proposes a text summarization method based on the Text-to- Text Transfer Transformer (T5) model. We use the University of California, Irvine's (UCI) drug reviews dataset. We manually created human summaries for the ten most useful reviews of a particular drug for 500 different drugs from the dataset. We fine-tune the Text-to- Text Transfer Transformer (T5) model to perform abstractive text summarization. The model's effectiveness was evaluated using the ROUGE metrics, and our model achieved an average of ROUGE1, ROUGE2, and ROUGEL scores of 45.62, 25.58, and 36.53, respectively. We also fine-tuned this model on a standard dataset(BBC News Dataset) previously used for text summarization and got average ROUGE1, ROUGE2, and ROUGEL scores of 69.05, 59.70, and 52.97, respectively.",
    "venue":"International Conference on Social Networks Analysis, Management and Security",
    "year":2022,
    "publication_date":"2022-11-29",
    "citation_count":19,
    "reference_count":28,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/snams\/RanganathanA22",
      "DOI":"10.1109\/SNAMS58071.2022.10062698",
      "CorpusId":257536408
    }
  },
  {
    "paper_id":"af93e1accba69994cdc36254ef93584af307fd8a",
    "title":"Neural Text Summarization: A Critical Evaluation",
    "authors":[
      "Wojciech Kryscinski",
      "N. Keskar",
      "Bryan McCann",
      "Caiming Xiong",
      "R. Socher"
    ],
    "abstract":"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-08-23",
    "citation_count":381,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2971034336",
      "DBLP":"journals\/corr\/abs-1908-08960",
      "ArXiv":"1908.08960",
      "ACL":"D19-1051",
      "DOI":"10.18653\/v1\/D19-1051",
      "CorpusId":201666437
    }
  },
  {
    "paper_id":"3bcea238b0c323d8f891829714bbe6e8a3de894c",
    "title":"Toward Unifying Text Segmentation and Long Document Summarization",
    "authors":[
      "Sangwoo Cho",
      "Kaiqiang Song",
      "Xiaoyang Wang",
      "Fei Liu",
      "Dong Yu"
    ],
    "abstract":"Text segmentation is important for signaling a document’s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in transcripts of audio\/video recordings. In this paper, we explore the role that section segmentation plays in extractive summarization of written and spoken documents. Our approach learns robust sentence representations by performing summarization and segmentation simultaneously, which is further enhanced by an optimization-based regularizer to promote selection of diverse summary sentences. We conduct experiments on multiple datasets ranging from scientific articles to spoken transcripts to evaluate the model’s performance. Our findings suggest that the model can not only achieve state-of-the-art performance on publicly available benchmarks, but demonstrate better cross-genre transferability when equipped with text segmentation. We perform a series of analyses to quantify the impact of section segmentation on summarizing written and spoken documents of substantial length and complexity.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-10-28",
    "citation_count":38,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/ChoSW0022",
      "ArXiv":"2210.16422",
      "ACL":"2022.emnlp-main.8",
      "DOI":"10.48550\/arXiv.2210.16422",
      "CorpusId":253237148
    }
  },
  {
    "paper_id":"d4e9bf1d5fab52698a3aa48c65b098ea327deea6",
    "title":"CTRLsum: Towards Generic Controllable Text Summarization",
    "authors":[
      "Junxian He",
      "Wojciech Kryscinski",
      "Bryan McCann",
      "Nazneen Rajani",
      "Caiming Xiong"
    ],
    "abstract":"Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time CTRLsum features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of CTRLsum on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, CTRLsum is comparable or better than strong pretrained systems.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-12-08",
    "citation_count":151,
    "reference_count":93,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/HeKMRX22",
      "ACL":"2022.emnlp-main.396",
      "ArXiv":"2012.04281",
      "MAG":"3111372071",
      "DOI":"10.18653\/v1\/2022.emnlp-main.396",
      "CorpusId":227745074
    }
  },
  {
    "paper_id":"5c9cb3ace5d4355035f7a44ea79dd694de1d3c2a",
    "title":"A Survey of Automatic Text Summarization: Progress, Process and Challenges",
    "authors":[
      "M. F. Mridha",
      "Aklima Akter Lima",
      "Prof. Dr. Kamruddin Nur",
      "S. Das",
      "M. Hasan",
      "Muhammad Mohsin Kabir"
    ],
    "abstract":"With the evolution of the Internet and multimedia technology, the amount of text data has increased exponentially. This text volume is a precious source of information and knowledge that needs to be efficiently summarized. Text summarization is the method to reduce the source text into a compact variant, preserving its knowledge and the actual meaning. Here we thoroughly investigate the automatic text summarization (ATS) and summarize the widely recognized ATS architectures. This paper outlines extractive and abstractive text summarization technologies and provides a deep taxonomy of the ATS domain. The taxonomy presents the classical ATS algorithms to modern deep learning ATS architectures. Every modern text summarization approach’s workflow and significance are reviewed with the limitations with potential recovery methods, including the feature extraction approaches, datasets, performance measurement techniques, and challenges of the ATS domain, etc. In addition, this paper concisely presents the past, present, and future research directions in the ATS domain.",
    "venue":"IEEE Access",
    "year":2021,
    "publication_date":null,
    "citation_count":104,
    "reference_count":362,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/MridhaLNDHK21",
      "DOI":"10.1109\/ACCESS.2021.3129786",
      "CorpusId":244510269
    }
  },
  {
    "paper_id":"96c7a2519b41c976dc311ada9353a21daba284a3",
    "title":"A Survey of the State-of-the-Art Models in Neural Abstractive Text Summarization",
    "authors":[
      "A. Syed",
      "F. Gaol",
      "T. Matsuo"
    ],
    "abstract":"Dealing with vast amounts of textual data requires the use of efficient systems. Automatic summarization systems are capable of addressing this issue. Therefore, it becomes highly essential to work on the design of existing automatic summarization systems and innovate them to make them capable of meeting the demands of continuously increasing data, based on user needs. This study tends to survey the scientific literature to obtain information and knowledge about the recent research in automatic text summarization specifically abstractive summarization based on neural networks. A review of various neural networks based abstractive summarization models have been presented. The proposed conceptual framework includes five key elements identified as encoder-decoder architecture, mechanisms, training strategies and optimization algorithms, dataset, and evaluation metric. A description of these elements is also included in this article. The purpose of this research is to provide an overall understanding and familiarity with the elements of recent neural networks based abstractive text summarization models with an up-to-date review as well as to render an awareness of the challenges and issues with these systems. Analysis has been performed qualitatively with the help of a concept matrix indicating common trends in the design of recent neural abstractive summarization systems. Models employing a transformer-based encoder-decoder architecture are found to be the new state-of-the-art. Based on the knowledge acquired from the survey, this article suggests the use of pre-trained language models in complement with neural network architecture for abstractive summarization task.",
    "venue":"IEEE Access",
    "year":2021,
    "publication_date":null,
    "citation_count":74,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/SyedGM21",
      "DOI":"10.1109\/ACCESS.2021.3052783",
      "CorpusId":231715075
    }
  },
  {
    "paper_id":"264f2f04d55051a18ca45ebc06bb051c29c079b6",
    "title":"What Have We Achieved on Text Summarization?",
    "authors":[
      "Dandan Huang",
      "Leyang Cui",
      "Sen Yang",
      "Guangsheng Bao",
      "Kun Wang",
      "Jun Xie",
      "Yue Zhang"
    ],
    "abstract":"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive\/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-10-09",
    "citation_count":114,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3102999512",
      "DBLP":"conf\/emnlp\/HuangCYBWXZ20",
      "ArXiv":"2010.04529",
      "ACL":"2020.emnlp-main.33",
      "DOI":"10.18653\/V1\/2020.EMNLP-MAIN.33",
      "CorpusId":222272417
    }
  },
  {
    "paper_id":"76e37d6e05059f97b3b99db6877f5879810ce817",
    "title":"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation",
    "authors":[
      "Yuexiang Xie",
      "Fei Sun",
      "Yang Deng",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "abstract":"Despite significant progress has been achieved in text summarization, factual inconsistency in generated summaries still severely limits its practical applications. Among the key factors to ensure factual consistency, a reliable automatic evaluation metric is the first and the most crucial one. However, existing metrics either neglect the intrinsic cause of the factual inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation with human judgments or increasing the inconvenience of usage in practice. In light of these challenges, we propose a novel metric to evaluate the factual consistency in text summarization via counterfactual estimation, which formulates the causal relationship among the source document, the generated summary, and the language prior. We remove the effect of language prior, which can cause factual inconsistency, from the total causal effect on the generated summary, and provides a simple yet effective way to evaluate consistency without relying on other auxiliary tasks. We conduct a series of experiments on three public abstractive text summarization datasets, and demonstrate the advantages of the proposed metric in both improving the correlation with human judgments and the convenience of usage. The source code is available at https:\/\/github.com\/xieyxclack\/factual_coco.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-08-30",
    "citation_count":55,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2108.13134",
      "DBLP":"conf\/emnlp\/Xie00LD21",
      "DOI":"10.18653\/v1\/2021.findings-emnlp.10",
      "CorpusId":237353254
    }
  },
  {
    "paper_id":"5e2a141d5bf0452bb78ada4a7c71c29821d8bb14",
    "title":"A systematic review of automatic text summarization for biomedical literature and EHRs",
    "authors":[
      "Mengqian Wang",
      "Manhua Wang",
      "Fei Yu",
      "Yue Yang",
      "Jennifer S. Walker",
      "Javed Mostafa"
    ],
    "abstract":"OBJECTIVE\nBiomedical text summarization helps biomedical information seekers avoid information overload by reducing the length of a document while preserving the contents' essence. Our systematic review investigates the most recent biomedical text summarization researches on biomedical literature and electronic health records by analyzing their techniques, areas of application, and evaluation methods. We identify gaps and propose potential directions for future research.\n\n\nMATERIALS AND METHODS\nThis review followed the PRISMA methodology and replicated the approaches adopted by the previous systematic review published on the same topic. We searched 4 databases (PubMed, ACM Digital Library, Scopus, and Web of Science) from January 1, 2013 to April 8, 2021. Two reviewers independently screened title, abstract, and full-text for all retrieved articles. The conflicts were resolved by the third reviewer. The data extraction of the included articles was in 5 dimensions: input, purpose, output, method, and evaluation.\n\n\nRESULTS\nFifty-eight out of 7235 retrieved articles met the inclusion criteria. Thirty-nine systems used single-document biomedical research literature as their input, 17 systems were explicitly designed for clinical support, 47 systems generated extractive summaries, and 53 systems adopted hybrid methods combining computational linguistics, machine learning, and statistical approaches. As for the assessment, 51 studies conducted an intrinsic evaluation using predefined metrics.\n\n\nDISCUSSION AND CONCLUSION\nThis study found that current biomedical text summarization systems have achieved good performance using hybrid methods. Studies on electronic health records summarization have been increasing compared to a previous survey. However, the majority of the works still focus on summarizing literature.",
    "venue":"J. Am. Medical Informatics Assoc.",
    "year":2021,
    "publication_date":"2021-08-02",
    "citation_count":57,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "DBLP":"journals\/jamia\/WangWYYWM21",
      "DOI":"10.1093\/jamia\/ocab143",
      "CorpusId":236775018,
      "PubMed":"34338801"
    }
  },
  {
    "paper_id":"7e4213efb30cd1ad24b131d1eecd5c8096051a95",
    "title":"Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks",
    "authors":[
      "Peng Cui",
      "Le Hu",
      "Yuanchao Liu"
    ],
    "abstract":"Text summarization aims to compress a textual document to a short summary while keeping salient information. Extractive approaches are widely used in text summarization because of their fluency and efficiency. However, most of existing extractive models hardly capture inter-sentence relationships, particularly in long documents. They also often ignore the effect of topical information on capturing important contents. To address these issues, this paper proposes a graph neural network (GNN)-based extractive summarization model, enabling to capture inter-sentence relationships efficiently via graph-structured document representation. Moreover, our model integrates a joint neural topic model (NTM) to discover latent topics, which can provide document-level features for sentence selection. The experimental results demonstrate that our model not only substantially achieves state-of-the-art results on CNN\/DM and NYT datasets but also considerably outperforms existing approaches on scientific paper datasets consisting of much longer documents, indicating its better robustness in document genres and lengths. Further discussions show that topical information can help the model preselect salient contents from an entire document, which interprets its effectiveness in long document summarization.",
    "venue":"International Conference on Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-13",
    "citation_count":73,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3117801652",
      "DBLP":"journals\/corr\/abs-2010-06253",
      "ArXiv":"2010.06253",
      "ACL":"2020.coling-main.468",
      "DOI":"10.18653\/V1\/2020.COLING-MAIN.468",
      "CorpusId":222310577
    }
  },
  {
    "paper_id":"39a4db6e7b7063f4dcbf8f1564cbe011e58680f5",
    "title":"Extractive Automatic Text Summarization using SpaCy in Python & NLP",
    "authors":[
      "Swaranjali Jugran",
      "Ashish Kumar",
      "Bhupendr Singh Tyagi",
      "V. Anand"
    ],
    "abstract":"Propulsion of the everchanging technological innovations, has led to consider the data generated in the present era very crucial with significant roles both in technical & nontechnical fields. In the digital world, as the amount of data produced at every instance is very huge; there is an ultimate need to develop a machine that can reduce the length of the texts automatically. Moreover, applying text summarization gears up the procedure of researching, reduces reading time, and increases the amount of important information being generated in the specific field. The main agenda is to develop a meaningful and coherent summary to recapitulate highlights of the text. From the collection of fascinating problems, we have opted for the Automatic Text Summarization. The solution to this problem unlike doing manually has proved to be essential in accurately summarizing voluminous texts in a cost and time efficient manner.",
    "venue":"2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)",
    "year":2021,
    "publication_date":"2021-03-04",
    "citation_count":50,
    "reference_count":0,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/icacite51222.2021.9404712",
      "CorpusId":233332193
    }
  },
  {
    "paper_id":"07d8b6e6fb2c842b3f28fddeeb9c0300cc5d0951",
    "title":"Natural Language Processing (NLP) based Text Summarization - A Survey",
    "authors":[
      "Ishitva Awasthi",
      "K. Gupta",
      "Prabjot Singh Bhogal",
      "Sahejpreet Singh Anand",
      "P. Soni"
    ],
    "abstract":"The size of data on the Internet has risen in an exponential manner over the past decade. Thus, the need for a solution emerges, that transforms this vast raw information into useful information which a human brain can understand. One such common technique in research that helps in dealing with enormous data is text summarization. Automatic summarization is a renowned approach which is used to reduce a document to its main ideas. It operates by preserving substantial information by creating a shortened version of the text. Text Summarization is categorized into Extractive and Abstractive methods. Extractive methods of summarization minimize the burden of summarization by choosing from the actual text a subset of sentences that are relevant. Although there are a ton of methods, researchers specializing in Natural Language Processing (NLP) are particularly drawn to extractive methods. Based on linguistic and statistical characteristics, the implications of sentences are calculated. A study of extractive and abstract methods for summarizing texts has been made in this paper. This paper also analyses above mentioned methods which yields a less repetitive and a more concentrated summary.",
    "venue":"International Congress on Information and Communication Technology",
    "year":2021,
    "publication_date":"2021-01-20",
    "citation_count":51,
    "reference_count":36,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICICT50816.2021.9358703",
      "CorpusId":232072212
    }
  },
  {
    "paper_id":"7e0c7fdad758482375cb89a110b2f5ad4bee57dd",
    "title":"Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
    "authors":[
      "Md Tahmid Rahman Laskar",
      "Enamul Hoque",
      "J. Huang"
    ],
    "abstract":"Abstract The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.",
    "venue":"Computational Linguistics",
    "year":2021,
    "publication_date":"2021-12-22",
    "citation_count":48,
    "reference_count":95,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/coling\/LaskarHH22",
      "ArXiv":"2112.11670",
      "DOI":"10.1162\/coli_a_00434",
      "CorpusId":245385261
    }
  },
  {
    "paper_id":"762baed866a8f23e19ea52f265c9ba7f353896ce",
    "title":"Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2",
    "authors":[
      "V. Kieuvongngam",
      "Bowen Tan",
      "Yiming Niu"
    ],
    "abstract":"With the COVID-19 pandemic, there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus-related literature. As a result, the COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications. Here, we take advantage of the recent advances in pre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by performing text summarization on this dataset. We evaluate the results using ROUGE scores and visual inspection. Our model provides abstractive and comprehensive information based on keywords extracted from the original articles. Our work can help the the medical community, by providing succinct summaries of articles for which the abstract are not already available.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-06-03",
    "citation_count":104,
    "reference_count":17,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2006.01997",
      "MAG":"3033544963",
      "DBLP":"journals\/corr\/abs-2006-01997",
      "CorpusId":219260480
    }
  },
  {
    "paper_id":"84a9b5af6b5b3c259dffa4ebb7c13e27e342a63b",
    "title":"Multiplex Graph Neural Network for Extractive Text Summarization",
    "authors":[
      "Baoyu Jing",
      "Zeyu You",
      "Tao Yang",
      "Wei Fan",
      "Hanghang Tong"
    ],
    "abstract":"Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN\/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-08-29",
    "citation_count":45,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2108-12870",
      "ACL":"2021.emnlp-main.11",
      "ArXiv":"2108.12870",
      "DOI":"10.18653\/v1\/2021.emnlp-main.11",
      "CorpusId":237353135
    }
  },
  {
    "paper_id":"77c5ebea69b0c11653d79c2c721e456d6bcc8b3a",
    "title":"D2S: Document-to-Slide Generation Via Query-Based Text Summarization",
    "authors":[
      "Edward Sun",
      "Yufang Hou",
      "Dakuo Wang",
      "Yunfeng Zhang",
      "N. Wang"
    ],
    "abstract":"Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years’ NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-05-08",
    "citation_count":44,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2105-03664",
      "ACL":"2021.naacl-main.111",
      "ArXiv":"2105.03664",
      "MAG":"3169613483",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.111",
      "CorpusId":234334020
    }
  },
  {
    "paper_id":"1c4839c9e8163afc53831c205cbcbd3310e8cd90",
    "title":"Dual Encoding for Abstractive Text Summarization",
    "authors":[
      "Kaichun Yao",
      "Libo Zhang",
      "Dawei Du",
      "Tiejian Luo",
      "Lili Tao",
      "Y. Wu"
    ],
    "abstract":"Recurrent neural network-based sequence-to-sequence attentional models have proven effective in abstractive text summarization. In this paper, we model abstractive text summarization using a dual encoding model. Different from the previous works only using a single encoder, the proposed method employs a dual encoder including the primary and the secondary encoders. Specifically, the primary encoder conducts coarse encoding in a regular way, while the secondary encoder models the importance of words and generates more fine encoding based on the input raw text and the previously generated output text summarization. The two level encodings are combined and fed into the decoder to generate more diverse summary that can decrease repetition phenomenon for long sequence generation. The experimental results on two challenging datasets (i.e., CNN\/DailyMail and DUC 2004) demonstrate that our dual encoding model performs against existing methods.",
    "venue":"IEEE Transactions on Cybernetics",
    "year":2020,
    "publication_date":"2020-03-01",
    "citation_count":80,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "DBLP":"journals\/tcyb\/YaoZDLTW20",
      "MAG":"2899426803",
      "DOI":"10.1109\/TCYB.2018.2876317",
      "CorpusId":53208305,
      "PubMed":"30403646"
    }
  },
  {
    "paper_id":"c6a712f98be7f0a9957c373fafa4b2fcfe4d661b",
    "title":"Enriching and Controlling Global Semantics for Text Summarization",
    "authors":[
      "Thong Nguyen",
      "A. Luu",
      "Truc Lu",
      "T. Quan"
    ],
    "abstract":"Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN\/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-09-22",
    "citation_count":38,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2109-10616",
      "ArXiv":"2109.10616",
      "ACL":"2021.emnlp-main.744",
      "DOI":"10.18653\/v1\/2021.emnlp-main.744",
      "CorpusId":237592636
    }
  },
  {
    "paper_id":"20febc38ef9187f2e288f25530e95012dfc1eab4",
    "title":"Enhancements of Attention-Based Bidirectional LSTM for Hybrid Automatic Text Summarization",
    "authors":[
      "Jiawen Jiang",
      "Haiyang Zhang",
      "Chenxu Dai",
      "Qingjuan Zhao",
      "Hao Feng",
      "Zhanlin Ji",
      "Ivan Ganchev"
    ],
    "abstract":"The automatic generation of a text summary is a task of generating a short summary for a relatively long text document by capturing its key information. In the past, supervised statistical machine learning was widely used for the Automatic Text Summarization (ATS) task, but due to its high dependence on the quality of text features, the generated summaries lack accuracy and coherence, while the computational power involved, and performance achieved, could not easily meet the current needs. This paper proposes four novel ATS models with a Sequence-to-Sequence (Seq2Seq) structure, utilizing an attention-based bidirectional Long Short-Term Memory (LSTM), with added enhancements for increasing the correlation between the generated text summary and the source text, and solving the problem of out-of-vocabulary (OOV) words, suppressing the repeated words, and preventing the spread of cumulative errors in generated text summaries. Experiments conducted on two public datasets confirmed that the proposed ATS models achieve indeed better performance than the baselines and some of the state-of-the-art models considered.",
    "venue":"IEEE Access",
    "year":2021,
    "publication_date":null,
    "citation_count":39,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/JiangZDZFJG21",
      "DOI":"10.1109\/ACCESS.2021.3110143",
      "CorpusId":237520837
    }
  },
  {
    "paper_id":"37e06f3622c17dc6194b547c944462b2a513b878",
    "title":"Multi-Fact Correction in Abstractive Text Summarization",
    "authors":[
      "Yue Dong",
      "Shuohang Wang",
      "Zhe Gan",
      "Yu Cheng",
      "J. Cheung",
      "Jingjing Liu"
    ],
    "abstract":"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-10-01",
    "citation_count":125,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2010.02443",
      "MAG":"3092054955",
      "DBLP":"conf\/emnlp\/DongWGCCL20",
      "ACL":"2020.emnlp-main.749",
      "DOI":"10.18653\/v1\/2020.emnlp-main.749",
      "CorpusId":222142312
    }
  },
  {
    "paper_id":"bbb8a88be9b7b6f24e39aa48c6352ce1fb7826fb",
    "title":"Method Of Text Summarization Using Lsa And Sentence Based Topic Modelling With Bert",
    "authors":[
      "Hritvik Gupta",
      "Mayank Patel"
    ],
    "abstract":"Document summarization is one such task of the natural language processing which deals with the long textual data to make its concise and fluent summaries that contains all of document relevant information. The Branch of NLP that deals with it, is automatic text summarizer. Automatic text summarizer does the task of converting the long textual document into short fluent summaries. There are generally two ways of summarizing text using automatic text summarizer, first is using extractive text summarizer and another abstractive text summarizer. This paper has demonstrated an experiment in contrast with the extractive text summarizer for summarizing the text. On the other hand topic modelling is a NLP task that extracts the relevant topic from the textual document. One such method is Latent semantic Analysis (LSA) using truncated SVD which extracts all the relevant topics from the text. This paper has demonstrated the experiment in which the proposed research work will be summarizing the long textual document using LSA topic modelling along with TFIDF keyword extractor for each sentence in a text document and also using BERT encoder model for encoding the sentences from textual document in order to retrieve the positional embedding of topics word vectors. The algorithm proposed algorithm in this paper is able to achieve the score greater than that of text summarization using Latent Dirichlet Allocation (LDA) topic modelling.",
    "venue":"International Conference on Adaptive and Intelligent Systems",
    "year":2021,
    "publication_date":"2021-03-25",
    "citation_count":37,
    "reference_count":29,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1109\/ICAIS50930.2021.9395976",
      "CorpusId":233263810
    }
  },
  {
    "paper_id":"76037594f29a663fbd2799de2e5c7463c02a8a1d",
    "title":"Discourse-Aware Neural Extractive Text Summarization",
    "authors":[
      "Jiacheng Xu",
      "Zhe Gan",
      "Yu Cheng",
      "Jingjing Liu"
    ],
    "abstract":"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-10-30",
    "citation_count":288,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1910.14142",
      "MAG":"3019713789",
      "DBLP":"conf\/acl\/XuGCL20",
      "ACL":"2020.acl-main.451",
      "DOI":"10.18653\/v1\/2020.acl-main.451",
      "CorpusId":219036690
    }
  },
  {
    "paper_id":"17ce56c9acc5f9c7d2d2a98a4aa5df72e55b2083",
    "title":"Leveraging BERT for Extractive Text Summarization on Lectures",
    "authors":[
      "Derek Miller"
    ],
    "abstract":"In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called Lecture Summarization Service, a python based RESTful service that utilizes the BERT model for text embeddings and KMeans clustering to identify sentences closes to the centroid for summary selection. The purpose of the service was to provide students a utility that could summarize lecture content, based on their desired number of sentences. On top of the summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive summarization were promising, there were still areas where the model struggled, providing feature research opportunities for further improvement.",
    "venue":"arXiv.org",
    "year":2019,
    "publication_date":"2019-06-07",
    "citation_count":259,
    "reference_count":15,
    "fields_of_study":[
      "Computer Science",
      "Engineering",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1906-04165",
      "ArXiv":"1906.04165",
      "MAG":"2950130316",
      "CorpusId":184487094
    }
  },
  {
    "paper_id":"917f8fd2802b04ea0e8e51210457f0f904de97ae",
    "title":"Pretraining-Based Natural Language Generation for Text Summarization",
    "authors":[
      "Haoyu Zhang",
      "Yeyun Gong",
      "Yu Yan",
      "Nan Duan",
      "Jianjun Xu",
      "Ji Wang",
      "Ming Gong",
      "M. Zhou"
    ],
    "abstract":"In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN\/Daily Mail and New York Times datasets.",
    "venue":"Conference on Computational Natural Language Learning",
    "year":2019,
    "publication_date":"2019-02-25",
    "citation_count":221,
    "reference_count":27,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2985808369",
      "ACL":"K19-1074",
      "ArXiv":"1902.09243",
      "DBLP":"journals\/corr\/abs-1902-09243",
      "DOI":"10.18653\/v1\/K19-1074",
      "CorpusId":67855893
    }
  },
  {
    "paper_id":"3e6d7f495ab7a8bdd66e3eca60c60a8f65c080d0",
    "title":"NLP based Machine Learning Approaches for Text Summarization",
    "authors":[
      "Rahul",
      "Surabhi Adhikar",
      "Monika"
    ],
    "abstract":"Due to the plethora of data available today, text summarization has become very essential to gain just the right amount of information from huge texts. We see long articles in news websites, blogs, customers’ review websites, and so on. This review paper presents various approaches to generate summary of huge texts. Various papers have been studied for different methods that have been used so far for text summarization. Mostly, the methods described in this paper produce Abstractive (ABS) or Extractive (EXT) summaries of text documents. Query-based summarization techniques are also discussed. The paper mostly discusses about the structured based and semantic based approaches for summarization of the text documents. Various datasets were used to test the summaries produced by these models, such as the CNN corpus, DUC2000, single and multiple text documents etc. We have studied these methods and also the tendencies, achievements, past work and future scope of them in text summarization as well as other fields.",
    "venue":"International Conference Computing Methodologies and Communication",
    "year":2020,
    "publication_date":"2020-03-01",
    "citation_count":66,
    "reference_count":15,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3018972887",
      "DOI":"10.1109\/ICCMC48092.2020.ICCMC-00099",
      "CorpusId":216105736
    }
  },
  {
    "paper_id":"b4c3be06f695a3aa04a90b540cbbd6e99643cfb9",
    "title":"Arabic text summarization using deep learning approach",
    "authors":[
      "Molham Al-Maleh",
      "Said Desouki"
    ],
    "abstract":"Natural language processing has witnessed remarkable progress with the advent of deep learning techniques. Text summarization, along other tasks like text translation and sentiment analysis, used deep neural network models to enhance results. The new methods of text summarization are subject to a sequence-to-sequence framework of encoder–decoder model, which is composed of neural networks trained jointly on both input and output. Deep neural networks take advantage of big datasets to improve their results. These networks are supported by the attention mechanism, which can deal with long texts more efficiently by identifying focus points in the text. They are also supported by the copy mechanism that allows the model to copy words from the source to the summary directly. In this research, we are re-implementing the basic summarization model that applies the sequence-to-sequence framework on the Arabic language, which has not witnessed the employment of this model in the text summarization before. Initially, we build an Arabic data set of summarized article headlines. This data set consists of approximately 300 thousand entries, each consisting of an article introduction and the headline corresponding to this introduction. We then apply baseline summarization models to the previous data set and compare the results using the ROUGE scale.",
    "venue":"Journal of Big Data",
    "year":2020,
    "publication_date":"2020-12-01",
    "citation_count":59,
    "reference_count":31,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3110700463",
      "DBLP":"journals\/jbd\/Al-MalehD20",
      "DOI":"10.1186\/s40537-020-00386-7",
      "CorpusId":228097649
    }
  },
  {
    "paper_id":"de0225c89f892778ff804ba3186ab199ad5a57e8",
    "title":"A Survey on Automatic Text Summarization Techniques",
    "authors":[
      "S. Hima",
      "Bindu Sri",
      "Sushma Rani Dutta"
    ],
    "abstract":"In recent years, there is a tremendous explosion in the amount of text data on the internet and in the archives of news articles, scientific papers, legal documents and even in online product reviews. Text summarization is playing an important role in automatic content creation, minutes of meeting generation, helping disabled people and also for quick online document reading. To achieve these, several automation techniques have been proposed in various researches. In this regard, performing an exclusive survey on different methods, approaches of automatic text summarization which are published in different articles in most recent three years.",
    "venue":"Journal of Physics: Conference Series",
    "year":2021,
    "publication_date":"2021-10-01",
    "citation_count":21,
    "reference_count":21,
    "fields_of_study":[
      "Physics"
    ],
    "external_ids":{
      "DOI":"10.1088\/1742-6596\/2040\/1\/012044",
      "CorpusId":239878140
    }
  },
  {
    "paper_id":"0aceb4e92c605fb5264725badaee976c0d721cde",
    "title":"A Survey on NLP based Text Summarization for Summarizing Product Reviews",
    "authors":[
      "Ravali Boorugu",
      "Dr. G. Ramesh"
    ],
    "abstract":"No one can imagine life without a smartphone and internet nowadays. It has become essential for people of all age groups. With an increase in the usage of internet and smartphones, there has been a steady increase in online shopping too. Everyone wishes to get their products delivered at their home without any hassle. How to detect which products are genuine and pick the best among the unlimited options at the same price? Every user looks at the reviews before ordering anything online. Nevertheless reading those long reviews is not easy for everyone. Therefore, there must be something that can reduce the long reviews to short sentences of limited words depicting the same meaning. Text Summarization can come in hand in this aspect. Many NLP researchers are interested in Text Summarization. This paper is a survey on the various types of text summarization techniques starting from the basic to the advanced techniques. According to this survey, seq2seq model along with the LSTM and attention mechanism is used for increased accuracy.",
    "venue":"2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)",
    "year":2020,
    "publication_date":"2020-07-01",
    "citation_count":58,
    "reference_count":14,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3081841258",
      "DOI":"10.1109\/ICIRCA48905.2020.9183355",
      "CorpusId":221473330
    }
  },
  {
    "paper_id":"0263ef3c5635554d920e3c0e6f013075d58f10bd",
    "title":"Text Summarization Method Based on Double Attention Pointer Network",
    "authors":[
      "Zhixin Li",
      "Zhi Peng",
      "Suqin Tang",
      "Canlong Zhang",
      "Huifang Ma"
    ],
    "abstract":"A good document summary should summarize the core content of the text. Research on automatic text summarization attempts to solve this problem. The encoder-decoder model is widely used in text summarization research. Soft attention is used to obtain the required contextual semantic information during decoding. However, due to the lack of access to the key features, the generated summary deviates from the core content. In this paper, we proposed an encoder-decoder model based on a double attention pointer network (DAPT). In DAPT, the self-attention mechanism collects key information from the encoder, the soft attention and the pointer network generate more coherent core content, and the fusion of both generates accurate and coherent summaries. In addition, the improved coverage mechanism is used to address the repetition problem and improve the quality of the generated summaries. Simultaneously, scheduled sampling and reinforcement learning (RL) are combined to generate new training methods to optimize the model. Experiments on the CNN\/Daily Mail dataset and the LCSTS dataset show that our model performs as well as many state-of-the-art models. The experimental analysis shows that our model achieves higher summarization performance and reduces the occurrence of repetition.",
    "venue":"IEEE Access",
    "year":2020,
    "publication_date":"2020-01-10",
    "citation_count":53,
    "reference_count":37,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2998919732",
      "DBLP":"journals\/access\/LiPTZM20",
      "DOI":"10.1109\/ACCESS.2020.2965575",
      "CorpusId":210930338
    }
  },
  {
    "paper_id":"92d879ed4be5438c2498d5269e356959da2030bd",
    "title":"Noisy Self-Knowledge Distillation for Text Summarization",
    "authors":[
      "Yang Liu",
      "S. Shen",
      "Mirella Lapata"
    ],
    "abstract":"In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-09-15",
    "citation_count":46,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/LiuSL21",
      "MAG":"3087395956",
      "ACL":"2021.naacl-main.56",
      "ArXiv":"2009.07032",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.56",
      "CorpusId":221703021
    }
  },
  {
    "paper_id":"807c41699e4208774920d1f3bb7d69407a6d6ecd",
    "title":"Extractive Text Summarization from Web pages using Selenium and TF-IDF algorithm",
    "authors":[
      "Usha Manjari",
      "Syed Rousha",
      "Dr Dasi Sumanth",
      "J. S. Devi"
    ],
    "abstract":"To obtain an overview of the content present in numerous documents, is a time-consuming task. Similarly, searching for specific information online, from multiple websites and webpages is a monotonous task. To avoid this, automatic text summarization is one of the most widely adopted techniques today to get a concise and brief outline of the information. In this paper, a novel process is proposed to generate an extractive summary of the information based on the user's query by extracting data from multiple websites over the internet. Web-scraping through Selenium is also discussed. The Term Frequency-Inverse Document Frequency (TF-IDF) algorithm is applied for text summarization. The proposed approach is unique and efficient for generating summaries as per the user's request.",
    "venue":"2020 4th International Conference on Trends in Electronics and Informatics (ICOEI)(48184)",
    "year":2020,
    "publication_date":"2020-06-01",
    "citation_count":56,
    "reference_count":12,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3042406079",
      "DOI":"10.1109\/ICOEI48184.2020.9142938",
      "CorpusId":220668062
    }
  },
  {
    "paper_id":"eb18be41441260c40cdee36f17fc7ad48f426c5f",
    "title":"Fine-tuning GPT-3 for Russian Text Summarization",
    "authors":[
      "Alexandr Nikolich",
      "Arina Puchkova"
    ],
    "abstract":"Automatic summarization techniques aim to shorten and generalize information given in the text while preserving its core message and the most relevant ideas. This task can be approached and treated with a variety of methods, however, not many attempts have been made to produce solutions specifically for the Russian language despite existing localizations of the state-of-the-art models. In this paper, we aim to showcase ruGPT3 ability to summarize texts, fine-tuning it on the corpora of Russian news with their corresponding human-generated summaries. Additionally, we employ hyperparameter tuning so that the model's output becomes less random and more tied to the original text. We evaluate the resulting texts with a set of metrics, showing that our solution can surpass the state-of-the-art model's performance without additional changes in architecture or loss function. Despite being able to produce sensible summaries, our model still suffers from a number of flaws, namely, it is prone to altering Named Entities present in the original text (such as surnames, places, dates), deviating from facts stated in the given document, and repeating the information in the summary.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-08-07",
    "citation_count":30,
    "reference_count":16,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2108.03502",
      "DBLP":"journals\/corr\/abs-2108-03502",
      "CorpusId":236957382
    }
  },
  {
    "paper_id":"b7da726c244287748575ef404009609afde45bea",
    "title":"Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges",
    "authors":[
      "Dima Suleiman",
      "A. Awajan"
    ],
    "abstract":"In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and analysing information. To retrieve useful knowledge within a reasonable time period, this information must be summarised. This paper reviews recent approaches for abstractive text summarisation using deep learning models. In addition, existing datasets for training and validating these approaches are reviewed, and their features and limitations are presented. The Gigaword dataset is commonly employed for single-sentence summary approaches, while the Cable News Network (CNN)\/Daily Mail dataset is commonly employed for multisentence summary approaches. Furthermore, the measures that are utilised to evaluate the quality of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGE-L are determined to be the most commonly applied metrics. The challenges that are encountered during the summarisation process and the solutions proposed in each approach are analysed. The analysis of the several approaches shows that recurrent neural networks with an attention mechanism and long short-term memory (LSTM) are the most prevalent techniques for abstractive text summarisation. The experimental results show that text summarisation with a pretrained encoder model achieved the highest values for ROUGE1, ROUGE2, and ROUGE-L (43.85, 20.34, and 39.9, respectively). Furthermore, it was determined that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, out-of-vocabulary (OOV) words, summary sentence repetition, inaccurate sentences, and fake facts.",
    "venue":"Mathematical Problems in Engineering",
    "year":2020,
    "publication_date":"2020-08-24",
    "citation_count":111,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3080114912",
      "DOI":"10.1155\/2020\/9365340",
      "CorpusId":221701867
    }
  },
  {
    "paper_id":"a1e96cea0bff6d6ebd15e37b0e562d1663fb1b42",
    "title":"Multidocument Arabic Text Summarization Based on Clustering and Word2Vec to Reduce Redundancy",
    "authors":[
      "Samer Abdulateef",
      "Naseer Ahmed Khan",
      "Bolin Chen",
      "Xuequn Shang"
    ],
    "abstract":"Arabic is one of the most semantically and syntactically complex languages in the world. A key challenging issue in text mining is text summarization, so we propose an unsupervised score-based method which combines the vector space model, continuous bag of words (CBOW), clustering, and a statistically-based method. The problems with multidocument text summarization are the noisy data, redundancy, diminished readability, and sentence incoherency. In this study, we adopt a preprocessing strategy to solve the noise problem and use the word2vec model for two purposes, first, to map the words to fixed-length vectors and, second, to obtain the semantic relationship between each vector based on the dimensions. Similarly, we use a k-means algorithm for two purposes: (1) Selecting the distinctive documents and tokenizing these documents to sentences, and (2) using another iteration of the k-means algorithm to select the key sentences based on the similarity metric to overcome the redundancy problem and generate the initial summary. Lastly, we use weighted principal component analysis (W-PCA) to map the sentences’ encoded weights based on a list of features. This selects the highest set of weights, which relates to important sentences for solving incoherency and readability problems. We adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as an evaluation measure to examine our proposed technique and compare it with state-of-the-art methods. Finally, an experiment on the Essex Arabic Summaries Corpus (EASC) using the ROUGE-1 and ROUGE-2 metrics showed promising results in comparison with existing methods.",
    "venue":"Inf.",
    "year":2020,
    "publication_date":"2020-01-23",
    "citation_count":47,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3003039250",
      "DBLP":"journals\/information\/AbdulateefKCS20",
      "DOI":"10.3390\/info11020059",
      "CorpusId":213204378
    }
  },
  {
    "paper_id":"b480661fe147158fa6da46602fe38a4c053be2df",
    "title":"Hierarchical Human-Like Deep Neural Networks for Abstractive Text Summarization",
    "authors":[
      "Min Yang",
      "Chengming Li",
      "Ying Shen",
      "Qingyao Wu",
      "Zhou Zhao",
      "Xiaojun Chen"
    ],
    "abstract":"Developing an abstractive text summarization (ATS) system that is capable of generating concise, appropriate, and plausible summaries for the source documents is a long-term goal of artificial intelligence (AI). Recent advances in ATS are overwhelmingly contributed by deep learning techniques, which have taken the state-of-the-art of ATS to a new level. Despite the significant success of previous methods, generating high-quality and human-like abstractive summaries remains a challenge in practice. The human reading cognition, which is essential for reading comprehension and logical thinking, is still relatively new territory and underexplored in deep neural networks. In this article, we propose a novel Hierarchical Human-like deep neural network for ATS (HH-ATS), inspired by the process of how humans comprehend an article and write the corresponding summary. Specifically, HH-ATS is composed of three primary components (i.e., a knowledge-aware hierarchical attention module, a multitask learning module, and a dual discriminator generative adversarial network), which mimic the three stages of human reading cognition (i.e., rough reading, active reading, and postediting). Experimental results on two benchmark data sets (CNN\/Daily Mail and Gigaword) demonstrate that HH-ATS consistently and substantially outperforms the compared methods.",
    "venue":"IEEE Transactions on Neural Networks and Learning Systems",
    "year":2020,
    "publication_date":"2020-07-23",
    "citation_count":48,
    "reference_count":50,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "DBLP":"journals\/tnn\/YangLSWZC21",
      "MAG":"3044101393",
      "DOI":"10.1109\/TNNLS.2020.3008037",
      "CorpusId":225464098,
      "PubMed":"32701451"
    }
  },
  {
    "paper_id":"1495855bc99e7f5fe272b3b08c7711be23455dc4",
    "title":"Extractive Automatic Text Summarization Based on Lexical-Semantic Keywords",
    "authors":[
      "Ángel Hernández-Castañeda",
      "René Arnulfo García-Hernández",
      "Yulia Ledeneva",
      "Christian Eduardo Millán-Hernández"
    ],
    "abstract":"The automatic text summarization (ATS) task consists in automatically synthesizing a document to provide a condensed version of it. Creating a summary requires not only selecting the main topics of the sentences but also identifying the key relationships between these topics. Related works rank text units (mainly sentences) to select those that could form the summary. However, the resulting summaries may not include all the topics covered in the source text because important information may have been discarded. In addition, the semantic structure of documents has been barely explored in this field. Thus, this study proposes a new method for the ATS task that takes advantage of semantic information to improve keyword detection. This proposed method increases not only the coverage by clustering the sentences to identify the main topics in the source document but also the precision by detecting the keywords in the clusters. The experimental results of this work indicate that the proposed method outperformed previous methods with a standard collection.",
    "venue":"IEEE Access",
    "year":2020,
    "publication_date":null,
    "citation_count":46,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/access\/Hernandez-Castaneda20",
      "MAG":"3010871516",
      "DOI":"10.1109\/ACCESS.2020.2980226",
      "CorpusId":214595523
    }
  },
  {
    "paper_id":"23f1d4b46bc7c8f357a5a89144d5d32af7be13a5",
    "title":"Training Dynamics for Text Summarization Models",
    "authors":[
      "Tanya Goyal",
      "Jiacheng Xu",
      "J. Li",
      "Greg Durrett"
    ],
    "abstract":"Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN\/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
    "venue":"Findings",
    "year":2021,
    "publication_date":"2021-10-15",
    "citation_count":33,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/GoyalXLD22",
      "ACL":"2022.findings-acl.163",
      "ArXiv":"2110.08370",
      "DOI":"10.18653\/v1\/2022.findings-acl.163",
      "CorpusId":239016608
    }
  },
  {
    "paper_id":"12a50024da4b1ad71ddab2fb68785dc56c2e540f",
    "title":"Text Summarization Techniques: A Brief Survey",
    "authors":[
      "M. Allahyari",
      "Seyedamin Pouriyeh",
      "Mehdi Assefi",
      "S. Safaei",
      "Elizabeth D. Trippe",
      "Juan B. Gutiérrez",
      "K. Kochut"
    ],
    "abstract":"In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.",
    "venue":"International Journal of Advanced Computer Science and Applications",
    "year":2017,
    "publication_date":"2017-07-07",
    "citation_count":534,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2733964180",
      "DBLP":"journals\/corr\/AllahyariPASTGK17",
      "ArXiv":"1707.02268",
      "DOI":"10.14569\/IJACSA.2017.081052",
      "CorpusId":304226
    }
  },
  {
    "paper_id":"57e849d0de13ed5f91d086936296721d4ff75a75",
    "title":"LLaMA: Open and Efficient Foundation Language Models",
    "authors":[
      "Hugo Touvron",
      "Thibaut Lavril",
      "Gautier Izacard",
      "Xavier Martinet",
      "M. Lachaux",
      "Timothée Lacroix",
      "Baptiste Rozière",
      "Naman Goyal",
      "Eric Hambro",
      "Faisal Azhar",
      "Aur'elien Rodriguez",
      "Armand Joulin",
      "Edouard Grave",
      "Guillaume Lample"
    ],
    "abstract":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-02-27",
    "citation_count":16683,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-13971",
      "ArXiv":"2302.13971",
      "CorpusId":257219404
    }
  },
  {
    "paper_id":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
    "title":"Language Models are Few-Shot Learners",
    "authors":[
      "Tom B. Brown",
      "Benjamin Mann",
      "Nick Ryder",
      "Melanie Subbiah",
      "J. Kaplan",
      "Prafulla Dhariwal",
      "Arvind Neelakantan",
      "Pranav Shyam",
      "Girish Sastry",
      "Amanda Askell",
      "Sandhini Agarwal",
      "Ariel Herbert-Voss",
      "Gretchen Krueger",
      "T. Henighan",
      "R. Child",
      "A. Ramesh",
      "Daniel M. Ziegler",
      "Jeff Wu",
      "Clemens Winter",
      "Christopher Hesse",
      "Mark Chen",
      "Eric Sigler",
      "Ma-teusz Litwin",
      "Scott Gray",
      "Benjamin Chess",
      "Jack Clark",
      "Christopher Berner",
      "Sam McCandlish",
      "Alec Radford",
      "I. Sutskever",
      "Dario Amodei"
    ],
    "abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
    "venue":"Neural Information Processing Systems",
    "year":2020,
    "publication_date":"2020-05-28",
    "citation_count":49968,
    "reference_count":146,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2005.14165",
      "DBLP":"conf\/nips\/BrownMRSKDNSSAA20",
      "MAG":"3030163527",
      "CorpusId":218971783
    }
  },
  {
    "paper_id":"d766bffc357127e0dc86dd69561d5aeb520d6f4c",
    "title":"Training language models to follow instructions with human feedback",
    "authors":[
      "Long Ouyang",
      "Jeff Wu",
      "Xu Jiang",
      "Diogo Almeida",
      "Carroll L. Wainwright",
      "Pamela Mishkin",
      "Chong Zhang",
      "Sandhini Agarwal",
      "Katarina Slama",
      "Alex Ray",
      "John Schulman",
      "Jacob Hilton",
      "Fraser Kelton",
      "Luke E. Miller",
      "Maddie Simens",
      "Amanda Askell",
      "Peter Welinder",
      "P. Christiano",
      "Jan Leike",
      "Ryan J. Lowe"
    ],
    "abstract":"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-03-04",
    "citation_count":16418,
    "reference_count":83,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2203.02155",
      "DBLP":"journals\/corr\/abs-2203-02155",
      "CorpusId":246426909
    }
  },
  {
    "paper_id":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
    "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "authors":[
      "Junnan Li",
      "Dongxu Li",
      "S. Savarese",
      "Steven C. H. Hoi"
    ],
    "abstract":"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-01-30",
    "citation_count":6136,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2301-12597",
      "ArXiv":"2301.12597",
      "DOI":"10.48550\/arXiv.2301.12597",
      "CorpusId":256390509
    }
  },
  {
    "paper_id":"1b6e810ce0afd0dd093f789d2b2742d047e316d5",
    "title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "authors":[
      "Jason Wei",
      "Xuezhi Wang",
      "Dale Schuurmans",
      "Maarten Bosma",
      "Ed H. Chi",
      "F. Xia",
      "Quoc Le",
      "Denny Zhou"
    ],
    "abstract":"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-01-28",
    "citation_count":13370,
    "reference_count":119,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2201.11903",
      "DBLP":"conf\/nips\/Wei0SBIXCLZ22",
      "CorpusId":246411621
    }
  },
  {
    "paper_id":"35b142ea69598e6241f0011312128031df55895c",
    "title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
    "authors":[
      "Zhihong Shao",
      "Peiyi Wang",
      "Qihao Zhu",
      "R. Xu",
      "Jun-Mei Song",
      "Mingchuan Zhang",
      "Y. K. Li",
      "Yu Wu",
      "Daya Guo"
    ],
    "abstract":"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-02-05",
    "citation_count":3139,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2402.03300",
      "DBLP":"journals\/corr\/abs-2402-03300",
      "DOI":"10.48550\/arXiv.2402.03300",
      "CorpusId":267412607
    }
  },
  {
    "paper_id":"a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "title":"LoRA: Low-Rank Adaptation of Large Language Models",
    "authors":[
      "J. E. Hu",
      "Yelong Shen",
      "Phillip Wallis",
      "Zeyuan Allen-Zhu",
      "Yuanzhi Li",
      "Shean Wang",
      "Weizhu Chen"
    ],
    "abstract":"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https:\/\/github.com\/microsoft\/LoRA.",
    "venue":"International Conference on Learning Representations",
    "year":2021,
    "publication_date":"2021-06-17",
    "citation_count":14141,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/HuSWALWWC22",
      "ArXiv":"2106.09685",
      "CorpusId":235458009
    }
  },
  {
    "paper_id":"5f19ae1135a9500940978104ec15a5b8751bc7d2",
    "title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models",
    "authors":[
      "Xuezhi Wang",
      "Jason Wei",
      "D. Schuurmans",
      "Quoc Le",
      "Ed H. Chi",
      "Denny Zhou"
    ],
    "abstract":"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-03-21",
    "citation_count":5077,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/0002WSLCNCZ23",
      "ArXiv":"2203.11171",
      "CorpusId":247595263
    }
  },
  {
    "paper_id":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
    "title":"Evaluating Large Language Models Trained on Code",
    "authors":[
      "Mark Chen",
      "Jerry Tworek",
      "Heewoo Jun",
      "Qiming Yuan",
      "Henrique Pondé",
      "Jared Kaplan",
      "Harrison Edwards",
      "Yura Burda",
      "Nicholas Joseph",
      "Greg Brockman",
      "Alex Ray",
      "Raul Puri",
      "Gretchen Krueger",
      "Michael Petrov",
      "Heidy Khlaaf",
      "Girish Sastry",
      "Pamela Mishkin",
      "Brooke Chan",
      "Scott Gray",
      "Nick Ryder",
      "Mikhail Pavlov",
      "Alethea Power",
      "Lukasz Kaiser",
      "Mo Bavarian",
      "Clemens Winter",
      "P. Tillet",
      "F. Such",
      "D. Cummings",
      "Matthias Plappert",
      "Fotios Chantzis",
      "Elizabeth Barnes",
      "Ariel Herbert-Voss",
      "William H. Guss",
      "Alex Nichol",
      "Igor Babuschkin",
      "S. Balaji",
      "Shantanu Jain",
      "A. Carr",
      "Jan Leike",
      "Josh Achiam",
      "Vedant Misra",
      "Evan Morikawa",
      "Alec Radford",
      "M. Knight",
      "Miles Brundage",
      "Mira Murati",
      "Katie Mayer",
      "Peter Welinder",
      "Bob McGrew",
      "Dario Amodei",
      "Sam McCandlish",
      "I. Sutskever",
      "Wojciech Zaremba"
    ],
    "abstract":"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-07-07",
    "citation_count":7191,
    "reference_count":127,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2107-03374",
      "ArXiv":"2107.03374",
      "CorpusId":235755472
    }
  },
  {
    "paper_id":"99832586d55f540f603637e458a292406a0ed75d",
    "title":"ReAct: Synergizing Reasoning and Acting in Language Models",
    "authors":[
      "Shunyu Yao",
      "Jeffrey Zhao",
      "Dian Yu",
      "Nan Du",
      "Izhak Shafran",
      "Karthik Narasimhan",
      "Yuan Cao"
    ],
    "abstract":"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https:\/\/react-lm.github.io",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-10-06",
    "citation_count":4561,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/YaoZYDSN023",
      "ArXiv":"2210.03629",
      "CorpusId":252762395
    }
  },
  {
    "paper_id":"13a0d8bb38f739990c8cd65a44061c6534f17221",
    "title":"OPT: Open Pre-trained Transformer Language Models",
    "authors":[
      "Susan Zhang",
      "Stephen Roller",
      "Naman Goyal",
      "Mikel Artetxe",
      "Moya Chen",
      "Shuohui Chen",
      "Christopher Dewan",
      "Mona T. Diab",
      "Xian Li",
      "Xi Victoria Lin",
      "Todor Mihaylov",
      "Myle Ott",
      "Sam Shleifer",
      "Kurt Shuster",
      "Daniel Simig",
      "Punit Singh Koura",
      "Anjali Sridhar",
      "Tianlu Wang",
      "Luke Zettlemoyer"
    ],
    "abstract":"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1\/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-05-02",
    "citation_count":4221,
    "reference_count":120,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-01068",
      "ArXiv":"2205.01068",
      "CorpusId":248496292
    }
  },
  {
    "paper_id":"e7ad08848d5d7c5c47673ffe0da06af443643bda",
    "title":"Large Language Models are Zero-Shot Reasoners",
    "authors":[
      "Takeshi Kojima",
      "S. Gu",
      "Machel Reid",
      "Yutaka Matsuo",
      "Yusuke Iwasawa"
    ],
    "abstract":"Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-05-24",
    "citation_count":5709,
    "reference_count":61,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-11916",
      "ArXiv":"2205.11916",
      "CorpusId":249017743
    }
  },
  {
    "paper_id":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
    "title":"Finetuned Language Models Are Zero-Shot Learners",
    "authors":[
      "Jason Wei",
      "Maarten Bosma",
      "Vincent Zhao",
      "Kelvin Guu",
      "Adams Wei Yu",
      "Brian Lester",
      "Nan Du",
      "Andrew M. Dai",
      "Quoc V. Le"
    ],
    "abstract":"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
    "venue":"International Conference on Learning Representations",
    "year":2021,
    "publication_date":"2021-09-03",
    "citation_count":4428,
    "reference_count":169,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2109-01652",
      "ArXiv":"2109.01652",
      "CorpusId":237416585
    }
  },
  {
    "paper_id":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
    "title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜",
    "authors":[
      "Emily M. Bender",
      "Timnit Gebru",
      "Angelina McMillan-Major",
      "Shmargaret Shmitchell"
    ],
    "abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2\/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.",
    "venue":"Conference on Fairness, Accountability and Transparency",
    "year":2021,
    "publication_date":"2021-03-03",
    "citation_count":5651,
    "reference_count":164,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/fat\/BenderGMS21",
      "DOI":"10.1145\/3442188.3445922",
      "CorpusId":262580630
    }
  },
  {
    "paper_id":"e6c561d02500b2596a230b341a8eb8b921ca5bf2",
    "title":"Scaling Laws for Neural Language Models",
    "authors":[
      "J. Kaplan",
      "Sam McCandlish",
      "T. Henighan",
      "Tom B. Brown",
      "Benjamin Chess",
      "R. Child",
      "Scott Gray",
      "Alec Radford",
      "Jeff Wu",
      "Dario Amodei"
    ],
    "abstract":"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model\/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-01-23",
    "citation_count":6214,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "MAG":"3001279689",
      "ArXiv":"2001.08361",
      "DBLP":"journals\/corr\/abs-2001-08361",
      "CorpusId":210861095
    }
  },
  {
    "paper_id":"8bd6a2a89503be083176f2cc26fabedb79238cbd",
    "title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
    "authors":[
      "Wenliang Dai",
      "Junnan Li",
      "Dongxu Li",
      "A. Tiong",
      "Junqi Zhao",
      "Weisheng Wang",
      "Boyang Albert Li",
      "Pascale Fung",
      "Steven C. H. Hoi"
    ],
    "abstract":"Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https:\/\/github.com\/salesforce\/LAVIS\/tree\/main\/projects\/instructblip.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-11",
    "citation_count":2682,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.06500",
      "DBLP":"journals\/corr\/abs-2305-06500",
      "DOI":"10.48550\/arXiv.2305.06500",
      "CorpusId":258615266
    }
  },
  {
    "paper_id":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
    "title":"A Survey of Large Language Models",
    "authors":[
      "Wayne Xin Zhao",
      "Kun Zhou",
      "Junyi Li",
      "Tianyi Tang",
      "Xiaolei Wang",
      "Yupeng Hou",
      "Yingqian Min",
      "Beichen Zhang",
      "Junjie Zhang",
      "Zican Dong",
      "Yifan Du",
      "Chen Yang",
      "Yushuo Chen",
      "Z. Chen",
      "Jinhao Jiang",
      "Ruiyang Ren",
      "Yifan Li",
      "Xinyu Tang",
      "Zikang Liu",
      "Peiyu Liu",
      "J. Nie",
      "Ji-rong Wen"
    ],
    "abstract":"Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-03-31",
    "citation_count":3602,
    "reference_count":417,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2303.18223",
      "DBLP":"journals\/corr\/abs-2303-18223",
      "CorpusId":257900969
    }
  },
  {
    "paper_id":"ca6a2bc279be5a3349a22bfd6866ed633d18734b",
    "title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "authors":[
      "Deyao Zhu",
      "Jun Chen",
      "Xiaoqian Shen",
      "Xiang Li",
      "Mohamed Elhoseiny"
    ],
    "abstract":"The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https:\/\/minigpt-4.github.io\/.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-04-20",
    "citation_count":2541,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2304.10592",
      "DBLP":"conf\/iclr\/Zhu0SLE24",
      "DOI":"10.48550\/arXiv.2304.10592",
      "CorpusId":258291930
    }
  },
  {
    "paper_id":"96ea07447d2f9adefe03852a878517a2a6d45b96",
    "title":"Learning to Prompt for Vision-Language Models",
    "authors":[
      "Kaiyang Zhou",
      "Jingkang Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract":"Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.",
    "venue":"International Journal of Computer Vision",
    "year":2021,
    "publication_date":"2021-09-02",
    "citation_count":3120,
    "reference_count":61,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2109.01134",
      "DBLP":"journals\/ijcv\/ZhouYLL22",
      "DOI":"10.1007\/s11263-022-01653-1",
      "CorpusId":237386023
    }
  },
  {
    "paper_id":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
    "title":"Scaling Instruction-Finetuned Language Models",
    "authors":[
      "Hyung Won Chung",
      "Le Hou",
      "S. Longpre",
      "Barret Zoph",
      "Yi Tay",
      "W. Fedus",
      "Eric Li",
      "Xuezhi Wang",
      "Mostafa Dehghani",
      "Siddhartha Brahma",
      "Albert Webson",
      "S. Gu",
      "Zhuyun Dai",
      "Mirac Suzgun",
      "Xinyun Chen",
      "A. Chowdhery",
      "Dasha Valter",
      "Sharan Narang",
      "Gaurav Mishra",
      "Adams Wei Yu",
      "Vincent Zhao",
      "Yanping Huang",
      "Andrew M. Dai",
      "Hongkun Yu",
      "Slav Petrov",
      "Ed H. Chi",
      "J. Dean",
      "Jacob Devlin",
      "Adam Roberts",
      "Denny Zhou",
      "Quoc V. Le",
      "Jason Wei"
    ],
    "abstract":"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
    "venue":"Journal of machine learning research",
    "year":2022,
    "publication_date":"2022-10-20",
    "citation_count":3676,
    "reference_count":106,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2210-11416",
      "ArXiv":"2210.11416",
      "DOI":"10.48550\/arXiv.2210.11416",
      "CorpusId":253018554
    }
  },
  {
    "paper_id":"47030369e97cc44d4b2e3cf1be85da0fd134904a",
    "title":"Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "authors":[
      "Andy Zou",
      "Zifan Wang",
      "J. Z. Kolter",
      "Matt Fredrikson"
    ],
    "abstract":"Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com\/llm-attacks\/llm-attacks.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-07-27",
    "citation_count":2085,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2307.15043",
      "DBLP":"journals\/corr\/abs-2307-15043",
      "CorpusId":260202961
    }
  },
  {
    "paper_id":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
    "title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "authors":[
      "Shunyu Yao",
      "Dian Yu",
      "Jeffrey Zhao",
      "Izhak Shafran",
      "T. Griffiths",
      "Yuan Cao",
      "Karthik Narasimhan"
    ],
    "abstract":"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https:\/\/github.com\/princeton-nlp\/tree-of-thought-llm.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-17",
    "citation_count":2832,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.10601",
      "DBLP":"conf\/nips\/YaoYZS00N23",
      "DOI":"10.48550\/arXiv.2305.10601",
      "CorpusId":258762525
    }
  },
  {
    "paper_id":"ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
    "title":"Gemma 2: Improving Open Language Models at a Practical Size",
    "authors":[
      "Gemma Team Morgane Riviere",
      "Shreya Pathak",
      "Pier Giuseppe Sessa",
      "Cassidy Hardin",
      "Surya Bhupatiraju",
      "L'eonard Hussenot",
      "Thomas Mesnard",
      "Bobak Shahriari",
      "Alexandre Ram'e",
      "Johan Ferret",
      "Peter Liu",
      "P. Tafti",
      "Abe Friesen",
      "Michelle Casbon",
      "Sabela Ramos",
      "Ravin Kumar",
      "Charline Le Lan",
      "Sammy Jerome",
      "Anton Tsitsulin",
      "Nino Vieillard",
      "P. Stańczyk",
      "Sertan Girgin",
      "Nikola Momchev",
      "Matt Hoffman",
      "S. Thakoor",
      "Jean-Bastien Grill",
      "Behnam Neyshabur",
      "Alanna Walton",
      "A. Severyn",
      "Alicia Parrish",
      "Aliya Ahmad",
      "Allen Hutchison",
      "Alvin Abdagic",
      "Amanda Carl",
      "Amy Shen",
      "Andy Brock",
      "Andy Coenen",
      "Anthony Laforge",
      "Antonia Paterson",
      "Ben Bastian",
      "Bilal Piot",
      "Boxi Wu",
      "Brandon Royal",
      "Charlie Chen",
      "Chintu Kumar",
      "Chris Perry",
      "Christoper A. Welty",
      "Christopher A. Choquette-Choo",
      "Danila Sinopalnikov",
      "David Weinberger",
      "Dimple Vijaykumar",
      "Dominika Rogozi'nska",
      "D. Herbison",
      "Elisa Bandy",
      "Emma Wang",
      "Eric Noland",
      "Erica Moreira",
      "Evan Senter",
      "Evgenii Eltyshev",
      "Francesco Visin",
      "Gabriel Rasskin",
      "Gary Wei",
      "Glenn Cameron",
      "Gus Martins",
      "Hadi Hashemi",
      "Hanna Klimczak-Pluci'nska",
      "Harleen Batra",
      "H. Dhand",
      "Ivan Nardini",
      "Jacinda Mein",
      "Jack Zhou",
      "James Svensson",
      "J. Stanway",
      "Jetha Chan",
      "Jin Zhou",
      "Joana Carrasqueira",
      "Joana Iljazi",
      "Jocelyn Becker",
      "Joe Fernandez",
      "Joost R. van Amersfoort",
      "Josh Gordon",
      "Josh Lipschultz",
      "Joshua Newlan",
      "Junsong Ji",
      "Kareem Mohamed",
      "Kartikeya Badola",
      "Kat Black",
      "Katie Millican",
      "Keelin McDonell",
      "Kelvin Nguyen",
      "Kiranbir Sodhia",
      "Kish Greene",
      "Lars Lowe Sjoesund",
      "Lauren Usui",
      "L. Sifre",
      "Lena Heuermann",
      "Leti-cia Lago",
      "Lilly McNealus",
      "Livio Baldini Soares",
      "Logan Kilpatrick",
      "Lucas Dixon",
      "Luciano Martins",
      "Machel Reid",
      "Manvinder Singh",
      "Mark Iverson",
      "Martin Gorner",
      "Mat Velloso",
      "Mateo Wirth",
      "Matt Davidow",
      "Matt Miller",
      "Matthew Rahtz",
      "Matthew Watson",
      "Meg Risdal",
      "Mehran Kazemi",
      "Michael Moynihan",
      "Ming Zhang",
      "Minsuk Kahng",
      "Minwoo Park",
      "Mofi Rahman",
      "Mohit Khatwani",
      "Natalie Dao",
      "Nen-shad Bardoliwalla",
      "N. Devanathan",
      "Neta Dumai",
      "Nilay Chauhan",
      "O. Wahltinez",
      "Pankil Botarda",
      "Parker Barnes",
      "P. Barham",
      "Paul Michel",
      "Peng-chong Jin",
      "Petko Georgiev",
      "Phil Culliton",
      "Pradeep Kuppala",
      "R. Comanescu",
      "Ramona Merhej",
      "Reena Jana",
      "R. Rokni",
      "Rishabh Agarwal",
      "Ryan Mullins",
      "Samaneh Saadat",
      "S. M. Carthy",
      "Sarah Perrin",
      "Sébastien M. R. Arnold",
      "Se-bastian Krause",
      "Shengyang Dai",
      "S. Garg",
      "Shruti Sheth",
      "S. Ronstrom",
      "Susan Chan",
      "Timothy Jordan",
      "Ting Yu",
      "Tom Eccles",
      "Tom Hennigan",
      "Tomás Kociský",
      "Tulsee Doshi",
      "Vihan Jain",
      "Vikas Yadav",
      "Vilobh Meshram",
      "Vishal Dharmadhikari",
      "Warren Barkley",
      "Wei Wei",
      "Wenming Ye",
      "Woohyun Han",
      "Woosuk Kwon",
      "Xiang Xu",
      "Zhe Shen",
      "Zhitao Gong",
      "Zichuan Wei",
      "Victor Cotruta",
      "Phoebe Kirk",
      "Anand Rao",
      "Minh Giang",
      "Ludovic Peran",
      "Tris Warkentin",
      "Eli Collins",
      "Joelle Barral",
      "Z. Ghahramani",
      "R. Hadsell",
      "D. Sculley",
      "Jeanine Banks",
      "Anca Dragan",
      "Slav Petrov",
      "O. Vinyals",
      "Jeffrey Dean",
      "D. Hassabis",
      "K. Kavukcuoglu",
      "Clément Farabet",
      "Elena Buchatskaya",
      "Sebastian Borgeaud",
      "Noah Fiedel",
      "Armand Joulin",
      "Kathleen Kenealy",
      "Robert Dadashi",
      "Alek Andreev"
    ],
    "abstract":"In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-07-31",
    "citation_count":1396,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2408.00118",
      "DBLP":"journals\/corr\/abs-2408-00118",
      "DOI":"10.48550\/arXiv.2408.00118",
      "CorpusId":270843326
    }
  },
  {
    "paper_id":"1733eb7792f7a43dd21f51f4d1017a1bffd217b5",
    "title":"Lost in the Middle: How Language Models Use Long Contexts",
    "authors":[
      "Nelson F. Liu",
      "Kevin Lin",
      "John Hewitt",
      "Ashwin Paranjape",
      "Michele Bevilacqua",
      "F. Petroni",
      "Percy Liang"
    ],
    "abstract":"While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-07-06",
    "citation_count":2345,
    "reference_count":58,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tacl\/LiuLHPBPL24",
      "ArXiv":"2307.03172",
      "ACL":"2024.tacl-1.9",
      "DOI":"10.1162\/tacl_a_00638",
      "CorpusId":259360665
    }
  },
  {
    "paper_id":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "title":"Retrieval-Augmented Generation for Large Language Models: A Survey",
    "authors":[
      "Yunfan Gao",
      "Yun Xiong",
      "Xinyu Gao",
      "Kangxiang Jia",
      "Jinliu Pan",
      "Yuxi Bi",
      "Yi Dai",
      "Jiawei Sun",
      "Qianyu Guo",
      "Meng Wang",
      "Haofen Wang"
    ],
    "abstract":"Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-12-18",
    "citation_count":2545,
    "reference_count":229,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2312.10997",
      "DBLP":"journals\/corr\/abs-2312-10997",
      "CorpusId":266359151
    }
  },
  {
    "paper_id":"53d128ea815bcc0526856eb5a9c42cc977cb36a7",
    "title":"Toolformer: Language Models Can Teach Themselves to Use Tools",
    "authors":[
      "Timo Schick",
      "Jane Dwivedi-Yu",
      "Roberto Dessì",
      "R. Raileanu",
      "M. Lomeli",
      "Luke Zettlemoyer",
      "Nicola Cancedda",
      "Thomas Scialom"
    ],
    "abstract":"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-02-09",
    "citation_count":2399,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-04761",
      "ArXiv":"2302.04761",
      "DOI":"10.48550\/arXiv.2302.04761",
      "CorpusId":256697342
    }
  },
  {
    "paper_id":"888728745dbb769e29ed475d4f7661eebe1a71cf",
    "title":"A Survey on Evaluation of Large Language Models",
    "authors":[
      "Yu-Chu Chang",
      "Xu Wang",
      "Jindong Wang",
      "Yuan Wu",
      "Kaijie Zhu",
      "Hao Chen",
      "Linyi Yang",
      "Xiaoyuan Yi",
      "Cunxiang Wang",
      "Yidong Wang",
      "Weirong Ye",
      "Yue Zhang",
      "Yi Chang",
      "Philip S. Yu",
      "Qian Yang",
      "Xingxu Xie"
    ],
    "abstract":"Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https:\/\/github.com\/MLGroupJLU\/LLM-eval-survey",
    "venue":"ACM Transactions on Intelligent Systems and Technology",
    "year":2023,
    "publication_date":"2023-07-06",
    "citation_count":2474,
    "reference_count":305,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tist\/ChangWWWYZCYWWYZCYYX24",
      "ArXiv":"2307.03109",
      "DOI":"10.1145\/3641289",
      "CorpusId":259360395
    }
  },
  {
    "paper_id":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
    "authors":[
      "Zihang Dai",
      "Zhilin Yang",
      "Yiming Yang",
      "J. Carbonell",
      "Quoc V. Le",
      "R. Salakhutdinov"
    ],
    "abstract":"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc\/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-01-09",
    "citation_count":4035,
    "reference_count":71,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "ArXiv":"1901.02860",
      "DBLP":"conf\/acl\/DaiYYCLS19",
      "MAG":"2964110616",
      "ACL":"P19-1285",
      "DOI":"10.18653\/v1\/P19-1285",
      "CorpusId":57759363
    }
  },
  {
    "paper_id":"a38e0f993e4805ba8a9beae4c275c91ffcec01df",
    "title":"Program Synthesis with Large Language Models",
    "authors":[
      "Jacob Austin",
      "Augustus Odena",
      "Maxwell Nye",
      "Maarten Bosma",
      "H. Michalewski",
      "David Dohan",
      "Ellen Jiang",
      "Carrie J. Cai",
      "Michael Terry",
      "Quoc V. Le",
      "Charles Sutton"
    ],
    "abstract":"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-08-16",
    "citation_count":2650,
    "reference_count":106,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2108-07732",
      "ArXiv":"2108.07732",
      "CorpusId":237142385
    }
  },
  {
    "paper_id":"8342b592fe238f3d230e4959b06fd10153c45db1",
    "title":"Training Compute-Optimal Large Language Models",
    "authors":[
      "Jordan Hoffmann",
      "Sebastian Borgeaud",
      "A. Mensch",
      "Elena Buchatskaya",
      "Trevor Cai",
      "Eliza Rutherford",
      "Diego de Las Casas",
      "Lisa Anne Hendricks",
      "Johannes Welbl",
      "Aidan Clark",
      "Tom Hennigan",
      "Eric Noland",
      "Katie Millican",
      "George van den Driessche",
      "Bogdan Damoc",
      "Aurelia Guy",
      "Simon Osindero",
      "K. Simonyan",
      "Erich Elsen",
      "Jack W. Rae",
      "O. Vinyals",
      "L. Sifre"
    ],
    "abstract":"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-03-29",
    "citation_count":2474,
    "reference_count":78,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2203-15556",
      "ArXiv":"2203.15556",
      "CorpusId":247778764
    }
  },
  {
    "paper_id":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
    "title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    "authors":[
      "Yizhong Wang",
      "Yeganeh Kordi",
      "Swaroop Mishra",
      "Alisa Liu",
      "Noah A. Smith",
      "Daniel Khashabi",
      "Hannaneh Hajishirzi"
    ],
    "abstract":"Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-12-20",
    "citation_count":2695,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2212-10560",
      "ArXiv":"2212.10560",
      "ACL":"2023.acl-long.754",
      "DOI":"10.48550\/arXiv.2212.10560",
      "CorpusId":254877310
    }
  },
  {
    "paper_id":"6052486bc9144dc1730c12bf35323af3792a1fd0",
    "title":"Large language models encode clinical knowledge",
    "authors":[
      "K. Singhal",
      "Shekoofeh Azizi",
      "T. Tu",
      "S. Mahdavi",
      "Jason Wei",
      "Hyung Won Chung",
      "Nathan Scales",
      "A. Tanwani",
      "H. Cole-Lewis",
      "S. Pfohl",
      "P. Payne",
      "Martin G. Seneviratne",
      "P. Gamble",
      "C. Kelly",
      "Nathaneal Scharli",
      "A. Chowdhery",
      "P. A. Mansfield",
      "B. A. Y. Arcas",
      "D. Webster",
      "Greg S. Corrado",
      "Yossi Matias",
      "K. Chou",
      "Juraj Gottweis",
      "Nenad Tomašev",
      "Yun Liu",
      "A. Rajkomar",
      "J. Barral",
      "Christopher Semturs",
      "A. Karthikesalingam",
      "Vivek Natarajan"
    ],
    "abstract":"Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",
    "venue":"Nature",
    "year":2022,
    "publication_date":"2022-12-26",
    "citation_count":3105,
    "reference_count":113,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "ArXiv":"2212.13138",
      "DBLP":"journals\/corr\/abs-2212-13138",
      "PubMedCentral":"10396962",
      "DOI":"10.1038\/s41586-023-06291-2",
      "CorpusId":255124952,
      "PubMed":"37438534"
    }
  },
  {
    "paper_id":"be55e8ec4213868db08f2c3168ae666001bea4b8",
    "title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
    "authors":[
      "Stella Biderman",
      "Hailey Schoelkopf",
      "Quentin Anthony",
      "Herbie Bradley",
      "Kyle O'Brien",
      "Eric Hallahan",
      "Mohammad Aflah Khan",
      "Shivanshu Purohit",
      "USVSN Sai Prashanth",
      "Edward Raff",
      "Aviya Skowron",
      "Lintang Sutawika",
      "Oskar van der Wal"
    ],
    "abstract":"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https:\/\/github.com\/EleutherAI\/pythia}.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-04-03",
    "citation_count":1557,
    "reference_count":101,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icml\/BidermanSABOHKP23",
      "ArXiv":"2304.01373",
      "DOI":"10.48550\/arXiv.2304.01373",
      "CorpusId":257921893
    }
  },
  {
    "paper_id":"e816f788767eec6a8ef0ea9eddd0e902435d4271",
    "title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    "authors":[
      "Suchin Gururangan",
      "Ana Marasović",
      "Swabha Swayamdipta",
      "Kyle Lo",
      "Iz Beltagy",
      "Doug Downey",
      "Noah A. Smith"
    ],
    "abstract":"Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-04-23",
    "citation_count":2658,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/GururanganMSLBD20",
      "ArXiv":"2004.10964",
      "MAG":"3017961061",
      "ACL":"2020.acl-main.740",
      "DOI":"10.18653\/v1\/2020.acl-main.740",
      "CorpusId":216080466
    }
  },
  {
    "paper_id":"dac3a172b504f4e33c029655e9befb3386e5f63a",
    "title":"Emergent Abilities of Large Language Models",
    "authors":[
      "Jason Wei",
      "Yi Tay",
      "Rishi Bommasani",
      "Colin Raffel",
      "Barret Zoph",
      "Sebastian Borgeaud",
      "Dani Yogatama",
      "Maarten Bosma",
      "Denny Zhou",
      "Donald Metzler",
      "Ed H. Chi",
      "Tatsunori Hashimoto",
      "O. Vinyals",
      "P. Liang",
      "J. Dean",
      "W. Fedus"
    ],
    "abstract":"Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
    "venue":"Trans. Mach. Learn. Res.",
    "year":2022,
    "publication_date":"2022-06-15",
    "citation_count":2986,
    "reference_count":107,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-07682",
      "ArXiv":"2206.07682",
      "DOI":"10.48550\/arXiv.2206.07682",
      "CorpusId":249674500
    }
  },
  {
    "paper_id":"85e7d63f75c0916bd350a229e040c5fbb1472e7a",
    "title":"Making Pre-trained Language Models Better Few-shot Learners",
    "authors":[
      "Tianyu Gao",
      "Adam Fisch",
      "Danqi Chen"
    ],
    "abstract":"The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-01-01",
    "citation_count":2132,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2012.15723",
      "DBLP":"conf\/acl\/GaoFC20",
      "ACL":"2021.acl-long.295",
      "DOI":"10.18653\/v1\/2021.acl-long.295",
      "CorpusId":229923710
    }
  },
  {
    "paper_id":"df7d26339adf4eb0c07160947b9d2973c24911ba",
    "title":"Extracting Training Data from Large Language Models",
    "authors":[
      "Nicholas Carlini",
      "Florian Tramèr",
      "Eric Wallace",
      "Matthew Jagielski",
      "Ariel Herbert-Voss",
      "Katherine Lee",
      "Adam Roberts",
      "Tom B. Brown",
      "D. Song",
      "Ú. Erlingsson",
      "Alina Oprea",
      "Colin Raffel"
    ],
    "abstract":"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
    "venue":"USENIX Security Symposium",
    "year":2020,
    "publication_date":"2020-12-14",
    "citation_count":2337,
    "reference_count":75,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2012-07805",
      "MAG":"3112689365",
      "ArXiv":"2012.07805",
      "CorpusId":229156229
    }
  },
  {
    "paper_id":"d5d7b26dbdf09737878644530b226a79c9f43bfb",
    "title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
    "authors":[
      "Yaowei Zheng",
      "Richong Zhang",
      "Junhao Zhang",
      "Yanhan Ye",
      "Zheyan Luo",
      "Yongqiang Ma"
    ],
    "abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https:\/\/github.com\/hiyouga\/LLaMA-Factory and received over 25,000 stars and 3,000 forks.",
    "venue":"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "year":2024,
    "publication_date":"2024-03-20",
    "citation_count":999,
    "reference_count":95,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2403-13372",
      "ArXiv":"2403.13372",
      "DOI":"10.48550\/arXiv.2403.13372",
      "CorpusId":268536974
    }
  },
  {
    "paper_id":"cf1f26e7cbed3958b3c2870656568c299fece6e3",
    "title":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
    "authors":[
      "Tiffany H. Kung",
      "Morgan Cheatham",
      "Arielle Medenilla",
      "Czarina Sillos",
      "Lorie De Leon",
      "Camille Elepaño",
      "Maria Madriaga",
      "Rimel Aggabao",
      "Giezel Diaz-Candido",
      "James Maningo",
      "Victor Tseng"
    ],
    "abstract":"We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.",
    "venue":"medRxiv",
    "year":2022,
    "publication_date":"2022-12-20",
    "citation_count":2848,
    "reference_count":29,
    "fields_of_study":[
      "Medicine"
    ],
    "external_ids":{
      "PubMedCentral":"9931230",
      "DOI":"10.1371\/journal.pdig.0000198",
      "CorpusId":254876189,
      "PubMed":"36812645"
    }
  },
  {
    "paper_id":"1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
    "title":"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
    "authors":[
      "Lei Huang",
      "Weijiang Yu",
      "Weitao Ma",
      "Weihong Zhong",
      "Zhangyin Feng",
      "Haotian Wang",
      "Qianglong Chen",
      "Weihua Peng",
      "Xiaocheng Feng",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract":"The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
    "venue":"ACM Trans. Inf. Syst.",
    "year":2023,
    "publication_date":"2023-11-09",
    "citation_count":1629,
    "reference_count":287,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2311.05232",
      "DBLP":"journals\/tois\/HuangYMZFWCPFQL25",
      "DOI":"10.1145\/3703155",
      "CorpusId":265067168
    }
  },
  {
    "paper_id":"ce913026f693101e54d3ab9152e107034d81fce1",
    "title":"Holistic Evaluation of Language Models",
    "authors":[
      "Percy Liang",
      "Rishi Bommasani",
      "Tony Lee",
      "Dimitris Tsipras",
      "Dilara Soylu",
      "Michihiro Yasunaga",
      "Yian Zhang",
      "D. Narayanan",
      "Yuhuai Wu",
      "Ananya Kumar",
      "Benjamin Newman",
      "Binhang Yuan",
      "Bobby Yan",
      "Ce Zhang",
      "Christian Cosgrove",
      "Christopher D. Manning",
      "Christopher R'e",
      "Diana Acosta-Navas",
      "Drew A. Hudson",
      "E. Zelikman",
      "Esin Durmus",
      "Faisal Ladhak",
      "Frieda Rong",
      "Hongyu Ren",
      "Huaxiu Yao",
      "Jue Wang",
      "Keshav Santhanam",
      "Laurel J. Orr",
      "Lucia Zheng",
      "Mert Yuksekgonul",
      "Mirac Suzgun",
      "Nathan S. Kim",
      "Neel Guha",
      "Niladri S. Chatterji",
      "O. Khattab",
      "Peter Henderson",
      "Qian Huang",
      "Ryan Chi",
      "Sang Michael Xie",
      "Shibani Santurkar",
      "S. Ganguli",
      "Tatsunori Hashimoto",
      "Thomas F. Icard",
      "Tianyi Zhang",
      "Vishrav Chaudhary",
      "William Wang",
      "Xuechen Li",
      "Yifan Mai",
      "Yuhui Zhang",
      "Yuta Koreeda"
    ],
    "abstract":"Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https:\/\/crfm.stanford.edu\/helm\/latest\/.",
    "venue":"Trans. Mach. Learn. Res.",
    "year":2023,
    "publication_date":"2023-05-25",
    "citation_count":1213,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "DBLP":"journals\/tmlr\/LiangBLTSYZNWKN23",
      "DOI":"10.1111\/nyas.15007",
      "CorpusId":253553585,
      "PubMed":"37230490"
    }
  },
  {
    "paper_id":"b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
    "title":"Conditional Prompt Learning for Vision-Language Models",
    "authors":[
      "Kaiyang Zhou",
      "Jingkang Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract":"With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning—a recent trend in NLP—to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https:\/\/github.com\/KaiyangZhou\/CoOp.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2022,
    "publication_date":"2022-03-10",
    "citation_count":1755,
    "reference_count":67,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/ZhouYL022",
      "ArXiv":"2203.05557",
      "DOI":"10.1109\/CVPR52688.2022.01631",
      "CorpusId":247363011
    }
  },
  {
    "paper_id":"bd1331b233e84bab7eba503abc60b31ac08e7881",
    "title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
    "authors":[
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "Adam Santoro",
      "Aditya Gupta",
      "Adrià Garriga-Alonso",
      "Agnieszka Kluska",
      "Aitor Lewkowycz",
      "Akshat Agarwal",
      "Alethea Power",
      "Alex Ray",
      "Alex Warstadt",
      "Alexander W. Kocurek",
      "Ali Safaya",
      "Ali Tazarv",
      "Alice Xiang",
      "Alicia Parrish",
      "Allen Nie",
      "Aman Hussain",
      "Amanda Askell",
      "A. Dsouza",
      "Ambrose Slone",
      "Ameet Rahane",
      "Anantharaman S. Iyer",
      "Anders Andreassen",
      "Andrea Madotto",
      "Andrea Santilli",
      "Andreas Stuhlmuller",
      "Andrew M. Dai",
      "A. La",
      "Andrew Kyle Lampinen",
      "Andy Zou",
      "Angela Jiang",
      "Angelica Chen",
      "Anh Vuong",
      "Animesh Gupta",
      "Anna Gottardi",
      "Antonio Norelli",
      "Anu Venkatesh",
      "Arash Gholamidavoodi",
      "A. Tabassum",
      "Arul Menezes",
      "Arun Kirubarajan",
      "A. Mullokandov",
      "Ashish Sabharwal",
      "Austin Herrick",
      "Avia Efrat",
      "Aykut Erdem",
      "Ayla Karakacs",
      "B. R. Roberts",
      "B. S. Loe",
      "Barret Zoph",
      "Bartlomiej Bojanowski",
      "Batuhan Ozyurt",
      "Behnam Hedayatnia",
      "Behnam Neyshabur",
      "Benjamin Inden",
      "Benno Stein",
      "Berk Ekmekci",
      "Bill Yuchen Lin",
      "B. Howald",
      "Bryan Orinion",
      "Cameron Diao",
      "Cameron Dour",
      "Catherine Stinson",
      "Cedrick Argueta",
      "C'esar Ferri Ram'irez",
      "Chandan Singh",
      "Charles Rathkopf",
      "Chenlin Meng",
      "Chitta Baral",
      "Chiyu Wu",
      "Chris Callison-Burch",
      "Chris Waites",
      "Christian Voigt",
      "Christopher D. Manning",
      "Christopher Potts",
      "Cindy Ramirez",
      "Clara E. Rivera",
      "Clemencia Siro",
      "Colin Raffel",
      "Courtney Ashcraft",
      "Cristina Garbacea",
      "Damien Sileo",
      "Dan Garrette",
      "Dan Hendrycks",
      "D. Kilman",
      "Dan Roth",
      "Daniel Freeman",
      "Daniel Khashabi",
      "Daniel Levy",
      "D. Gonz'alez",
      "Danielle R. Perszyk",
      "Danny Hernandez",
      "Danqi Chen",
      "Daphne Ippolito",
      "Dar Gilboa",
      "David Dohan",
      "D. Drakard",
      "David Jurgens",
      "Debajyoti Datta",
      "Deep Ganguli",
      "Denis Emelin",
      "Denis Kleyko",
      "Deniz Yuret",
      "Derek Chen",
      "Derek Tam",
      "Dieuwke Hupkes",
      "Diganta Misra",
      "Dilyar Buzan",
      "Dimitri Coelho Mollo",
      "Diyi Yang",
      "Dong-Ho Lee",
      "Dylan Schrader",
      "Ekaterina Shutova",
      "E. D. Cubuk",
      "Elad Segal",
      "Eleanor Hagerman",
      "Elizabeth Barnes",
      "Elizabeth Donoway",
      "Ellie Pavlick",
      "Emanuele Rodolà",
      "Emma Lam",
      "Eric Chu",
      "Eric Tang",
      "Erkut Erdem",
      "Ernie Chang",
      "Ethan A. Chi",
      "Ethan Dyer",
      "E. Jerzak",
      "Ethan Kim",
      "Eunice Engefu Manyasi",
      "Evgenii Zheltonozhskii",
      "Fanyue Xia",
      "Fatemeh Siar",
      "Fernando Mart'inez-Plumed",
      "Francesca Happ'e",
      "François Chollet",
      "Frieda Rong",
      "Gaurav Mishra",
      "Genta Indra Winata",
      "Gerard de Melo",
      "Germán Kruszewski",
      "Giambattista Parascandolo",
      "Giorgio Mariani",
      "Gloria Xinyue Wang",
      "Gonzalo Jaimovitch-L'opez",
      "Gregor Betz",
      "Guy Gur-Ari",
      "Hana Galijasevic",
      "Hannah Kim",
      "Hannah Rashkin",
      "Hannaneh Hajishirzi",
      "Harsh Mehta",
      "H. Bogar",
      "Henry Shevlin",
      "Hinrich Schutze",
      "H. Yakura",
      "Hongming Zhang",
      "Hugh Mee Wong",
      "Ian Ng",
      "Isaac Noble",
      "Jaap Jumelet",
      "Jack Geissinger",
      "John Kernion",
      "Jacob Hilton",
      "Jaehoon Lee",
      "J. Fisac",
      "James B. Simon",
      "James Koppel",
      "James Zheng",
      "James Zou",
      "Jan Koco'n",
      "Jana Thompson",
      "Janelle Wingfield",
      "Jared Kaplan",
      "Jarema Radom",
      "Jascha Narain Sohl-Dickstein",
      "Jason Phang",
      "Jason Wei",
      "J. Yosinski",
      "Jekaterina Novikova",
      "Jelle Bosscher",
      "Jennifer Marsh",
      "Jeremy Kim",
      "Jeroen Taal",
      "Jesse Engel",
      "Jesujoba Oluwadara Alabi",
      "Jiacheng Xu",
      "Jiaming Song",
      "Jillian Tang",
      "Jane W Waweru",
      "John Burden",
      "John Miller",
      "John U. Balis",
      "Jonathan Batchelder",
      "Jonathan Berant",
      "Jorg Frohberg",
      "Jos Rozen",
      "J. Hernández-Orallo",
      "Joseph Boudeman",
      "J. Guerr",
      "Joseph Jones",
      "Joshua B. Tenenbaum",
      "Joshua S. Rule",
      "Joyce Chua",
      "Kamil Kanclerz",
      "Karen Livescu",
      "K. Krauth",
      "Karthik Gopalakrishnan",
      "Katerina Ignatyeva",
      "K. Markert",
      "Kaustubh D. Dhole",
      "Kevin Gimpel",
      "Kevin Omondi",
      "K. Mathewson",
      "Kristen Chiafullo",
      "Ksenia Shkaruta",
      "Kumar Shridhar",
      "Kyle McDonell",
      "Kyle Richardson",
      "Laria Reynolds",
      "Leo Gao",
      "Li Zhang",
      "Liam Dugan",
      "Lianhui Qin",
      "Lidia Contreras-Ochando",
      "Louis-philippe Morency",
      "Luca Moschella",
      "Luca Lam",
      "Lucy Noble",
      "Ludwig Schmidt",
      "Luheng He",
      "Luis Oliveros Col'on",
      "Luke Metz",
      "Lutfi Kerem cSenel",
      "Maarten Bosma",
      "Maarten Sap",
      "Maartje ter Hoeve",
      "Maheen Farooqi",
      "Manaal Faruqui",
      "Mantas Mazeika",
      "Marco Baturan",
      "Marco Marelli",
      "Marco Maru",
      "Maria Jose Ram’irez Quintana",
      "M. Tolkiehn",
      "Mario Giulianelli",
      "Martha Lewis",
      "Martin Potthast",
      "Matthew L. Leavitt",
      "Matthias Hagen",
      "M. Schubert",
      "Medina Baitemirova",
      "Melody Arnaud",
      "M. McElrath",
      "Michael A. Yee",
      "Michael Cohen",
      "Michael Gu",
      "Michael Ivanitskiy",
      "Michael Starritt",
      "M. Strube",
      "Michal Swkedrowski",
      "Michele Bevilacqua",
      "Michihiro Yasunaga",
      "Mihir Kale",
      "Mike Cain",
      "Mimee Xu",
      "Mirac Suzgun",
      "Mitch Walker",
      "Monica Tiwari",
      "Mohit Bansal",
      "Moin Aminnaseri",
      "Mor Geva",
      "Mozhdeh Gheini",
      "T. MukundVarma",
      "Nanyun Peng",
      "Nathan A. Chi",
      "Nayeon Lee",
      "Neta Gur-Ari Krakover",
      "Nicholas Cameron",
      "Nicholas Roberts",
      "Nick Doiron",
      "Nicole Martinez",
      "Nikita Nangia",
      "Niklas Deckers",
      "Niklas Muennighoff",
      "N. Keskar",
      "Niveditha Iyer",
      "Noah Constant",
      "Noah Fiedel",
      "Nuan Wen",
      "Oliver Zhang",
      "Omar Agha",
      "Omar Elbaghdadi",
      "Omer Levy",
      "Owain Evans",
      "Pablo Antonio Moreno Casares",
      "P. Doshi",
      "Pascale Fung",
      "Paul Pu Liang",
      "Paul Vicol",
      "Pegah Alipoormolabashi",
      "Peiyuan Liao",
      "Percy Liang",
      "Peter Chang",
      "P. Eckersley",
      "Phu Mon Htut",
      "P. Hwang",
      "P. Milkowski",
      "P. Patil",
      "Pouya Pezeshkpour",
      "Priti Oli",
      "Qiaozhu Mei",
      "Qing Lyu",
      "Qinlang Chen",
      "Rabin Banjade",
      "Rachel Etta Rudolph",
      "Raefer Gabriel",
      "Rahel Habacker",
      "Ramon Risco",
      "Raphael Milliere",
      "Rhythm Garg",
      "Richard Barnes",
      "R. Saurous",
      "Riku Arakawa",
      "Robbe Raymaekers",
      "Robert Frank",
      "Rohan Sikand",
      "Roman Novak",
      "Roman Sitelew",
      "Ronan Le Bras",
      "Rosanne Liu",
      "Rowan Jacobs",
      "Rui Zhang",
      "R. Salakhutdinov",
      "Ryan Chi",
      "Ryan Lee",
      "Ryan Stovall",
      "R. Teehan",
      "Rylan Yang",
      "Sahib Singh",
      "Saif Mohammad",
      "Sajant Anand",
      "Sam Dillavou",
      "Sam Shleifer",
      "Sam Wiseman",
      "Samuel Gruetter",
      "Samuel R. Bowman",
      "S. Schoenholz",
      "Sanghyun Han",
      "Sanjeev Kwatra",
      "Sarah A. Rous",
      "Sarik Ghazarian",
      "Sayan Ghosh",
      "Sean Casey",
      "Sebastian Bischoff",
      "Sebastian Gehrmann",
      "Sebastian Schuster",
      "Sepideh Sadeghi",
      "Shadi S. Hamdan",
      "Sharon Zhou",
      "Shashank Srivastava",
      "Sherry Shi",
      "Shikhar Singh",
      "Shima Asaadi",
      "S. Gu",
      "Shubh Pachchigar",
      "Shubham Toshniwal",
      "Shyam Upadhyay",
      "Shyamolima Debnath",
      "Siamak Shakeri",
      "Simon Thormeyer",
      "S. Melzi",
      "Siva Reddy",
      "S. Makini",
      "Soo-Hwan Lee",
      "Spencer Bradley Torene",
      "Sriharsha Hatwar",
      "S. Dehaene",
      "Stefan Divic",
      "Stefano Ermon",
      "Stella Biderman",
      "Stephanie Lin",
      "Stephen Prasad",
      "Steven T Piantadosi",
      "Stuart M. Shieber",
      "Summer Misherghi",
      "S. Kiritchenko",
      "Swaroop Mishra",
      "Tal Linzen",
      "Tal Schuster",
      "Tao Li",
      "Tao Yu",
      "Tariq Ali",
      "Tatsunori Hashimoto",
      "Te-Lin Wu",
      "T. Desbordes",
      "Theodore Rothschild",
      "Thomas Phan",
      "Tianle Wang",
      "Tiberius Nkinyili",
      "Timo Schick",
      "T. Kornev",
      "T. Tunduny",
      "Tobias Gerstenberg",
      "T. Chang",
      "Trishala Neeraj",
      "Tushar Khot",
      "Tyler Shultz",
      "Uri Shaham",
      "Vedant Misra",
      "Vera Demberg",
      "Victoria Nyamai",
      "Vikas Raunak",
      "V. Ramasesh",
      "Vinay Uday Prabhu",
      "Vishakh Padmakumar",
      "Vivek Srikumar",
      "W. Fedus",
      "W. Saunders",
      "William Zhang",
      "Wout Vossen",
      "Xiang Ren",
      "Xiaoyu Tong",
      "Xinran Zhao",
      "Xinyi Wu",
      "Xudong Shen",
      "Yadollah Yaghoobzadeh",
      "Yair Lakretz",
      "Yangqiu Song",
      "Yasaman Bahri",
      "Yejin Choi",
      "Yichi Yang",
      "Yiding Hao",
      "Yifu Chen",
      "Yonatan Belinkov",
      "Yu Hou",
      "Yu Hou",
      "Yuntao Bai",
      "Zachary Seid",
      "Zhuoye Zhao",
      "Zijian Wang",
      "Zijie J. Wang",
      "Zirui Wang",
      "Ziyi Wu"
    ],
    "abstract":"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-06-09",
    "citation_count":2052,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-04615",
      "ArXiv":"2206.04615",
      "CorpusId":263625818
    }
  },
  {
    "paper_id":"d0086b86103a620a86bc918746df0aa642e2a8a3",
    "title":"Language Models as Knowledge Bases?",
    "authors":[
      "F. Petroni",
      "Tim Rocktäschel",
      "Patrick Lewis",
      "A. Bakhtin",
      "Yuxiang Wu",
      "Alexander H. Miller",
      "Sebastian Riedel"
    ],
    "abstract":"Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https:\/\/github.com\/facebookresearch\/LAMA.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-09-01",
    "citation_count":2919,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1909-01066",
      "MAG":"2996758945",
      "ArXiv":"1909.01066",
      "ACL":"D19-1250",
      "DOI":"10.18653\/v1\/D19-1250",
      "CorpusId":202539551
    }
  },
  {
    "paper_id":"94a5f96308729e31c1ffbc0f0618db87795092fe",
    "title":"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
    "authors":[
      "Carlos E. Jimenez",
      "John Yang",
      "Alexander Wettig",
      "Shunyu Yao",
      "Kexin Pei",
      "Ofir Press",
      "Karthik Narasimhan"
    ],
    "abstract":"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-10-10",
    "citation_count":1110,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/JimenezYWYPPN24",
      "ArXiv":"2310.06770",
      "DOI":"10.48550\/arXiv.2310.06770",
      "CorpusId":263829697
    }
  },
  {
    "paper_id":"fdc53c2c10742464087c0525f77e32604827a21d",
    "title":"Efficient Streaming Language Models with Attention Sinks",
    "authors":[
      "Guangxuan Xiao",
      "Yuandong Tian",
      "Beidi Chen",
      "Song Han",
      "Mike Lewis"
    ],
    "abstract":"Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https:\/\/github.com\/mit-han-lab\/streaming-llm.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-09-29",
    "citation_count":1125,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/XiaoTCHL24",
      "ArXiv":"2309.17453",
      "DOI":"10.48550\/arXiv.2309.17453",
      "CorpusId":263310483
    }
  },
  {
    "paper_id":"206400aba5f12f734cdd2e4ab48ef6014ea60773",
    "title":"Evaluating Object Hallucination in Large Vision-Language Models",
    "authors":[
      "Yifan Li",
      "Yifan Du",
      "Kun Zhou",
      "Jinpeng Wang",
      "Wayne Xin Zhao",
      "Ji-rong Wen"
    ],
    "abstract":"Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https:\/\/github.com\/RUCAIBox\/POPE.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-17",
    "citation_count":1136,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-10355",
      "ArXiv":"2305.10355",
      "DOI":"10.48550\/arXiv.2305.10355",
      "CorpusId":258740697
    }
  },
  {
    "paper_id":"08a80cb34d785258c770acecd302ab41ead46eed",
    "title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
    "authors":[
      "Can Xu",
      "Qingfeng Sun",
      "Kai Zheng",
      "Xiubo Geng",
      "Pu Zhao",
      "Jiazhan Feng",
      "Chongyang Tao",
      "Daxin Jiang"
    ],
    "abstract":"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https:\/\/github.com\/nlpxucan\/WizardLM",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-04-24",
    "citation_count":1133,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/XuSZG0FTLJ24",
      "ArXiv":"2304.12244",
      "CorpusId":258298159
    }
  },
  {
    "paper_id":"4637f79ddfaf923ce569996ffa5b6cda1996faa1",
    "title":"Jailbreaking Black Box Large Language Models in Twenty Queries",
    "authors":[
      "Patrick Chao",
      "Alexander Robey",
      "Edgar Dobriban",
      "Hamed Hassani",
      "George J. Pappas",
      "Eric Wong"
    ],
    "abstract":"There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR—which is inspired by social engineering attacks—uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5\/4, Vicuna, and Gemini.",
    "venue":"2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
    "year":2023,
    "publication_date":"2023-10-12",
    "citation_count":987,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/satml\/ChaoRDHP025",
      "ArXiv":"2310.08419",
      "DOI":"10.1109\/SaTML64287.2025.00010",
      "CorpusId":263908890
    }
  },
  {
    "paper_id":"7e32aac43e9f1df49e116add03327ee6f365dbf3",
    "title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
    "authors":[
      "Qinghao Ye",
      "Haiyang Xu",
      "Guohai Xu",
      "Jiabo Ye",
      "Ming Yan",
      "Yi Zhou",
      "Junyan Wang",
      "Anwen Hu",
      "Pengcheng Shi",
      "Yaya Shi",
      "Chenliang Li",
      "Yuanhong Xu",
      "Hehong Chen",
      "Junfeng Tian",
      "Qiang Qi",
      "Ji Zhang",
      "Feiyan Huang"
    ],
    "abstract":"Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https:\/\/github.com\/X-PLUG\/mPLUG-Owl. The online demo is available at https:\/\/www.modelscope.cn\/studios\/damo\/mPLUG-Owl.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-04-27",
    "citation_count":1122,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2304-14178",
      "ArXiv":"2304.14178",
      "DOI":"10.48550\/arXiv.2304.14178",
      "CorpusId":258352455
    }
  },
  {
    "paper_id":"ac45bbf9940512d9d686cf8cd3a95969bc313570",
    "title":"OLMo: Accelerating the Science of Language Models",
    "authors":[
      "Dirk Groeneveld",
      "Iz Beltagy",
      "Pete Walsh",
      "Akshita Bhagia",
      "Rodney Kinney",
      "Oyvind Tafjord",
      "A. Jha",
      "Hamish Ivison",
      "Ian Magnusson",
      "Yizhong Wang",
      "Shane Arora",
      "David Atkinson",
      "Russell Authur",
      "Khyathi Raghavi Chandu",
      "Arman Cohan",
      "Jennifer Dumas",
      "Yanai Elazar",
      "Yuling Gu",
      "Jack Hessel",
      "Tushar Khot",
      "William Merrill",
      "Jacob Daniel Morrison",
      "Niklas Muennighoff",
      "Aakanksha Naik",
      "Crystal Nam",
      "Matthew E. Peters",
      "Valentina Pyatkin",
      "Abhilasha Ravichander",
      "Dustin Schwenk",
      "Saurabh Shah",
      "Will Smith",
      "Emma Strubell",
      "Nishant Subramani",
      "Mitchell Wortsman",
      "Pradeep Dasigi",
      "Nathan Lambert",
      "Kyle Richardson",
      "Luke S. Zettlemoyer",
      "Jesse Dodge",
      "Kyle Lo",
      "Luca Soldaini",
      "Noah A. Smith",
      "Hanna Hajishirzi"
    ],
    "abstract":"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2024,
    "publication_date":"2024-02-01",
    "citation_count":524,
    "reference_count":99,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2402-00838",
      "ArXiv":"2402.00838",
      "DOI":"10.48550\/arXiv.2402.00838",
      "CorpusId":267365485
    }
  },
  {
    "paper_id":"a1f76db91c0debcf93ae9889736bce8470902113",
    "title":"Large Language Models: A Survey",
    "authors":[
      "Shervin Minaee",
      "Tomáš Mikolov",
      "Narjes Nikzad",
      "M. Chenaghlu",
      "R. Socher",
      "Xavier Amatriain",
      "Jianfeng Gao"
    ],
    "abstract":"Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-02-09",
    "citation_count":677,
    "reference_count":240,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2402.06196",
      "DBLP":"journals\/corr\/abs-2402-06196",
      "DOI":"10.48550\/arXiv.2402.06196",
      "CorpusId":267617032
    }
  },
  {
    "paper_id":"b3848d32f7294ec708627897833c4097eb4d8778",
    "title":"LaMDA: Language Models for Dialog Applications",
    "authors":[
      "R. Thoppilan",
      "Daniel De Freitas",
      "Jamie Hall",
      "Noam M. Shazeer",
      "Apoorv Kulshreshtha",
      "Heng-Tze Cheng",
      "Alicia Jin",
      "Taylor Bos",
      "Leslie Baker",
      "Yu Du",
      "Yaguang Li",
      "Hongrae Lee",
      "H. Zheng",
      "Amin Ghafouri",
      "Marcelo Menegali",
      "Yanping Huang",
      "M. Krikun",
      "Dmitry Lepikhin",
      "James Qin",
      "Dehao Chen",
      "Yuanzhong Xu",
      "Zhifeng Chen",
      "Adam Roberts",
      "Maarten Bosma",
      "Yanqi Zhou",
      "Chung-Ching Chang",
      "I. Krivokon",
      "W. Rusch",
      "Marc Pickett",
      "K. Meier-Hellstern",
      "M. Morris",
      "Tulsee Doshi",
      "Renelito Delos Santos",
      "Toju Duke",
      "J. Søraker",
      "Ben Zevenbergen",
      "Vinodkumar Prabhakaran",
      "Mark Díaz",
      "Ben Hutchinson",
      "Kristen Olson",
      "Alejandra Molina",
      "Erin Hoffman-John",
      "Josh Lee",
      "Lora Aroyo",
      "Ravi Rajakumar",
      "Alena Butryna",
      "Matthew Lamm",
      "V. Kuzmina",
      "Joseph Fenton",
      "Aaron Cohen",
      "R. Bernstein",
      "R. Kurzweil",
      "Blaise Aguera-Arcas",
      "Claire Cui",
      "M. Croak",
      "Ed H. Chi",
      "Quoc Le"
    ],
    "abstract":"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-01-20",
    "citation_count":1746,
    "reference_count":120,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2201-08239",
      "ArXiv":"2201.08239",
      "CorpusId":246063428
    }
  },
  {
    "paper_id":"f8d357d38bbcdd93889fe71762eb57842b2ab063",
    "title":"Simple and Effective Masked Diffusion Language Models",
    "authors":[
      "S. Sahoo",
      "Marianne Arriola",
      "Yair Schiff",
      "Aaron Gokaslan",
      "Edgar Marroquin",
      "Justin T Chiu",
      "Alexander Rush",
      "Volodymyr Kuleshov"
    ],
    "abstract":"While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code, along with a blog post and video tutorial on the project page: https:\/\/s-sahoo.com\/mdlm",
    "venue":"Neural Information Processing Systems",
    "year":2024,
    "publication_date":"2024-06-11",
    "citation_count":294,
    "reference_count":73,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/SahooASGMCRK24",
      "ArXiv":"2406.07524",
      "DOI":"10.48550\/arXiv.2406.07524",
      "CorpusId":270380319
    }
  },
  {
    "paper_id":"697e0add95e880bd42e00bef838181e105f91981",
    "title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
    "authors":[
      "Chaoyou Fu",
      "Peixian Chen",
      "Yunhang Shen",
      "Yulei Qin",
      "Mengdan Zhang",
      "Xu Lin",
      "Zhenyu Qiu",
      "Wei Lin",
      "Jinrui Yang",
      "Xiawu Zheng",
      "Ke Li",
      "Xing Sun",
      "Rongrong Ji"
    ],
    "abstract":"Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data are released at the project page https:\/\/github.com\/BradyFU\/Awesome-Multimodal-Large-Language-Models\/tree\/Evaluation.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-06-23",
    "citation_count":1121,
    "reference_count":70,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2306.13394",
      "DBLP":"journals\/corr\/abs-2306-13394",
      "DOI":"10.48550\/arXiv.2306.13394",
      "CorpusId":259243928
    }
  },
  {
    "paper_id":"7260442ef9c0448f07ce3803efd49cebaffcebe9",
    "title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "authors":[
      "DeepSeek-AI Xiao Bi",
      "Deli Chen",
      "Guanting Chen",
      "Shanhuang Chen",
      "Damai Dai",
      "C. Deng",
      "Honghui Ding",
      "Kai Dong",
      "Qiushi Du",
      "Zhe Fu",
      "Huazuo Gao",
      "Kaige Gao",
      "Wenjun Gao",
      "Ruiqi Ge",
      "Kang Guan",
      "Daya Guo",
      "Jianzhong Guo",
      "Guangbo Hao",
      "Zhewen Hao",
      "Ying He",
      "Wen-Hui Hu",
      "Panpan Huang",
      "Erhang Li",
      "Guowei Li",
      "Jiashi Li",
      "Yao Li",
      "Y. K. Li",
      "W. Liang",
      "Fangyun Lin",
      "A. Liu",
      "Bo Liu (Benjamin Liu)",
      "Wen Liu",
      "Xiaodong Liu",
      "Xin Liu",
      "Yiyuan Liu",
      "Haoyu Lu",
      "Shanghao Lu",
      "Fuli Luo",
      "Shirong Ma",
      "X. Nie",
      "Tian Pei",
      "Yishi Piao",
      "Junjie Qiu",
      "Hui Qu",
      "Tongzheng Ren",
      "Z. Ren",
      "C. Ruan",
      "Zhangli Sha",
      "Zhihong Shao",
      "Jun-Mei Song",
      "Xuecheng Su",
      "Jingxiang Sun",
      "Yaofeng Sun",
      "Min Tang",
      "Bing-Li Wang",
      "Peiyi Wang",
      "Shiyu Wang",
      "Yaohui Wang",
      "Yongji Wang",
      "Tong Wu",
      "Yu Wu",
      "Xin Xie",
      "Zhenda Xie",
      "Ziwei Xie",
      "Yi Xiong",
      "Hanwei Xu",
      "R. X. Xu",
      "Yanhong Xu",
      "Dejian Yang",
      "Yu-mei You",
      "Shuiping Yu",
      "Xin-yuan Yu",
      "Bo Zhang",
      "Haowei Zhang",
      "Lecong Zhang",
      "Liyue Zhang",
      "Mingchuan Zhang",
      "Minghu Zhang",
      "Wentao Zhang",
      "Yichao Zhang",
      "Chenggang Zhao",
      "Yao Zhao",
      "Shangyan Zhou",
      "Shunfeng Zhou",
      "Qihao Zhu",
      "Yuheng Zou"
    ],
    "abstract":"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-01-05",
    "citation_count":561,
    "reference_count":71,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2401.02954",
      "DBLP":"journals\/corr\/abs-2401-02954",
      "CorpusId":266818336
    }
  },
  {
    "paper_id":"ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
    "title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
    "authors":[
      "Zixiang Chen",
      "Yihe Deng",
      "Huizhuo Yuan",
      "Kaixuan Ji",
      "Quanquan Gu"
    ],
    "abstract":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available at https:\/\/github.com\/uclaml\/SPIN.",
    "venue":"International Conference on Machine Learning",
    "year":2024,
    "publication_date":"2024-01-02",
    "citation_count":414,
    "reference_count":99,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"conf\/icml\/ChenDYJG24",
      "ArXiv":"2401.01335",
      "DOI":"10.48550\/arXiv.2401.01335",
      "CorpusId":266725672
    }
  },
  {
    "paper_id":"4780d0a027c5c5a8e01d7cf697f6296880ffc945",
    "title":"Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "authors":[
      "Yilun Du",
      "Shuang Li",
      "A. Torralba",
      "J. Tenenbaum",
      "Igor Mordatch"
    ],
    "abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-05-23",
    "citation_count":1076,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-14325",
      "ArXiv":"2305.14325",
      "DOI":"10.48550\/arXiv.2305.14325",
      "CorpusId":258841118
    }
  },
  {
    "paper_id":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
    "title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "authors":[
      "Yujia Qin",
      "Shi Liang",
      "Yining Ye",
      "Kunlun Zhu",
      "Lan Yan",
      "Ya-Ting Lu",
      "Yankai Lin",
      "Xin Cong",
      "Xiangru Tang",
      "Bill Qian",
      "Sihan Zhao",
      "Runchu Tian",
      "Ruobing Xie",
      "Jie Zhou",
      "Marc H. Gerstein",
      "Dahai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract":"Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-07-31",
    "citation_count":1001,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2307-16789",
      "ArXiv":"2307.16789",
      "DOI":"10.48550\/arXiv.2307.16789",
      "CorpusId":260334759
    }
  },
  {
    "paper_id":"f197bf0fc2f228483f6af3285000d54d8d97f9eb",
    "title":"Voyager: An Open-Ended Embodied Agent with Large Language Models",
    "authors":[
      "Guanzhi Wang",
      "Yuqi Xie",
      "Yunfan Jiang",
      "Ajay Mandlekar",
      "Chaowei Xiao",
      "Yuke Zhu",
      "Linxi (Jim) Fan",
      "Anima Anandkumar"
    ],
    "abstract":"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https:\/\/voyager.minedojo.org\/.",
    "venue":"Trans. Mach. Learn. Res.",
    "year":2023,
    "publication_date":"2023-05-25",
    "citation_count":1092,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.16291",
      "DBLP":"journals\/corr\/abs-2305-16291",
      "DOI":"10.48550\/arXiv.2305.16291",
      "CorpusId":258887849
    }
  },
  {
    "paper_id":"ecdd53eaab7455daea27609b07a418a21aa7ad35",
    "title":"Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    "authors":[
      "Seungone Kim",
      "Juyoung Suk",
      "Shayne Longpre",
      "Bill Yuchen Lin",
      "Jamin Shin",
      "S. Welleck",
      "Graham Neubig",
      "Moontae Lee",
      "Kyungjae Lee",
      "Minjoon Seo"
    ],
    "abstract":"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2024,
    "publication_date":"2024-05-02",
    "citation_count":293,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2405.01535",
      "DBLP":"conf\/emnlp\/KimSLLSWNL0S24",
      "ACL":"2024.emnlp-main.248",
      "DOI":"10.48550\/arXiv.2405.01535",
      "CorpusId":269502688
    }
  },
  {
    "paper_id":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
    "title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
    "authors":[
      "Wenlong Huang",
      "P. Abbeel",
      "Deepak Pathak",
      "Igor Mordatch"
    ],
    "abstract":"Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https:\/\/huangwl18.github.io\/language-planner",
    "venue":"International Conference on Machine Learning",
    "year":2022,
    "publication_date":"2022-01-18",
    "citation_count":1326,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2201-07207",
      "ArXiv":"2201.07207",
      "CorpusId":246035276
    }
  },
  {
    "paper_id":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
    "title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "authors":[
      "Shirui Pan",
      "Linhao Luo",
      "Yufei Wang",
      "Chen Chen",
      "Jiapu Wang",
      "Xindong Wu"
    ],
    "abstract":"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
    "venue":"IEEE Transactions on Knowledge and Data Engineering",
    "year":2023,
    "publication_date":"2023-06-14",
    "citation_count":1055,
    "reference_count":300,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2306.08302",
      "DBLP":"journals\/corr\/abs-2306-08302",
      "DOI":"10.1109\/TKDE.2024.3352100",
      "CorpusId":259165563
    }
  },
  {
    "paper_id":"ca31b8584b6c022ef15ddfe994fe361e002b7729",
    "title":"A Comprehensive Overview of Large Language Models",
    "authors":[
      "Humza Naveed",
      "Asad Ullah Khan",
      "Shi Qiu",
      "Muhammad Saqib",
      "Saeed Anwar",
      "Muhammad Usman",
      "Nick Barnes",
      "A. Mian"
    ],
    "abstract":"Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multimodal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.",
    "venue":"ACM Transactions on Intelligent Systems and Technology",
    "year":2023,
    "publication_date":"2023-07-12",
    "citation_count":1012,
    "reference_count":529,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tist\/NaveedKQSAUABM25",
      "ArXiv":"2307.06435",
      "DOI":"10.1145\/3744746",
      "CorpusId":259847443
    }
  },
  {
    "paper_id":"04d64be16fb402f28348faffef484bd419c8bd8f",
    "title":"Self-Rewarding Language Models",
    "authors":[
      "Weizhe Yuan",
      "Richard Yuanzhe Pang",
      "Kyunghyun Cho",
      "Sainbayar Sukhbaatar",
      "Jing Xu",
      "Jason E. Weston"
    ],
    "abstract":"We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.",
    "venue":"International Conference on Machine Learning",
    "year":2024,
    "publication_date":"2024-01-18",
    "citation_count":423,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2401.10020",
      "DBLP":"journals\/corr\/abs-2401-10020",
      "DOI":"10.48550\/arXiv.2401.10020",
      "CorpusId":267035293
    }
  },
  {
    "paper_id":"fb4dc0178e5d7347b1615c48caf05347b6e5eb48",
    "title":"TrustLLM: Trustworthiness in Large Language Models",
    "authors":[
      "Lichao Sun",
      "Yue Huang",
      "Haoran Wang",
      "Siyuan Wu",
      "Qihui Zhang",
      "Chujie Gao",
      "Yixin Huang",
      "Wenhan Lyu",
      "Yixuan Zhang",
      "Xiner Li",
      "Zheng Liu",
      "Yixin Liu",
      "Yijue Wang",
      "Zhikun Zhang",
      "B. Kailkhura",
      "Caiming Xiong",
      "Chaowei Xiao",
      "Chun-Yan Li",
      "Eric P. Xing",
      "Furong Huang",
      "Haodong Liu",
      "Heng Ji",
      "Hongyi Wang",
      "Huan Zhang",
      "Huaxiu Yao",
      "M. Kellis",
      "M. Zitnik",
      "Meng Jiang",
      "Mohit Bansal",
      "James Zou",
      "Jian Pei",
      "Jian Liu",
      "Jianfeng Gao",
      "Jiawei Han",
      "Jieyu Zhao",
      "Jiliang Tang",
      "Jindong Wang",
      "John Mitchell",
      "Kai Shu",
      "Kaidi Xu",
      "Kai-Wei Chang",
      "Lifang He",
      "Lifu Huang",
      "M. Backes",
      "Neil Zhenqiang Gong",
      "Philip S. Yu",
      "Pin-Yu Chen",
      "Quanquan Gu",
      "Ran Xu",
      "Rex Ying",
      "Shuiwang Ji",
      "S. Jana",
      "Tian-Xiang Chen",
      "Tianming Liu",
      "Tianying Zhou",
      "William Wang",
      "Xiang Li",
      "Xiang-Yu Zhang",
      "Xiao Wang",
      "Xingyao Xie",
      "Xun Chen",
      "Xuyu Wang",
      "Yan Liu",
      "Yanfang Ye",
      "Yinzhi Cao",
      "Yue Zhao"
    ],
    "abstract":"Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-01-10",
    "citation_count":268,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2401-05561",
      "ArXiv":"2401.05561",
      "DOI":"10.48550\/arXiv.2401.05561",
      "CorpusId":266933236
    }
  },
  {
    "paper_id":"5437e8adab596d7294124c0e798708e050e25321",
    "title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
    "authors":[
      "Denny Zhou",
      "Nathanael Scharli",
      "Le Hou",
      "Jason Wei",
      "Nathan Scales",
      "Xuezhi Wang",
      "D. Schuurmans",
      "O. Bousquet",
      "Quoc Le",
      "Ed H. Chi"
    ],
    "abstract":"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-05-21",
    "citation_count":1392,
    "reference_count":74,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2205.10625",
      "DBLP":"conf\/iclr\/ZhouSHWS0SCBLC23",
      "DOI":"10.48550\/arXiv.2205.10625",
      "CorpusId":248986239
    }
  },
  {
    "paper_id":"8323c591e119eb09b28b29fd6c7bc76bd889df7a",
    "title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "authors":[
      "M. Shoeybi",
      "M. Patwary",
      "Raul Puri",
      "P. LeGresley",
      "J. Casper",
      "Bryan Catanzaro"
    ],
    "abstract":"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
    "venue":"arXiv.org",
    "year":2019,
    "publication_date":"2019-09-17",
    "citation_count":2284,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2973727699",
      "ArXiv":"1909.08053",
      "DBLP":"journals\/corr\/abs-1909-08053",
      "CorpusId":202660670
    }
  },
  {
    "paper_id":"c96297261467b5daa2d01227496a70d444602434",
    "title":"Baichuan 2: Open Large-scale Language Models",
    "authors":[
      "Ai Ming Yang",
      "Bin Xiao",
      "Bingning Wang",
      "Borong Zhang",
      "Ce Bian",
      "Chao Yin",
      "Chenxu Lv",
      "Da Pan",
      "Dian Wang",
      "Dong Yan",
      "Fan Yang",
      "Fei Deng",
      "Feng Wang",
      "Feng Liu",
      "Guangwei Ai",
      "Guosheng Dong",
      "Hai Zhao",
      "Hang Xu",
      "Hao-Lun Sun",
      "Hongda Zhang",
      "Hui Liu",
      "Jiaming Ji",
      "Jian Xie",
      "Juntao Dai",
      "Kuncheng Fang",
      "Lei Su",
      "Liang Song",
      "Lifeng Liu",
      "Liyun Ru",
      "Luyao Ma",
      "Mang Wang",
      "Mickel Liu",
      "Mingan Lin",
      "Nuolan Nie",
      "Pei Guo",
      "Ruiyang Sun",
      "Zhang Tao",
      "Tianpeng Li",
      "Tianyu Li",
      "Wei Cheng",
      "Weipeng Chen",
      "Xiangrong Zeng",
      "Xiaochuan Wang",
      "Xiaoxi Chen",
      "Xin Men",
      "Xin Yu",
      "Xuehai Pan",
      "Yan-Bin Shen",
      "Yiding Wang",
      "Yiyu Li",
      "Youxin Jiang",
      "Yuchen Gao",
      "Yupeng Zhang",
      "Zenan Zhou",
      "Zhiying Wu"
    ],
    "abstract":"Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-09-19",
    "citation_count":881,
    "reference_count":83,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2309-10305",
      "ArXiv":"2309.10305",
      "DOI":"10.48550\/arXiv.2309.10305",
      "CorpusId":261951743
    }
  },
  {
    "paper_id":"cb5b71a622aff47014d4f28a958679629a8b6363",
    "title":"A Watermark for Large Language Models",
    "authors":[
      "John Kirchenbauer",
      "Jonas Geiping",
      "Yuxin Wen",
      "Jonathan Katz",
      "Ian Miers",
      "T. Goldstein"
    ],
    "abstract":"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-01-24",
    "citation_count":673,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2301.10226",
      "DBLP":"conf\/icml\/KirchenbauerGWK23",
      "DOI":"10.48550\/arXiv.2301.10226",
      "CorpusId":256194179
    }
  },
  {
    "paper_id":"d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
    "title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models",
    "authors":[
      "Tony Zhao",
      "Eric Wallace",
      "Shi Feng",
      "D. Klein",
      "Sameer Singh"
    ],
    "abstract":"GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N\/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
    "venue":"International Conference on Machine Learning",
    "year":2021,
    "publication_date":"2021-02-19",
    "citation_count":1612,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3132736064",
      "ArXiv":"2102.09690",
      "DBLP":"journals\/corr\/abs-2102-09690",
      "CorpusId":231979430
    }
  },
  {
    "paper_id":"a122863d239643453195424c04067e89406246e1",
    "title":"Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    "authors":[
      "Ning Ding",
      "Yulin Chen",
      "Bokai Xu",
      "Yujia Qin",
      "Zhi Zheng",
      "Shengding Hu",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Bowen Zhou"
    ],
    "abstract":"Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\\footnote{\\url{https:\/\/github.com\/thunlp\/UltraChat}}.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-23",
    "citation_count":695,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/DingCXQHL0Z23",
      "ArXiv":"2305.14233",
      "DOI":"10.48550\/arXiv.2305.14233",
      "CorpusId":258840897
    }
  },
  {
    "paper_id":"c2f91f35df893714418cc29096083dce0b441229",
    "title":"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "authors":[
      "Chengyi Wang",
      "Sanyuan Chen",
      "Yu Wu",
      "Zi-Hua Zhang",
      "Long Zhou",
      "Shujie Liu",
      "Zhuo Chen",
      "Yanqing Liu",
      "Huaming Wang",
      "Jinyu Li",
      "Lei He",
      "Sheng Zhao",
      "Furu Wei"
    ],
    "abstract":"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 50 k hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capability and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment from the prompt in synthesis.",
    "venue":"IEEE Transactions on Audio, Speech, and Language Processing",
    "year":2023,
    "publication_date":"2023-01-05",
    "citation_count":961,
    "reference_count":83,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "ArXiv":"2301.02111",
      "DBLP":"journals\/corr\/abs-2301-02111",
      "DOI":"10.1109\/TASLPRO.2025.3530270",
      "CorpusId":255440307
    }
  },
  {
    "paper_id":"454c8fef2957aa2fb13eb2c7a454393a2ee83805",
    "title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "authors":[
      "Ziyang Luo",
      "Can Xu",
      "Pu Zhao",
      "Qingfeng Sun",
      "Xiubo Geng",
      "Wenxiang Hu",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "abstract":"Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https:\/\/github.com\/nlpxucan\/WizardLM",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-06-14",
    "citation_count":821,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2306-08568",
      "ArXiv":"2306.08568",
      "CorpusId":259164815
    }
  },
  {
    "paper_id":"9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
    "title":"Teaching Large Language Models to Self-Debug",
    "authors":[
      "Xinyun Chen",
      "Maxwell Lin",
      "Nathanael Schärli",
      "Denny Zhou"
    ],
    "abstract":"Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-04-11",
    "citation_count":857,
    "reference_count":76,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2304.05128",
      "DBLP":"journals\/corr\/abs-2304-05128",
      "DOI":"10.48550\/arXiv.2304.05128",
      "CorpusId":258059885
    }
  },
  {
    "paper_id":"aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
    "title":"Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
    "authors":[
      "Maciej Besta",
      "Nils Blach",
      "Aleš Kubíček",
      "Robert Gerstenberger",
      "Lukas Gianinazzi",
      "Joanna Gajda",
      "Tomasz Lehmann",
      "Michal Podstawski",
      "H. Niewiadomski",
      "P. Nyczyk",
      "Torsten Hoefler"
    ],
    "abstract":"We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2023,
    "publication_date":"2023-08-18",
    "citation_count":961,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/BestaBKGPGGLNNH24",
      "ArXiv":"2308.09687",
      "DOI":"10.1609\/aaai.v38i16.29720",
      "CorpusId":261030303
    }
  },
  {
    "paper_id":"bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
    "title":"Bias and Fairness in Large Language Models: A Survey",
    "authors":[
      "Isabel O. Gallegos",
      "Ryan A. Rossi",
      "Joe Barrow",
      "Md. Mehrab Tanjim",
      "Sungchul Kim",
      "Franck Dernoncourt",
      "Tong Yu",
      "Ruiyi Zhang",
      "Nesreen Ahmed"
    ],
    "abstract":"Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
    "venue":"Computational Linguistics",
    "year":2023,
    "publication_date":"2023-09-02",
    "citation_count":835,
    "reference_count":256,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/coling\/GallegosRBTKDYZA24",
      "ArXiv":"2309.00770",
      "DOI":"10.1162\/coli_a_00524",
      "CorpusId":261530629
    }
  },
  {
    "paper_id":"07b14c24833400b79978b0a5f084803337e30a15",
    "title":"REPLUG: Retrieval-Augmented Black-Box Language Models",
    "authors":[
      "Weijia Shi",
      "Sewon Min",
      "Michihiro Yasunaga",
      "Minjoon Seo",
      "Rich James",
      "M. Lewis",
      "Luke Zettlemoyer",
      "Wen-tau Yih"
    ],
    "abstract":"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com\/swj0419\/REPLUG.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-01-30",
    "citation_count":798,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/ShiMYS0LZY24",
      "ACL":"2024.naacl-long.463",
      "ArXiv":"2301.12652",
      "DOI":"10.48550\/arXiv.2301.12652",
      "CorpusId":256389797
    }
  },
  {
    "paper_id":"385c74957858e7d6856d48e72b5a902b4c1aa28c",
    "title":"Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
    "authors":[
      "Tian Liang",
      "Zhiwei He",
      "Wenxiang Jiao",
      "Xing Wang",
      "Yan Wang",
      "Rui Wang",
      "Yujiu Yang",
      "Zhaopeng Tu",
      "Shuming Shi"
    ],
    "abstract":"Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-30",
    "citation_count":724,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-19118",
      "ACL":"2024.emnlp-main.992",
      "ArXiv":"2305.19118",
      "DOI":"10.48550\/arXiv.2305.19118",
      "CorpusId":258967540
    }
  },
  {
    "paper_id":"16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
    "title":"Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
    "authors":[
      "Ming Jin",
      "Shiyu Wang",
      "Lintao Ma",
      "Zhixuan Chu",
      "James Y. Zhang",
      "X. Shi",
      "Pin-Yu Chen",
      "Yuxuan Liang",
      "Yuan-Fang Li",
      "Shirui Pan",
      "Qingsong Wen"
    ],
    "abstract":"Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-10-03",
    "citation_count":642,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2310-01728",
      "ArXiv":"2310.01728",
      "DOI":"10.48550\/arXiv.2310.01728",
      "CorpusId":263609325
    }
  },
  {
    "paper_id":"3b6179c293df29e31d31cea46476f104ab6950f2",
    "title":"Kosmos-2: Grounding Multimodal Large Language Models to the World",
    "authors":[
      "Zhiliang Peng",
      "Wenhui Wang",
      "Li Dong",
      "Y. Hao",
      "Shaohan Huang",
      "Shuming Ma",
      "Furu Wei"
    ],
    "abstract":"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https:\/\/aka.ms\/kosmos-2.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-06-26",
    "citation_count":961,
    "reference_count":68,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2306-14824",
      "ArXiv":"2306.14824",
      "DOI":"10.48550\/arXiv.2306.14824",
      "CorpusId":259262263
    }
  },
  {
    "paper_id":"c04067f03fba2df0c14ea51a170f213eb2983708",
    "title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
    "authors":[
      "Peng Gao",
      "Shijie Geng",
      "Renrui Zhang",
      "Teli Ma",
      "Rongyao Fang",
      "Yongfeng Zhang",
      "Hongsheng Li",
      "Y. Qiao"
    ],
    "abstract":"Large-scale contrastive vision-language pretraining has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in Radford et al. (International conference on machine learning, PMLR, 2021) to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions. To avoid non-trivial prompt engineering, context optimization (Zhou et al. in Int J Comput Vis 130(9):2337–2348, 2022) has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples. In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning. While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pretrained features. As a consequence, CLIP-Adapter is able to outperform context optimization while maintaining a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach.",
    "venue":"International Journal of Computer Vision",
    "year":2021,
    "publication_date":"2021-10-09",
    "citation_count":1358,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2110.04544",
      "DBLP":"journals\/corr\/abs-2110-04544",
      "DOI":"10.1007\/s11263-023-01891-x",
      "CorpusId":238583492
    }
  },
  {
    "paper_id":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
    "title":"Large Language Models Can Be Easily Distracted by Irrelevant Context",
    "authors":[
      "Freda Shi",
      "Xinyun Chen",
      "Kanishka Misra",
      "Nathan Scales",
      "David Dohan",
      "Ed H. Chi",
      "Nathanael Scharli",
      "Denny Zhou"
    ],
    "abstract":"Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-01-31",
    "citation_count":762,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-00093",
      "ArXiv":"2302.00093",
      "DOI":"10.48550\/arXiv.2302.00093",
      "CorpusId":256459776
    }
  },
  {
    "paper_id":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
    "title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
    "authors":[
      "Wenlong Huang",
      "Chen Wang",
      "Ruohan Zhang",
      "Yunzhu Li",
      "Jiajun Wu",
      "Li Fei-Fei"
    ],
    "abstract":"Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https:\/\/voxposer.github.io",
    "venue":"Conference on Robot Learning",
    "year":2023,
    "publication_date":"2023-07-12",
    "citation_count":677,
    "reference_count":146,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/corl\/HuangWZL0023",
      "ArXiv":"2307.05973",
      "DOI":"10.48550\/arXiv.2307.05973",
      "CorpusId":259837330
    }
  },
  {
    "paper_id":"38d64919ba526868a850a0e5f6239d4c474b7e7e",
    "title":"Large Language Models are not Fair Evaluators",
    "authors":[
      "Peiyi Wang",
      "Lei Li",
      "Liang Chen",
      "Dawei Zhu",
      "Binghuai Lin",
      "Yunbo Cao",
      "Qi Liu",
      "Tianyu Liu",
      "Zhifang Sui"
    ],
    "abstract":"In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win\/tie\/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https:\/\/github.com\/i-Eval\/FairEval} to facilitate future research.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-05-29",
    "citation_count":733,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-17926",
      "ArXiv":"2305.17926",
      "DOI":"10.48550\/arXiv.2305.17926",
      "CorpusId":258960339
    }
  },
  {
    "paper_id":"9695824d7a01fad57ba9c01d7d76a519d78d65e7",
    "title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
    "authors":[
      "Chitwan Saharia",
      "William Chan",
      "Saurabh Saxena",
      "Lala Li",
      "Jay Whang",
      "Emily L. Denton",
      "Seyed Kamyar Seyed Ghasemipour",
      "Burcu Karagol Ayan",
      "S. S. Mahdavi",
      "Raphael Gontijo Lopes",
      "Tim Salimans",
      "Jonathan Ho",
      "David J. Fleet",
      "Mohammad Norouzi"
    ],
    "abstract":"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https:\/\/imagen.research.google\/ for an overview of the results.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-05-23",
    "citation_count":7148,
    "reference_count":108,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-11487",
      "ArXiv":"2205.11487",
      "DOI":"10.48550\/arXiv.2205.11487",
      "CorpusId":248986576
    }
  },
  {
    "paper_id":"a757999ed260d7bc45484dc6b4456bf33fe6f679",
    "title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
    "authors":[
      "Renrui Zhang",
      "Jiaming Han",
      "Aojun Zhou",
      "Xiangfei Hu",
      "Shilin Yan",
      "Pan Lu",
      "Hongsheng Li",
      "Peng Gao",
      "Y. Qiao"
    ],
    "abstract":"We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https:\/\/github.com\/OpenGVLab\/LLaMA-Adapter.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-03-28",
    "citation_count":905,
    "reference_count":138,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2303.16199",
      "DBLP":"journals\/corr\/abs-2303-16199",
      "DOI":"10.48550\/arXiv.2303.16199",
      "CorpusId":257771811
    }
  },
  {
    "paper_id":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
    "title":"Solving Quantitative Reasoning Problems with Language Models",
    "authors":[
      "Aitor Lewkowycz",
      "Anders Andreassen",
      "David Dohan",
      "Ethan Dyer",
      "H. Michalewski",
      "V. Ramasesh",
      "Ambrose Slone",
      "Cem Anil",
      "Imanol Schlag",
      "Theo Gutman-Solo",
      "Yuhuai Wu",
      "Behnam Neyshabur",
      "Guy Gur-Ari",
      "Vedant Misra"
    ],
    "abstract":"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-06-29",
    "citation_count":1208,
    "reference_count":70,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/LewkowyczADDMRS22",
      "ArXiv":"2206.14858",
      "DOI":"10.48550\/arXiv.2206.14858",
      "CorpusId":250144408
    }
  },
  {
    "paper_id":"2141ed804636a1cf339d606cd03fd3b3e9582133",
    "title":"VILA: On Pre-training for Visual Language Models",
    "authors":[
      "Ji Lin",
      "Hongxu Yin",
      "Wei Ping",
      "Yao Lu",
      "Pavlo Molchanov",
      "Andrew Tao",
      "Huizi Mao",
      "Jan Kautz",
      "M. Shoeybi",
      "Song Han"
    ],
    "abstract":"Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge. VILA is also deployable on Jetson Orin for on-device VLM.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2023,
    "publication_date":"2023-12-12",
    "citation_count":609,
    "reference_count":74,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/LinYP0SH24",
      "ArXiv":"2312.07533",
      "DOI":"10.1109\/CVPR52733.2024.02520",
      "CorpusId":266174746
    }
  },
  {
    "paper_id":"017010b941d902a467f6d329ae5e74fd67e67912",
    "title":"LLM-Pruner: On the Structural Pruning of Large Language Models",
    "authors":[
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ],
    "abstract":"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https:\/\/github.com\/horseee\/LLM-Pruner",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-19",
    "citation_count":609,
    "reference_count":75,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-11627",
      "ArXiv":"2305.11627",
      "DOI":"10.48550\/arXiv.2305.11627",
      "CorpusId":258823276
    }
  },
  {
    "paper_id":"68f141724814839d556a989646194be88641b143",
    "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
    "authors":[
      "Jack W. Rae",
      "Sebastian Borgeaud",
      "Trevor Cai",
      "Katie Millican",
      "Jordan Hoffmann",
      "Francis Song",
      "John Aslanides",
      "Sarah Henderson",
      "Roman Ring",
      "Susannah Young",
      "Eliza Rutherford",
      "Tom Hennigan",
      "Jacob Menick",
      "Albin Cassirer",
      "Richard Powell",
      "George van den Driessche",
      "Lisa Anne Hendricks",
      "Maribeth Rauh",
      "Po-Sen Huang",
      "Amelia Glaese",
      "Johannes Welbl",
      "Sumanth Dathathri",
      "Saffron Huang",
      "Jonathan Uesato",
      "John F. J. Mellor",
      "I. Higgins",
      "Antonia Creswell",
      "Nat McAleese",
      "Amy Wu",
      "Erich Elsen",
      "Siddhant M. Jayakumar",
      "Elena Buchatskaya",
      "D. Budden",
      "Esme Sutherland",
      "K. Simonyan",
      "Michela Paganini",
      "L. Sifre",
      "Lena Martens",
      "Xiang Lorraine Li",
      "A. Kuncoro",
      "Aida Nematzadeh",
      "E. Gribovskaya",
      "Domenic Donato",
      "Angeliki Lazaridou",
      "A. Mensch",
      "Jean-Baptiste Lespiau",
      "M. Tsimpoukelli",
      "N. Grigorev",
      "Doug Fritz",
      "Thibault Sottiaux",
      "Mantas Pajarskas",
      "Tobias Pohlen",
      "Z. Gong",
      "Daniel Toyama",
      "Cyprien de Masson d'Autume",
      "Yujia Li",
      "Tayfun Terzi",
      "Vladimir Mikulik",
      "Igor Babuschkin",
      "Aidan Clark",
      "Diego de Las Casas",
      "Aurelia Guy",
      "Chris Jones",
      "James Bradbury",
      "Matthew G. Johnson",
      "Blake A. Hechtman",
      "Laura Weidinger",
      "Iason Gabriel",
      "William S. Isaac",
      "Edward Lockhart",
      "Simon Osindero",
      "Laura Rimell",
      "Chris Dyer",
      "O. Vinyals",
      "Kareem W. Ayoub",
      "J. Stanway",
      "L. Bennett",
      "D. Hassabis",
      "K. Kavukcuoglu",
      "G. Irving"
    ],
    "abstract":"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-12-08",
    "citation_count":1472,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2112.11446",
      "DBLP":"journals\/corr\/abs-2112-11446",
      "CorpusId":245353475
    }
  },
  {
    "paper_id":"e38a29f6463f38f43797b128673b9e44d18a991e",
    "title":"Whose Opinions Do Language Models Reflect?",
    "authors":[
      "Shibani Santurkar",
      "Esin Durmus",
      "Faisal Ladhak",
      "Cinoo Lee",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "abstract":"Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https:\/\/github.com\/tatsu-lab\/opinions_qa.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-03-30",
    "citation_count":599,
    "reference_count":68,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icml\/SanturkarDLLLH23",
      "ArXiv":"2303.17548",
      "CorpusId":257834040
    }
  },
  {
    "paper_id":"31d2ccff82e313eb5c1620c44bb8322da4a38513",
    "title":"A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
    "authors":[
      "Pranab Sahoo",
      "Ayush Kumar Singh",
      "Sriparna Saha",
      "Vinija Jain",
      "S. Mondal",
      "Aman Chadha"
    ],
    "abstract":"Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-02-05",
    "citation_count":576,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2402.07927",
      "DBLP":"journals\/corr\/abs-2402-07927",
      "DOI":"10.48550\/arXiv.2402.07927",
      "CorpusId":267636769
    }
  },
  {
    "paper_id":"002c256d30d6be4b23d365a8de8ae0e67e4c9641",
    "title":"Improving language models by retrieving from trillions of tokens",
    "authors":[
      "Sebastian Borgeaud",
      "A. Mensch",
      "Jordan Hoffmann",
      "Trevor Cai",
      "Eliza Rutherford",
      "Katie Millican",
      "George van den Driessche",
      "Jean-Baptiste Lespiau",
      "Bogdan Damoc",
      "Aidan Clark",
      "Diego de Las Casas",
      "Aurelia Guy",
      "Jacob Menick",
      "Roman Ring",
      "T. Hennigan",
      "Saffron Huang",
      "Lorenzo Maggiore",
      "Chris Jones",
      "Albin Cassirer",
      "Andy Brock",
      "Michela Paganini",
      "G. Irving",
      "O. Vinyals",
      "Simon Osindero",
      "K. Simonyan",
      "Jack W. Rae",
      "Erich Elsen",
      "L. Sifre"
    ],
    "abstract":"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
    "venue":"International Conference on Machine Learning",
    "year":2021,
    "publication_date":"2021-12-08",
    "citation_count":1345,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2112-04426",
      "ArXiv":"2112.04426",
      "CorpusId":244954723
    }
  },
  {
    "paper_id":"7d22ad3573101337bca2091fb0114b377c4f3db6",
    "title":"A Simple and Effective Pruning Approach for Large Language Models",
    "authors":[
      "Mingjie Sun",
      "Zhuang Liu",
      "Anna Bair",
      "J. Z. Kolter"
    ],
    "abstract":"As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https:\/\/github.com\/locuslab\/wanda.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-06-20",
    "citation_count":595,
    "reference_count":107,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/Sun0BK24",
      "ArXiv":"2306.11695",
      "DOI":"10.48550\/arXiv.2306.11695",
      "CorpusId":259203115
    }
  },
  {
    "paper_id":"f8a2dca1e8fe56e698984c077f7ff58d8ca867e9",
    "title":"Large Language Models as Optimizers",
    "authors":[
      "Chengrun Yang",
      "Xuezhi Wang",
      "Yifeng Lu",
      "Hanxiao Liu",
      "Quoc V. Le",
      "Denny Zhou",
      "Xinyun Chen"
    ],
    "abstract":"Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https:\/\/github.com\/google-deepmind\/opro.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-09-07",
    "citation_count":591,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2309-03409",
      "ArXiv":"2309.03409",
      "DOI":"10.48550\/arXiv.2309.03409",
      "CorpusId":261582296
    }
  },
  {
    "paper_id":"1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
    "title":"Instruction-Following Evaluation for Large Language Models",
    "authors":[
      "Jeffrey Zhou",
      "Tianjian Lu",
      "Swaroop Mishra",
      "Siddhartha Brahma",
      "Sujoy Basu",
      "Yi Luan",
      "Denny Zhou",
      "Le Hou"
    ],
    "abstract":"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of\"verifiable instructions\"such as\"write in more than 400 words\"and\"mention the keyword of AI at least 3 times\". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https:\/\/github.com\/google-research\/google-research\/tree\/master\/instruction_following_eval",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-11-14",
    "citation_count":497,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2311.07911",
      "DBLP":"journals\/corr\/abs-2311-07911",
      "DOI":"10.48550\/arXiv.2311.07911",
      "CorpusId":265157752
    }
  },
  {
    "paper_id":"8e773b1840b894603c06b677a0f15ebcf0f26378",
    "title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
    "authors":[
      "Tao Yu",
      "Rui Zhang",
      "Kai-Chou Yang",
      "Michihiro Yasunaga",
      "Dongxu Wang",
      "Zifan Li",
      "James Ma",
      "Irene Z Li",
      "Qingning Yao",
      "Shanelle Roman",
      "Zilin Zhang",
      "Dragomir R. Radev"
    ],
    "abstract":"We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https:\/\/yale-lily.github.io\/seq2sql\/spider.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-09-24",
    "citation_count":1480,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/YuZYYWLMLYRZR18",
      "ACL":"D18-1425",
      "ArXiv":"1809.08887",
      "MAG":"2890431379",
      "DOI":"10.18653\/v1\/D18-1425",
      "CorpusId":52815560
    }
  },
  {
    "paper_id":"b29447ba499507a259ae9d8f685d60cc1597d7d3",
    "title":"Semantic Parsing on Freebase from Question-Answer Pairs",
    "authors":[
      "Jonathan Berant",
      "A. Chou",
      "Roy Frostig",
      "Percy Liang"
    ],
    "abstract":"In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2013,
    "publication_date":"2013-10-01",
    "citation_count":2094,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"D13-1160",
      "MAG":"2252136820",
      "DBLP":"conf\/emnlp\/BerantCFL13",
      "DOI":"10.18653\/v1\/d13-1160",
      "CorpusId":6401679
    }
  },
  {
    "paper_id":"b0cf4c64467a0529ca3d5be5dc326a21269257b6",
    "title":"Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques",
    "authors":[
      "Daking Rai",
      "Bailin Wang",
      "Yilun Zhou",
      "Ziyu Yao"
    ],
    "abstract":"Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM’s generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-05-27",
    "citation_count":34,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/RaiWZY23",
      "ArXiv":"2305.17378",
      "ACL":"2023.acl-short.15",
      "DOI":"10.48550\/arXiv.2305.17378",
      "CorpusId":258960278
    }
  },
  {
    "paper_id":"1c91b23d78944f7f237cb512029c2165972ae9d5",
    "title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
    "authors":[
      "Terry Yue Zhuo",
      "Zhuang Li",
      "Yujin Huang",
      "Yuan-Fang Li",
      "Weiqing Wang",
      "Gholamreza Haffari",
      "Fatemeh Shiri"
    ],
    "abstract":"Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.",
    "venue":"Conference of the European Chapter of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-01-30",
    "citation_count":64,
    "reference_count":82,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2023.eacl-main.77",
      "DBLP":"journals\/corr\/abs-2301-12868",
      "ArXiv":"2301.12868",
      "DOI":"10.48550\/arXiv.2301.12868",
      "CorpusId":256389762
    }
  },
  {
    "paper_id":"25e7c9dcc294d77d184c4c1122c8304cdb58c69d",
    "title":"One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline",
    "authors":[
      "Michele Bevilacqua",
      "Rexhina Blloshmi",
      "Roberto Navigli"
    ],
    "abstract":"In Text-to-AMR parsing, current state-of-the-art semantic parsers use cumbersome pipelines integrating several different modules or components, and exploit graph recategorization, i.e., a set of content-specific heuristics that are developed on the basis of the training set. However, the generalizability of graph recategorization in an out-of-distribution setting is unclear. In contrast, state-of-the-art AMR-to-Text generation, which can be seen as the inverse to parsing, is based on simpler seq2seq. In this paper, we cast Text-to-AMR and AMR-to-Text as a symmetric transduction task and show that by devising a careful graph linearization and extending a pretrained encoder-decoder model, it is possible to obtain state-of-the-art performances in both tasks using the very same seq2seq approach, i.e., SPRING (Symmetric PaRsIng aNd Generation). Our model does not require complex pipelines, nor heuristics built on heavy assumptions. In fact, we drop the need for graph recategorization, showing that this technique is actually harmful outside of the standard benchmark. Finally, we outperform the previous state of the art on the English AMR 2.0 dataset by a large margin: on Text-to-AMR we obtain an improvement of 3.6 Smatch points, while on AMR-to-Text we outperform the state of the art by 11.2 BLEU points. \nWe release the software at github.com\/SapienzaNLP\/spring.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-05-18",
    "citation_count":179,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/BevilacquaBN21",
      "DOI":"10.1609\/aaai.v35i14.17489",
      "CorpusId":235349016
    }
  },
  {
    "paper_id":"b41e95c8c97846d5ca4c11ef79d7814499cc9663",
    "title":"Compositional Semantic Parsing on Semi-Structured Tables",
    "authors":[
      "Panupong Pasupat",
      "Percy Liang"
    ],
    "abstract":"Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2015,
    "publication_date":"2015-08-03",
    "citation_count":893,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/PasupatL15",
      "MAG":"2950466129",
      "ACL":"P15-1142",
      "ArXiv":"1508.00305",
      "DOI":"10.3115\/v1\/P15-1142",
      "CorpusId":9027681
    }
  },
  {
    "paper_id":"267ebbabf19e7f67aae6359be42ee2e12479ba17",
    "title":"Identity-Guided Human Semantic Parsing for Person Re-Identification",
    "authors":[
      "Kuan Zhu",
      "Haiyun Guo",
      "Zhiwei Liu",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract":"Existing alignment-based methods have to employ the pretrained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identity-guided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person re-ID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Specifically, for the pixels of all images of a person, we first group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the selflearned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods. Our code is available at this https URL.",
    "venue":"European Conference on Computer Vision",
    "year":2020,
    "publication_date":"2020-07-27",
    "citation_count":347,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/eccv\/ZhuGLTW20",
      "MAG":"3045817950",
      "ArXiv":"2007.13467",
      "DOI":"10.1007\/978-3-030-58580-8_21",
      "CorpusId":220793215
    }
  },
  {
    "paper_id":"65da5a05fa6b49e8aa11b4acce965a0f439c1716",
    "title":"Human Semantic Parsing for Person Re-identification",
    "authors":[
      "Mahdi M. Kalayeh",
      "Emrah Basaran",
      "M. Gokmen",
      "M. Kamasak",
      "M. Shah"
    ],
    "abstract":"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that, by employing a simple yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 [48] by ~17% in mAP and ~6% in rank-1, CUHK03 [24] by ~4% in rank-1 and DukeMTMC-reID [50] by ~24% in mAP and ~10% in rank-1.",
    "venue":"2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition",
    "year":2018,
    "publication_date":"2018-03-31",
    "citation_count":616,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1804-00216",
      "MAG":"2963805953",
      "ArXiv":"1804.00216",
      "DOI":"10.1109\/CVPR.2018.00117",
      "CorpusId":4564819
    }
  },
  {
    "paper_id":"9fa3e92dd042555483b30b7f70bc4ecb1bdf7bc7",
    "title":"Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing",
    "authors":[
      "Hao Yan",
      "Saurabh Srivastava",
      "Yintao Tai",
      "Sida I. Wang",
      "Wen-tau Yih",
      "Ziyu Yao"
    ],
    "abstract":"Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full set of human annotations.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-05-14",
    "citation_count":21,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-08195",
      "ArXiv":"2305.08195",
      "ACL":"2023.acl-long.177",
      "DOI":"10.48550\/arXiv.2305.08195",
      "CorpusId":258685538
    }
  },
  {
    "paper_id":"a8a168d53e01b0c35d626cfced103656e22b8343",
    "title":"MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
    "authors":[
      "Haoran Li",
      "Abhinav Arora",
      "Shuohui Chen",
      "Anchit Gupta",
      "Sonal Gupta",
      "Yashar Mehdad"
    ],
    "abstract":"Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.",
    "venue":"Conference of the European Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-08-21",
    "citation_count":198,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/eacl\/LiACGGM21",
      "ArXiv":"2008.09335",
      "ACL":"2021.eacl-main.257",
      "MAG":"3080427942",
      "DOI":"10.18653\/v1\/2021.eacl-main.257",
      "CorpusId":221246333
    }
  },
  {
    "paper_id":"8b2cbb2f101b025c16e12d0d7628f65e5378e10d",
    "title":"GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
    "authors":[
      "Tao Yu",
      "Chien-Sheng Wu",
      "Xi Victoria Lin",
      "Bailin Wang",
      "Y. Tan",
      "Xinyi Yang",
      "Dragomir R. Radev",
      "R. Socher",
      "Caiming Xiong"
    ],
    "abstract":"We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG) induced from existing text-to-SQL datasets. We pre-train our model on the synthetic data using a novel text-schema linking objective that predicts the syntactic role of a table field in the SQL for each question-SQL pair. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) over several existing table-and-language datasets to regularize the pre-training process. On four popular fully supervised and weakly supervised table semantic parsing benchmarks, GraPPa significantly outperforms RoBERTa-large as the feature representation layers and establishes new state-of-the-art results on all of them.",
    "venue":"International Conference on Learning Representations",
    "year":2020,
    "publication_date":"2020-09-29",
    "citation_count":271,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2009.13845",
      "DBLP":"journals\/corr\/abs-2009-13845",
      "MAG":"3091229675",
      "CorpusId":221995589
    }
  },
  {
    "paper_id":"c624ee21c69ec9cbc94b50221f6bba116d4c7859",
    "title":"Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base",
    "authors":[
      "Wen-tau Yih",
      "Ming-Wei Chang",
      "Xiaodong He",
      "Jianfeng Gao"
    ],
    "abstract":"We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5% on the WEBQUESTIONS dataset.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2015,
    "publication_date":"2015-07-28",
    "citation_count":799,
    "reference_count":26,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"P15-1128",
      "MAG":"2251079237",
      "DBLP":"conf\/acl\/YihCHG15",
      "DOI":"10.3115\/v1\/P15-1128",
      "CorpusId":18309765
    }
  },
  {
    "paper_id":"40047a74b707743157051d38f76061ba5ff9aab4",
    "title":"Compositional Semantic Parsing with Large Language Models",
    "authors":[
      "Andrew Drozdov",
      "Nathanael Scharli",
      "Ekin Akyuurek",
      "Nathan Scales",
      "Xinying Song",
      "Xinyun Chen",
      "O. Bousquet",
      "Denny Zhou"
    ],
    "abstract":"Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-09-29",
    "citation_count":100,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2209-15003",
      "ArXiv":"2209.15003",
      "CorpusId":252596001
    }
  },
  {
    "paper_id":"232b40980acb55afa89ec50dd9806a5e551f699b",
    "title":"Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing",
    "authors":[
      "Xi Victoria Lin",
      "R. Socher",
      "Caiming Xiong"
    ],
    "abstract":"We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by BERT with minimal subsequent layers and the text-DB contextualization is realized via the fine-tuned deep attention in BERT. Combined with a pointer-generator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on the well-studied Spider benchmark (65.5% dev, 59.2% test), despite being much simpler than most recently proposed models for this task. Our analysis shows that BRIDGE effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-DB related tasks. Our model implementation is available at https:\/\/github.com\/ salesforce\/TabularSemanticParsing.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-11-01",
    "citation_count":222,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2012-12627",
      "ArXiv":"2012.12627",
      "ACL":"2020.findings-emnlp.438",
      "MAG":"3103801878",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.438",
      "CorpusId":226283779
    }
  },
  {
    "paper_id":"acf8a1040034820bf99379a3422815f4e0859ec9",
    "title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    "authors":[
      "Peter Shaw",
      "Ming-Wei Chang",
      "Panupong Pasupat",
      "Kristina Toutanova"
    ],
    "abstract":"Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-24",
    "citation_count":191,
    "reference_count":68,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3093630174",
      "DBLP":"journals\/corr\/abs-2010-12725",
      "ArXiv":"2010.12725",
      "ACL":"2021.acl-long.75",
      "DOI":"10.18653\/v1\/2021.acl-long.75",
      "CorpusId":225066984
    }
  },
  {
    "paper_id":"db9d7c9641fee85c7647740567f1b3fadb32fd34",
    "title":"SmBoP: Semi-autoregressive Bottom-up Semantic Parsing",
    "authors":[
      "Ohad Rubin",
      "Jonathan Berant"
    ],
    "abstract":"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GraPPa.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-23",
    "citation_count":161,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3170656151",
      "DBLP":"conf\/acl-spnlp\/RubinB21",
      "ACL":"2021.naacl-main.29",
      "ArXiv":"2010.12412",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.29",
      "CorpusId":225062282
    }
  },
  {
    "paper_id":"6e10343767ab09dde83cf99ea3442907402a9810",
    "title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing",
    "authors":[
      "Linlu Qiu",
      "Peter Shaw",
      "Panupong Pasupat",
      "Tianze Shi",
      "Jonathan Herzig",
      "Emily Pitler",
      "Fei Sha",
      "Kristina Toutanova"
    ],
    "abstract":"Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-05-24",
    "citation_count":59,
    "reference_count":87,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-12253",
      "ArXiv":"2205.12253",
      "ACL":"2022.emnlp-main.624",
      "DOI":"10.48550\/arXiv.2205.12253",
      "CorpusId":249017865
    }
  },
  {
    "paper_id":"38e1a9c5599fc7597b7c5ffd37951ba5f528094c",
    "title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
    "authors":[
      "Peng Shi",
      "Rui Zhang",
      "Richard He Bai",
      "Jimmy J. Lin"
    ],
    "abstract":"In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English). This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at https:\/\/github.com\/Impavidity\/XRICL.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-10-25",
    "citation_count":52,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2210-13693",
      "ArXiv":"2210.13693",
      "DOI":"10.48550\/arXiv.2210.13693",
      "CorpusId":253107357
    }
  },
  {
    "paper_id":"716f9d0f6e96f437e127de90c87f7b2f7a6c8f12",
    "title":"SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models",
    "authors":[
      "Jingfeng Yang",
      "Haoming Jiang",
      "Qingyu Yin",
      "Danqing Zhang",
      "Bing Yin",
      "Diyi Yang"
    ],
    "abstract":"Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method -- SeqZero. SeqZero decomposes the problem into a sequence of sub-problems, which correspond to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SeqZero avoids generating a long canonical utterance at once. Moreover, SeqZero employs not only a few-shot model but also a zero-shot model to alleviate the overfitting. In particular, SeqZero brings out the merits from both models via ensemble equipped with our proposed constrained rescaling. SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.",
    "venue":"NAACL-HLT",
    "year":2022,
    "publication_date":"2022-05-15",
    "citation_count":51,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2205-07381",
      "ArXiv":"2205.07381",
      "DOI":"10.48550\/arXiv.2205.07381",
      "CorpusId":248811193
    }
  },
  {
    "paper_id":"b42501d9f418bf11d1886e617ab125b37fd2d87c",
    "title":"Modern Baselines for SPARQL Semantic Parsing",
    "authors":[
      "Debayan Banerjee",
      "Pranav Ajit Nair",
      "J. Kaur",
      "Ricardo Usbeck",
      "Chris Biemann"
    ],
    "abstract":"In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.",
    "venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year":2022,
    "publication_date":"2022-04-27",
    "citation_count":38,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2204-12793",
      "ArXiv":"2204.12793",
      "DOI":"10.1145\/3477495.3531841",
      "CorpusId":248405891
    }
  },
  {
    "paper_id":"9358952fad5a109160ea43cf09a7a821518e4be1",
    "title":"Stop: A Dataset for Spoken Task Oriented Semantic Parsing",
    "authors":[
      "Paden Tomasello",
      "Po-chun Hsu",
      "Akshat Shrivastava",
      "Daniel Lazar",
      "Duc Le",
      "Adithya Sagar",
      "A. Elkahky",
      "Jade Copet",
      "Wei-Ning Hsu",
      "Yossef Mordechay",
      "Robin Algayres",
      "Tu Nguyen",
      "Emmanuel Dupoux",
      "Luke Zettlemoyer",
      "Abdel-rahman Mohamed"
    ],
    "abstract":"End-to-end spoken language understanding (SLU) predicts intent directly from audio using a single model. It promises to improve the performance of assistant systems by leveraging acoustic information lost in the intermediate textual representation and preventing cascading errors from Automatic Speech Recognition (ASR). Further, having one unified model has efficiency advantages when deploying assistant systems on-device. However, the limited number of public audio datasets with semantic parse labels hinders the research progress in this area. In this paper, we release the Spoken Task-Oriented semantic Parsing (STOP) dataset 1, the largest and most complex SLU dataset publicly available. Additionally, we define low-resource splits to establish a benchmark for improving SLU when limited labeled data is available. Furthermore, in addition to the human-recorded audio, we are releasing a TTS-generated versions to benchmark the performance for low-resource and domain adaptation of end-to-end SLU systems.",
    "venue":"Spoken Language Technology Workshop",
    "year":2022,
    "publication_date":"2022-06-29",
    "citation_count":39,
    "reference_count":25,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2207-10643",
      "ArXiv":"2207.10643",
      "DOI":"10.1109\/SLT54892.2023.10022703",
      "CorpusId":250918233
    }
  },
  {
    "paper_id":"c4bd3c684b56d076c5ee3afec48fad4c9c14579c",
    "title":"Grounded Adaptation for Zero-shot Executable Semantic Parsing",
    "authors":[
      "Victor Zhong",
      "M. Lewis",
      "Sida I. Wang",
      "Luke Zettlemoyer"
    ],
    "abstract":"We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-09-16",
    "citation_count":105,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3103962261",
      "ArXiv":"2009.07396",
      "ACL":"2020.emnlp-main.558",
      "DBLP":"conf\/emnlp\/ZhongLWZ20",
      "DOI":"10.18653\/v1\/2020.emnlp-main.558",
      "CorpusId":221739233
    }
  },
  {
    "paper_id":"3e60cba99b4e8a45f2e3ba3df462ac949a720833",
    "title":"Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty",
    "authors":[
      "Zi Lin",
      "J. Liu",
      "Jingbo Shang"
    ],
    "abstract":"Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference. In this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case study. Specifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categories.The neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly. To address this, we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty. Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark.",
    "venue":"Findings",
    "year":2022,
    "publication_date":null,
    "citation_count":36,
    "reference_count":58,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/LinLS22",
      "ACL":"2022.findings-acl.328",
      "DOI":"10.18653\/v1\/2022.findings-acl.328",
      "CorpusId":248780252
    }
  },
  {
    "paper_id":"ffac48a405f1474b9de0f01adaefccbc9a7c25f8",
    "title":"Knowledge Base Question Answering: A Semantic Parsing Perspective",
    "authors":[
      "Yu Gu",
      "Vardaan Pahuja",
      "Gong Cheng",
      "Yu Su"
    ],
    "abstract":"Recent advances in deep learning have greatly propelled the research on semantic parsing. Improvement has since been made in many downstream tasks, including natural language interface to web APIs, text-to-SQL generation, among others. However, despite the close connection shared with these tasks, research on question answering over knowledge bases (KBQA) has comparatively been progressing slowly. We identify and attribute this to two unique challenges of KBQA, schema-level complexity and fact-level complexity. In this survey, we situate KBQA in the broader literature of semantic parsing and give a comprehensive account of how existing KBQA approaches attempt to address the unique challenges. Regardless of the unique challenges, we argue that we can still take much inspiration from the literature of semantic parsing, which has been overlooked by existing research on KBQA. Based on our discussion, we can better understand the bottleneck of current KBQA research and shed light on promising directions for KBQA to keep up with the literature of semantic parsing, particularly in the era of pre-trained language models.",
    "venue":"Conference on Automated Knowledge Base Construction",
    "year":2022,
    "publication_date":"2022-09-12",
    "citation_count":31,
    "reference_count":84,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2209-04994",
      "ArXiv":"2209.04994",
      "DOI":"10.48550\/arXiv.2209.04994",
      "CorpusId":252200059
    }
  },
  {
    "paper_id":"c428f1621f79925311082d8d7425dd4d50cd64ed",
    "title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
    "authors":[
      "Elias Stengel-Eskin",
      "Benjamin Van Durme"
    ],
    "abstract":"Abstract Sequence generation models are increasingly being used to translate natural language into programs, i.e., to perform executable semantic parsing. The fact that semantic parsing aims to predict programs that can lead to executed actions in the real world motivates developing safe systems. This in turn makes measuring calibration—a central component to safety—particularly important. We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.1",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-11-14",
    "citation_count":32,
    "reference_count":102,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2211-07443",
      "ArXiv":"2211.07443",
      "DOI":"10.1162\/tacl_a_00598",
      "CorpusId":253510101
    }
  },
  {
    "paper_id":"f377e2768aaab569db6c985a91b5eca4afbd4e1c",
    "title":"MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing",
    "authors":[
      "Longxu Dou",
      "Yan Gao",
      "Mingyang Pan",
      "Dingzirui Wang",
      "Wanxiang Che",
      "Dechen Zhan",
      "Jian-Guang Lou"
    ],
    "abstract":"Text-to-SQL semantic parsing is an important NLP task, which facilitates the interaction between users and the database. Much recent progress in text-to-SQL has been driven by large-scale datasets, but most of them are centered on English. In this work, we present MultiSpider, the largest multilingual text-to-SQL semantic parsing dataset which covers seven languages (English, German, French, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider we further identify the lexical and structural challenges of text-to-SQL (caused by specific language properties and dialect sayings) and their intensity across different languages. Experimental results under various settings (zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in accuracy in non-English languages. Qualitative and quantitative analyses are conducted to understand the reason for the performance drop of each language. Besides the dataset, we also propose a simple schema augmentation framework SAVe (Schema-Augmentation-with-Verification), which significantly boosts the overall performance by about 1.8% and closes the 29.5% performance gap across languages.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2022,
    "publication_date":"2022-12-27",
    "citation_count":28,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/Dou0PWCZL23",
      "ArXiv":"2212.13492",
      "DOI":"10.48550\/arXiv.2212.13492",
      "CorpusId":255186284
    }
  },
  {
    "paper_id":"b06c41432dc060c5591b91f8ed40ae15913a150d",
    "title":"SUBS: Subtree Substitution for Compositional Semantic Parsing",
    "authors":[
      "Jingfeng Yang",
      "Le Zhang",
      "Diyi Yang"
    ],
    "abstract":"Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial grammar or rules for data augmentation, which resulted in limited improvement. We propose to use subtree substitution for compositional data augmentation, where we consider subtrees with similar semantic functions as exchangeable. Our experiments showed that such augmented data led to significantly better performance on Scan and GeoQuery, and reached new SOTA on compositional split of GeoQuery.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-05-03",
    "citation_count":21,
    "reference_count":24,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/YangZY22",
      "ArXiv":"2205.01538",
      "ACL":"2022.naacl-main.12",
      "DOI":"10.48550\/arXiv.2205.01538",
      "CorpusId":248506194
    }
  },
  {
    "paper_id":"a4689571794bbda5f07267bd717def916cb86d6f",
    "title":"Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation",
    "authors":[
      "Abelardo Carlos Martínez Lorenzo",
      "Marco Maru",
      "Roberto Navigli"
    ],
    "abstract":"A language-independent representation of meaning is one of the most coveted dreams in Natural Language Understanding. With this goal in mind, several formalisms have been proposed as frameworks for meaning representation in Semantic Parsing. And yet, the dependencies these formalisms share with respect to language-specific repositories of knowledge make the objective of closing the gap between high- and low-resourced languages hard to accomplish. In this paper, we present the BabelNet Meaning Representation (BMR), an interlingual formalism that abstracts away from language-specific constraints by taking advantage of the multilingual semantic resources of BabelNet and VerbAtlas. We describe the rationale behind the creation of BMR and put forward BMR 1.0, a dataset labeled entirely according to the new formalism. Moreover, we show how BMR is able to outperform previous formalisms thanks to its fully-semantic framing, which enables top-notch multilingual parsing and generation. We release the code at https:\/\/github.com\/SapienzaNLP\/bmr.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":null,
    "citation_count":22,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2022.acl-long.121",
      "DBLP":"conf\/acl\/LorenzoMN22",
      "DOI":"10.18653\/v1\/2022.acl-long.121",
      "CorpusId":248780580
    }
  },
  {
    "paper_id":"4faf26ae9b3ec62c3bf49864cdd1c71037c5a60d",
    "title":"Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
    "authors":[
      "Yury Zemlyanskiy",
      "Michiel de Jong",
      "J. Ainslie",
      "Panupong Pasupat",
      "Peter Shaw",
      "Linlu Qiu",
      "Sumit K. Sanghai",
      "Fei Sha"
    ],
    "abstract":"A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandR first generates a preliminary prediction with input-based retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.",
    "venue":"International Conference on Computational Linguistics",
    "year":2022,
    "publication_date":"2022-09-29",
    "citation_count":21,
    "reference_count":19,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2209-14899",
      "ArXiv":"2209.14899",
      "ACL":"2022.coling-1.438",
      "DOI":"10.48550\/arXiv.2209.14899",
      "CorpusId":252596262
    }
  },
  {
    "paper_id":"95e2f656017f9ec5d9cd411b1f744b278131ce6c",
    "title":"BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing",
    "authors":[
      "Subhro Roy",
      "Sam Thomson",
      "Tongfei Chen",
      "Richard Shin",
      "Adam Pauls",
      "Jason Eisner",
      "Benjamin Van Durme",
      "Microsoft Semantic Machines",
      "Scaled Cognition"
    ],
    "abstract":"Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-art methods for syntactic and semantic parsing when the model output is constrained to be valid.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-06-21",
    "citation_count":15,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/RoyTCSPED23",
      "ArXiv":"2206.10668",
      "CorpusId":266876125
    }
  },
  {
    "paper_id":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a",
    "title":"Few-Shot Semantic Parsing with Language Models Trained on Code",
    "authors":[
      "Richard Shin",
      "Benjamin Van Durme"
    ],
    "abstract":"Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-12-16",
    "citation_count":72,
    "reference_count":21,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2112.08696",
      "ACL":"2022.naacl-main.396",
      "DBLP":"journals\/corr\/abs-2112-08696",
      "DOI":"10.18653\/v1\/2022.naacl-main.396",
      "CorpusId":245218525
    }
  },
  {
    "paper_id":"fd04e31c25451f9103a0ac2220ac8d7e7884c343",
    "title":"Coarse-to-Fine Decoding for Neural Semantic Parsing",
    "authors":[
      "Li Dong",
      "Mirella Lapata"
    ],
    "abstract":"Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2018,
    "publication_date":"2018-05-12",
    "citation_count":398,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1805.04793",
      "MAG":"2963357517",
      "ACL":"P18-1068",
      "DBLP":"conf\/acl\/LapataD18",
      "DOI":"10.18653\/v1\/P18-1068",
      "CorpusId":44167998
    }
  },
  {
    "paper_id":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d",
    "title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures",
    "authors":[
      "Daniel Furrer",
      "Marc van Zee",
      "Nathan Scales",
      "Nathanael Scharli"
    ],
    "abstract":"While mainstream machine learning methods are known to have limited ability to compositionally generalize, new architectures and techniques continue to be proposed to address this limitation. We investigate state-of-the-art techniques and architectures in order to assess their effectiveness in improving compositional generalization in semantic parsing tasks based on the SCAN and CFQ datasets. We show that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits. On a more complex compositional task, we show that pre-training leads to significant improvements in performance vs. comparable non-pre-trained models, whereas architectures proposed to encourage compositional generalization on SCAN or in the area of algorithm learning fail to lead to significant improvements. We establish a new state of the art on the CFQ compositional generalization benchmark using MLM pre-training together with an intermediate representation.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-07-17",
    "citation_count":116,
    "reference_count":73,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2007.08970",
      "MAG":"3043172396",
      "DBLP":"journals\/corr\/abs-2007-08970",
      "CorpusId":220633286
    }
  },
  {
    "paper_id":"5f05a83cd1ff25a541fd7d25bc8c0efcc2f31696",
    "title":"Don’t Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing",
    "authors":[
      "Subendhu Rongali",
      "Luca Soldaini",
      "Emilio Monti",
      "Wael Hamza"
    ],
    "abstract":"Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant often rely on a semantic parsing component to understand which action(s) to execute for an utterance spoken by its users. Traditionally, rule-based or statistical slot-filling systems have been used to parse “simple” queries; that is, queries that contain a single action and can be decomposed into a set of non-overlapping entities. More recently, shift-reduce parsers have been proposed to process more complex utterances. These methods, while powerful, impose specific limitations on the type of queries that can be parsed; namely, they require a query to be representable as a parse tree. In this work, we propose a unified architecture based on Sequence to Sequence models and Pointer Generator Network to handle both simple and complex queries. Unlike other works, our approach does not impose any restriction on the semantic parse schema. Furthermore, experiments show that it achieves state of the art performance on three publicly available datasets (ATIS, SNIPS, Facebook TOP), relatively improving between 3.3% and 7.7% in exact match accuracy over previous systems. Finally, we show the effectiveness of our approach on two internal datasets.",
    "venue":"The Web Conference",
    "year":2020,
    "publication_date":"2020-01-30",
    "citation_count":96,
    "reference_count":37,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3003813211",
      "DBLP":"conf\/www\/RongaliSMH20",
      "ArXiv":"2001.11458",
      "DOI":"10.1145\/3366423.3380064",
      "CorpusId":210966220
    }
  },
  {
    "paper_id":"e71885cfa161b3fce024cb75887c06727abe8800",
    "title":"Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing",
    "authors":[
      "Xilun Chen",
      "Asish Ghoshal",
      "Yashar Mehdad",
      "Luke Zettlemoyer",
      "S. Gupta"
    ],
    "abstract":"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user's intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). \nIn this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-10-07",
    "citation_count":93,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/ChenGMZG20",
      "ACL":"2020.emnlp-main.413",
      "ArXiv":"2010.03546",
      "MAG":"3091813257",
      "DOI":"10.18653\/v1\/2020.emnlp-main.413",
      "CorpusId":222177762
    }
  },
  {
    "paper_id":"7c41e58832f3af5fd9e09674924d6b5f822e8eac",
    "title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing",
    "authors":[
      "Alane Suhr",
      "Ming-Wei Chang",
      "Peter Shaw",
      "Kenton Lee"
    ],
    "abstract":"We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training. Recently, several datasets, including Spider, were proposed to support development of XSP systems. We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use. We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead. We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set. Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-07-01",
    "citation_count":97,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2020.acl-main.742",
      "MAG":"3035172316",
      "DBLP":"conf\/acl\/SuhrCSL20",
      "DOI":"10.18653\/v1\/2020.acl-main.742",
      "CorpusId":220047209
    }
  },
  {
    "paper_id":"1c75fcc86946b36297598a9abe192d97a19a917e",
    "title":"SPARQA: Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases",
    "authors":[
      "Yawei Sun",
      "Lingling Zhang",
      "Gong Cheng",
      "Yuzhong Qu"
    ],
    "abstract":"Semantic parsing transforms a natural language question into a formal query over a knowledge base. Many existing methods rely on syntactic parsing like dependencies. However, the accuracy of producing such expressive formalisms is not satisfying on long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This dedicated coarse-grained formalism with a BERT-based parsing algorithm helps to improve the accuracy of the downstream fine-grained semantic parsing. Besides, to align the structure of a question with the structure of a knowledge base, our multi-strategy method combines sentence-level and word-level semantics. Our approach shows promising performance on several datasets.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2020,
    "publication_date":"2020-03-31",
    "citation_count":128,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2003-13956",
      "MAG":"3014185253",
      "ArXiv":"2003.13956",
      "DOI":"10.1609\/AAAI.V34I05.6426",
      "CorpusId":214344519
    }
  },
  {
    "paper_id":"6b2ccee1ac5823ed3d0cf97c75470ea335916c07",
    "title":"On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries",
    "authors":[
      "Tianze Shi",
      "Chen Zhao",
      "Jordan L. Boyd-Graber",
      "Hal Daum'e",
      "Lillian Lee"
    ],
    "abstract":"Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL, a dataset that enriches 11,276 WIKITABLEQUESTIONS English-language questions with manually created SQL equivalents plus alignments between SQL and question fragments. Our annotation enables new training possibilities for encoderdecoder models, including approaches from machine translation previously precluded by the absence of alignments. We propose and test two methods: (1) supervised attention; (2) adopting an auxiliary objective of disambiguating references in the input queries to table columns. In 5-fold cross validation, these strategies improve over strong baselines by 4.4% execution accuracy. Oracle experiments suggest that annotated alignments can support further accuracy gains of up to 23.9%.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-10-21",
    "citation_count":84,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2010-11246",
      "MAG":"3094380266",
      "ACL":"2020.findings-emnlp.167",
      "ArXiv":"2010.11246",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.167",
      "CorpusId":225039884
    }
  },
  {
    "paper_id":"307ec233777755b3d89b2096f4b54c83d9cd80ba",
    "title":"Span-based Semantic Parsing for Compositional Generalization",
    "authors":[
      "Jonathan Herzig",
      "Jonathan Berant"
    ],
    "abstract":"Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. SpanBasedSP extends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY. On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from 61.0 → 88.9 average accuracy.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-09-13",
    "citation_count":102,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/HerzigB20",
      "ArXiv":"2009.06040",
      "ACL":"2021.acl-long.74",
      "MAG":"3086307406",
      "DOI":"10.18653\/v1\/2021.acl-long.74",
      "CorpusId":221655744
    }
  },
  {
    "paper_id":"afd8fb26a7094ed3a8838000d50e2b22f815dc28",
    "title":"Learning to Synthesize Data for Semantic Parsing",
    "authors":[
      "Bailin Wang",
      "Wenpeng Yin",
      "Xi Victoria Lin",
      "Caiming Xiong"
    ],
    "abstract":"Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-12",
    "citation_count":54,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/WangYLX21",
      "MAG":"3170014162",
      "ArXiv":"2104.05827",
      "ACL":"2021.naacl-main.220",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.220",
      "CorpusId":233219350
    }
  },
  {
    "paper_id":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa",
    "title":"Controllable Semantic Parsing via Retrieval Augmentation",
    "authors":[
      "Panupong Pasupat",
      "Yuan Zhang",
      "Kelvin Guu"
    ],
    "abstract":"In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-10-16",
    "citation_count":49,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2110.08458",
      "DBLP":"conf\/emnlp\/PasupatZG21",
      "ACL":"2021.emnlp-main.607",
      "DOI":"10.18653\/v1\/2021.emnlp-main.607",
      "CorpusId":239016988
    }
  },
  {
    "paper_id":"f1aef5403012d2a70344bc70d58d720aef85834c",
    "title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem",
    "authors":[
      "Shucheng Li",
      "Lingfei Wu",
      "Shiwei Feng",
      "Fangli Xu",
      "Fengyuan Xu",
      "Sheng Zhong"
    ],
    "abstract":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-04-07",
    "citation_count":78,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "ArXiv":"2004.13781",
      "MAG":"3021288808",
      "DBLP":"conf\/emnlp\/LiWFXXZ20",
      "ACL":"2020.findings-emnlp.255",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.255",
      "CorpusId":216642054
    }
  },
  {
    "paper_id":"6f73e253030e960780ecca906a6d6375616caf93",
    "title":"Hierarchical Human Semantic Parsing With Comprehensive Part-Relation Modeling",
    "authors":[
      "Wenguan Wang",
      "Tianfei Zhou",
      "Siyuan Qi",
      "Jianbing Shen",
      "Song-Chun Zhu"
    ],
    "abstract":"Modeling the human structure is central for human parsing that extracts pixel-wise semantic information from images. We start with analyzing three types of inference processes over the hierarchical structure of human bodies: direct inference (directly predicting human semantic parts using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). We then formulate the problem as a compositional neural information fusion (CNIF) framework, which assembles the information from the three inference processes in a conditional manner, i.e., considering the confidence of the sources. Based on CNIF, we further present a part-relation-aware human parser (PRHP), which precisely describes three kinds of human part relations, i.e., decomposition, composition, and dependency, by three distinct relation networks. Expressive relation information can be captured by imposing the parameters in the relation networks to satisfy specific geometric characteristics of different relations. By assimilating generic message-passing networks with their edge-typed, convolutional counterparts, PRHP performs iterative reasoning over the human body hierarchy. With these efforts, PRHP provides a more general and powerful form of CNIF, and lays the foundation for more sophisticated and flexible human relation patterns of reasoning. Experiments on five datasets demonstrate that our two human parsers outperform the state-of-the-arts in all cases.",
    "venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year":2021,
    "publication_date":"2021-01-29",
    "citation_count":73,
    "reference_count":108,
    "fields_of_study":[
      "Medicine",
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/pami\/WangZQSZ22",
      "DOI":"10.1109\/TPAMI.2021.3055780",
      "CorpusId":231755913,
      "PubMed":"33513100"
    }
  },
  {
    "paper_id":"becc21ab34a7858e9bec469c9329ddacf39472fa",
    "title":"Dynamic Hybrid Relation Exploration Network for Cross-Domain Context-Dependent Semantic Parsing",
    "authors":[
      "Binyuan Hui",
      "Ruiying Geng",
      "Qiyu Ren",
      "Binhua Li",
      "Yongbin Li",
      "Jian Sun",
      "Fei Huang",
      "Luo Si",
      "P. Zhu",
      "Xiao-Dan Zhu"
    ],
    "abstract":"Semantic parsing has long been a fundamental problem in natural language processing. Recently, cross-domain context-dependent semantic parsing has become a new focus of research. Central to the problem is the challenge of leveraging contextual information of both natural language queries and database schemas in the interaction history. In this paper, we present a dynamic graph framework that is capable of effectively modelling contextual utterances, tokens, database schemas, and their complicated interaction as the conversation proceeds. The framework employs a dynamic memory decay mechanism that incorporates inductive bias to integrate enriched contextual relation representation, which is further enhanced with a powerful reranking model. At the time of writing, we demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state-of-the-art performance on two large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the model attains a 55.8% question-match and 30.8% interaction-match accuracy on SParC, and a 46.8% question-match and 17.0% interaction-match accuracy on CoSQL.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-05-18",
    "citation_count":48,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/HuiGRLLSHSZZ21",
      "DOI":"10.1609\/aaai.v35i14.17550",
      "CorpusId":235363936
    }
  },
  {
    "paper_id":"af9d09b4b7592540f9cb6b4208960074b2e227f9",
    "title":"Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing",
    "authors":[
      "Tianfei Zhou",
      "Wenguan Wang",
      "Si Liu",
      "Yi Yang",
      "L. Gool"
    ],
    "abstract":"To address the challenging task of instance-aware human part parsing, a new bottom-up regime is proposed to learn category-level human semantic segmentation as well as multi-person pose estimation in a joint and end-to-end manner. It is a compact, efficient and powerful framework that exploits structural information over different human granularities and eases the difficulty of person partitioning. Specifically, a dense-to-sparse projection field, which allows explicitly associating dense human semantics with sparse keypoints, is learnt and progressively improved over the network feature pyramid for robustness. Then, the difficult pixel grouping problem is cast as an easier, multi-person joint assembling task. By formulating joint association as maximum-weight bipartite matching, a differentiable solution is developed to exploit projected gradient descent and Dykstra’s cyclic projection algorithm. This makes our method end-to-end trainable and allows back-propagating the grouping error to directly supervise multi-granularity human representation learning. This is distinguished from current bottom-up human parsers or pose estimators which require sophisticated post-processing or heuristic greedy algorithms. Experiments on three instance-aware human parsing datasets show that our model outperforms other bottom-up alternatives with much more efficient inference.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2021,
    "publication_date":"2021-03-08",
    "citation_count":68,
    "reference_count":69,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2103.04570",
      "DBLP":"journals\/corr\/abs-2103-04570",
      "DOI":"10.1109\/CVPR46437.2021.00167",
      "CorpusId":232148028
    }
  },
  {
    "paper_id":"0098123efc851b67137c1028f7bac8d8bffbc8fd",
    "title":"Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
    "authors":[
      "Qian Liu",
      "Dejian Yang",
      "Jiahui Zhang",
      "Jiaqi Guo",
      "Bin Zhou",
      "Jian-Guang Lou"
    ],
    "abstract":"Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.",
    "venue":"Findings",
    "year":2021,
    "publication_date":"2021-09-22",
    "citation_count":42,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/LiuYZGZL21",
      "ArXiv":"2109.10540",
      "ACL":"2021.findings-acl.100",
      "DOI":"10.18653\/v1\/2021.findings-acl.100",
      "CorpusId":236478283
    }
  },
  {
    "paper_id":"2e92f5fc040388a9f16e307f0d6f3a05e823171c",
    "title":"Meta-Learning for Domain Generalization in Semantic Parsing",
    "authors":[
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ],
    "abstract":"The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-10-22",
    "citation_count":75,
    "reference_count":58,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2010-11988",
      "ArXiv":"2010.11988",
      "MAG":"3094144930",
      "ACL":"2021.naacl-main.33",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.33",
      "CorpusId":225062200
    }
  },
  {
    "paper_id":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4",
    "title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention",
    "authors":[
      "Pengcheng Yin",
      "Hao Fang",
      "Graham Neubig",
      "Adam Pauls",
      "Emmanouil Antonios Platanios",
      "Yu Su",
      "Sam Thomson",
      "Jacob Andreas"
    ],
    "abstract":"We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-06-01",
    "citation_count":61,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.naacl-main.225",
      "MAG":"3168664364",
      "DBLP":"conf\/naacl\/YinFNPPSTA21",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.225",
      "CorpusId":235097473
    }
  },
  {
    "paper_id":"d430009e6fb33ec725888384673955b23d015afa",
    "title":"RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene Parsing",
    "authors":[
      "Jiahang Li",
      "Yikang Zhang",
      "Peng Yun",
      "Guangliang Zhou",
      "Qijun Chen",
      "Rui Fan"
    ],
    "abstract":"The recent advancements in deep convolutional neural networks have shown significant promise in the domain of road scene parsing. Nevertheless, the existing works focus primarily on freespace detection, with little attention given to hazardous road defects that could compromise both driving safety and comfort. In this article, we introduce RoadFormer, a novel Transformer-based data-fusion network developed for road scene parsing. RoadFormer utilizes a duplex encoder architecture to extract heterogeneous features from both RGB images and surface normal information. The encoded features are subsequently fed into a novel heterogeneous feature synergy block for effective feature fusion and recalibration. The pixel decoder then learns multi-scale long-range dependencies from the fused and recalibrated heterogeneous features, which are subsequently processed by a Transformer decoder to produce the final semantic prediction. Additionally, we release SYN-UDTIRI, the first large-scale road scene parsing dataset that contains over 10,407 RGB images, dense depth images, and the corresponding pixel-level annotations for both freespace and road defects of different shapes and sizes. Extensive experimental evaluations conducted on our SYN-UDTIRI dataset, as well as on three public datasets, including KITTI road, CityScapes, and ORFD, demonstrate that RoadFormer outperforms all other state-of-the-art networks for road scene parsing. Specifically, RoadFormer ranks first on the KITTI road benchmark.",
    "venue":"IEEE Transactions on Intelligent Vehicles",
    "year":2023,
    "publication_date":"2023-09-19",
    "citation_count":39,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2309-10356",
      "ArXiv":"2309.10356",
      "DOI":"10.1109\/TIV.2024.3388726",
      "CorpusId":262053782
    }
  },
  {
    "paper_id":"2f05cb3dd8194276aa26c4e71841a86edb51914f",
    "title":"The Power of Prompt Tuning for Low-Resource Semantic Parsing",
    "authors":[
      "Nathan Schucher",
      "Siva Reddy",
      "H. D. Vries"
    ],
    "abstract":"Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-10-16",
    "citation_count":36,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/SchucherRV22",
      "ACL":"2022.acl-short.17",
      "ArXiv":"2110.08525",
      "DOI":"10.18653\/v1\/2022.acl-short.17",
      "CorpusId":239016577
    }
  },
  {
    "paper_id":"d1ddc42d2735ddc852ca43d3983f998b29eda07f",
    "title":"Weakly Supervised Visual Semantic Parsing",
    "authors":[
      "Alireza Zareian",
      "Svebor Karaman",
      "Shih-Fu Chang"
    ],
    "abstract":"Scene Graph Generation (SGG) aims to extract entities, predicates and their semantic structure from images, enabling deep understanding of visual content, with many applications such as visual reasoning and image retrieval. Nevertheless, existing SGG methods require millions of manually annotated bounding boxes for training, and are computationally inefficient, as they exhaustively process all pairs of object proposals to detect predicates. In this paper, we address those two limitations by first proposing a generalized formulation of SGG, namely Visual Semantic Parsing, which disentangles entity and predicate recognition, and enables sub-quadratic performance. Then we propose the Visual Semantic Parsing Network, VSPNet, based on a dynamic, attention-based, bipartite message passing framework that jointly infers graph nodes and edges through an iterative process. Additionally, we propose the first graph-based weakly supervised learning framework, based on a novel graph alignment algorithm, which enables training without bounding box annotations. Through extensive experiments, we show that VSPNet outperforms weakly supervised baselines significantly and approaches fully supervised performance, while being several times faster. We publicly release the source code of our method.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2020,
    "publication_date":"2020-01-08",
    "citation_count":58,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2001.02359",
      "MAG":"3034910302",
      "DBLP":"conf\/cvpr\/ZareianKC20",
      "DOI":"10.1109\/cvpr42600.2020.00379",
      "CorpusId":210064348
    }
  },
  {
    "paper_id":"a51098acabb1f7eb4ae3c60fc93f2c91afd6e766",
    "title":"A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese",
    "authors":[
      "A. Nguyen",
      "M. Dao",
      "Dat Quoc Nguyen"
    ],
    "abstract":"Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020).",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-10-05",
    "citation_count":58,
    "reference_count":34,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/NguyenDN20",
      "ArXiv":"2010.01891",
      "ACL":"2020.findings-emnlp.364",
      "MAG":"3091857408",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.364",
      "CorpusId":219963709
    }
  },
  {
    "paper_id":"c75a2ee17056d2b8c14ac25f9f328a09eb4cf040",
    "title":"Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training",
    "authors":[
      "Peng Shi",
      "Patrick Ng",
      "Zhiguo Wang",
      "Henghui Zhu",
      "Alexander Hanbo Li",
      "Jun Wang",
      "C. D. Santos",
      "Bing Xiang"
    ],
    "abstract":"Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train powerful language models with self-supervised learning objectives, such as Masked Language Model (MLM). Based on a pilot study, we observe three issues of existing general-purpose language models when they are applied in the text-to-SQL semantic parsers: fail to detect the column mentions in the utterances, to infer the column mentions from the cell values, and to compose target SQL queries when they are complex. To mitigate these issues, we present a model pretraining framework, Generation-Augmented Pre-training (GAP), that jointly learns representations of natural language utterance and table schemas, by leveraging generation models to generate high-quality pre-train data. GAP Model is trained on 2 million utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are generated by generation models. Based on experimental results, neural semantic parsers that leverage GAP Model as a representation encoder obtain new state-of-the-art results on both Spider and Criteria-to-SQL benchmarks.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2020,
    "publication_date":"2020-12-18",
    "citation_count":120,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/ShiNWZLWSX21",
      "ArXiv":"2012.10309",
      "DOI":"10.1609\/aaai.v35i15.17627",
      "CorpusId":229331741
    }
  },
  {
    "paper_id":"37882abaec01eba1bf5bda8a36c904aaea0d5642",
    "title":"Improving Compositional Generalization in Semantic Parsing",
    "authors":[
      "I. Oren",
      "Jonathan Herzig",
      "Nitish Gupta",
      "Matt Gardner",
      "Jonathan Berant"
    ],
    "abstract":"Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-10-12",
    "citation_count":64,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2010.05647",
      "DBLP":"journals\/corr\/abs-2010-05647",
      "MAG":"3102532528",
      "ACL":"2020.findings-emnlp.225",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.225",
      "CorpusId":222291650
    }
  },
  {
    "paper_id":"472a5227279b45f25508017816af34e3cb3ac0d7",
    "title":"Semantic Parsing for Task Oriented Dialog using Hierarchical Representations",
    "authors":[
      "S. Gupta",
      "Rushin Shah",
      "Mrinal Mohit",
      "Anuj Kumar",
      "M. Lewis"
    ],
    "abstract":"Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (http:\/\/fb.me\/semanticparsingdialog), and show that parsing models outperform sequence-to-sequence approaches on this dataset.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-10-18",
    "citation_count":210,
    "reference_count":18,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-1810-07942",
      "ACL":"D18-1300",
      "MAG":"2949500715",
      "ArXiv":"1810.07942",
      "DOI":"10.18653\/v1\/D18-1300",
      "CorpusId":53017700
    }
  },
  {
    "paper_id":"c5ad46ae286e1c8ad3ffa52d7c5481f7dba83e88",
    "title":"Conversational Semantic Parsing",
    "authors":[
      "Armen Aghajanyan",
      "Jean Maillard",
      "Akshat Shrivastava",
      "K. Diedrick",
      "Mike Haeger",
      "Haoran Li",
      "Yashar Mehdad",
      "Ves Stoyanov",
      "Anuj Kumar",
      "M. Lewis",
      "S. Gupta"
    ],
    "abstract":"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-09-28",
    "citation_count":50,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3105218021",
      "DBLP":"journals\/corr\/abs-2009-13655",
      "ArXiv":"2009.13655",
      "ACL":"2020.emnlp-main.408",
      "DOI":"10.18653\/v1\/2020.emnlp-main.408",
      "CorpusId":221995543
    }
  },
  {
    "paper_id":"a9453111cb31e69a4ddc43a45cb20985699ac063",
    "title":"Conversational Semantic Parsing for Dialog State Tracking",
    "authors":[
      "Jianpeng Cheng",
      "Devang Agrawal",
      "Héctor Martínez Alonso",
      "Shruti Bhargava",
      "Joris Driesen",
      "F. Flego",
      "D. Kaplan",
      "Dimitri Kartsaklis",
      "Lin Li",
      "Dhivya Piraviperumal",
      "J. Williams",
      "Hong Yu",
      "Diarmuid Ó Séaghdha",
      "Anders Johannsen"
    ],
    "abstract":"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to 20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-10-24",
    "citation_count":64,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/ChengAABDFKKLPW20",
      "MAG":"3100544532",
      "ACL":"2020.emnlp-main.651",
      "ArXiv":"2010.12770",
      "DOI":"10.18653\/v1\/2020.emnlp-main.651",
      "CorpusId":225066960
    }
  },
  {
    "paper_id":"b7eac64a8410976759445cce235469163d23ee65",
    "title":"Data Recombination for Neural Semantic Parsing",
    "authors":[
      "Robin Jia",
      "Percy Liang"
    ],
    "abstract":"Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2016,
    "publication_date":"2016-06-01",
    "citation_count":474,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1606.03622",
      "ACL":"P16-1002",
      "DBLP":"conf\/acl\/JiaL16",
      "MAG":"2952227929",
      "DOI":"10.18653\/v1\/P16-1002",
      "CorpusId":7218315
    }
  },
  {
    "paper_id":"649c1148439a4e875dab414ba67bf8c80350af4a",
    "title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation",
    "authors":[
      "Pengcheng Yin",
      "Graham Neubig"
    ],
    "abstract":"We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2018,
    "publication_date":"2018-10-05",
    "citation_count":241,
    "reference_count":25,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2890867094",
      "ArXiv":"1810.02720",
      "DBLP":"conf\/emnlp\/YinN18",
      "ACL":"D18-2002",
      "DOI":"10.18653\/v1\/D18-2002",
      "CorpusId":52926380
    }
  },
  {
    "paper_id":"bc5b0323c7fe4ac48d3963c70f3dd8b614132662",
    "title":"A Semantic Parsing and Reasoning-Based Approach to Knowledge Base Question Answering",
    "authors":[
      "I. Abdelaziz",
      "Srinivas Ravishankar",
      "Pavan Kapanipathi",
      "S. Roukos",
      "Alexander G. Gray"
    ],
    "abstract":"Knowledge Base Question Answering (KBQA) is a task where existing techniques have faced significant challenges, such as the need for complex question understanding, reasoning, and large training datasets. In this work, we demonstrate Deep Thinking Question Answering (DTQA), a semantic parsing and reasoning-based KBQA system. DTQA (1) integrates multiple, reusable modules that are trained specifically for\ntheir individual tasks (e.g. semantic parsing, entity linking, and relationship linking), eliminating the need for end-to-end KBQA training data; (2) leverages semantic parsing and a reasoner for improved question understanding. DTQA is a system of systems that achieves state-of-the-art performance on two popular KBQA datasets.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-05-18",
    "citation_count":31,
    "reference_count":16,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/AbdelazizRKRG21",
      "DOI":"10.1609\/aaai.v35i18.17988",
      "CorpusId":235363625
    }
  },
  {
    "paper_id":"0bfb508fd350cc988e8880f37215e10f02c835cd",
    "title":"Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing",
    "authors":[
      "Ruisheng Cao",
      "Su Zhu",
      "Chenyu Yang",
      "Chen Liu",
      "Rao Ma",
      "Yanbin Zhao",
      "Lu Chen",
      "Kai Yu"
    ],
    "abstract":"One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-05-27",
    "citation_count":49,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3034475786",
      "DBLP":"journals\/corr\/abs-2005-13485",
      "ACL":"2020.acl-main.608",
      "ArXiv":"2005.13485",
      "DOI":"10.18653\/v1\/2020.acl-main.608",
      "CorpusId":218900918
    }
  },
  {
    "paper_id":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5",
    "title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data",
    "authors":[
      "M. Nicosia",
      "Zhongdi Qu",
      "Y. Altun"
    ],
    "abstract":"While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel Translate-and-Fill (TaF) method to produce silver training data for a multilingual semantic parser. This method simplifies the popular Translate-Align-Project (TAP) pipeline and consists of a sequence-to-sequence filler model that constructs a full parse conditioned on an utterance and a view of the same parse. Our filler is trained on English data only but can accurately complete instances in other languages (i.e., translations of the English training utterances), in a zero-shot fashion. Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-09-09",
    "citation_count":26,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2109-04319",
      "ArXiv":"2109.04319",
      "DOI":"10.18653\/v1\/2021.findings-emnlp.279",
      "CorpusId":237452221
    }
  },
  {
    "paper_id":"aee8581c60d8c812bd31503fab00f9df8d88a766",
    "title":"Zero-Shot Cross-lingual Semantic Parsing",
    "authors":[
      "Tom Sherborne",
      "Mirella Lapata"
    ],
    "abstract":"Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-15",
    "citation_count":31,
    "reference_count":111,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/SherborneL22",
      "ACL":"2022.acl-long.285",
      "ArXiv":"2104.07554",
      "DOI":"10.18653\/v1\/2022.acl-long.285",
      "CorpusId":233241079
    }
  },
  {
    "paper_id":"f8a7aa9c556703d27dd5bf98265de1ff89175635",
    "title":"From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding",
    "authors":[
      "Shan Wu",
      "Bo Chen",
      "Chunlei Xin",
      "Xianpei Han",
      "Le Sun",
      "Weipeng Zhang",
      "Jiansong Chen",
      "Fan Yang",
      "Xunliang Cai"
    ],
    "abstract":"Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-06-11",
    "citation_count":25,
    "reference_count":61,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.acl-long.397",
      "DBLP":"conf\/acl\/Wu0XH0ZCYC20",
      "ArXiv":"2106.06228",
      "DOI":"10.18653\/v1\/2021.acl-long.397",
      "CorpusId":235417352
    }
  },
  {
    "paper_id":"fae272cedc2c050f56fe19e0cfaf2c6b01039e5f",
    "title":"RetroNLU: Retrieval Augmented Task-Oriented Semantic Parsing",
    "authors":[
      "Vivek Gupta",
      "Akshat Shrivastava",
      "Adithya Sagar",
      "Armen Aghajanyan",
      "Denis Savenkov"
    ],
    "abstract":"While large pre-trained language models accumulate a lot of knowledge in their parameters, it has been demonstrated that augmenting it with non-parametric retrieval-based memory has a number of benefits ranging from improved accuracy to data efficiency for knowledge-focused tasks such as question answering. In this work, we apply retrieval-based modeling ideas to the challenging complex task of multi-domain task-oriented semantic parsing for conversational assistants. Our technique, RetroNLU, extends a sequence-to-sequence model architecture with a retrieval component, which is used to retrieve existing similar samples and present them as an additional context to the model. In particular, we analyze two settings, where we augment an input with (a) retrieved nearest neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of nearest neighbor utterances (semparse-nn). Our technique outperforms the baseline method by 1.5% absolute macro-F1, especially at the low resource setting, matching the baseline model accuracy with only 40% of the complete data.Furthermore, we analyse the quality, model sensitivity, and performance of the nearest neighbor retrieval component’s for semantic parses of varied utterance complexity.",
    "venue":"NLP4CONVAI",
    "year":2021,
    "publication_date":"2021-09-21",
    "citation_count":23,
    "reference_count":50,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl-convai\/GuptaSSAS22",
      "ACL":"2022.nlp4convai-1.15",
      "ArXiv":"2109.10410",
      "DOI":"10.18653\/v1\/2022.nlp4convai-1.15",
      "CorpusId":237592700
    }
  },
  {
    "paper_id":"1c24276edc10c23da88f6523943659de56de9de9",
    "title":"Pyramid ALKNet for Semantic Parsing of Building Facade Image",
    "authors":[
      "Wenguang Ma",
      "Wei Ma",
      "Shibiao Xu",
      "H. Zha"
    ],
    "abstract":"The semantic parsing of building facade images is a fundamental yet challenging task in urban scene understanding. Existing works sought to tackle this task by using facade grammars or convolutional neural networks (CNNs). The former can hardly generate parsing results coherent with real images while the latter often fails to capture relationships among facade elements. In this letter, we propose a pyramid atrous large kernel (ALK) network (ALKNet) for the semantic segmentation of facade images. The pyramid ALKNet captures long-range dependencies among building elements by using ALK modules in multiscale feature maps. It makes full use of the regular structures of facades to aggregate useful nonlocal context information and thereby is capable of dealing with challenging image regions caused by occlusions, ambiguities, and so on. Experiments on both rectified and unrectified facade data sets show that ALKNet has better performances than those of state-of-the-art methods.",
    "venue":"IEEE Geoscience and Remote Sensing Letters",
    "year":2021,
    "publication_date":"2021-06-01",
    "citation_count":23,
    "reference_count":32,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/lgrs\/MaMXZ21",
      "MAG":"3027028760",
      "DOI":"10.1109\/LGRS.2020.2993451",
      "CorpusId":219457852
    }
  },
  {
    "paper_id":"a48743b889db1faf0f04d4b29382634d19975f3f",
    "title":"Toward Code Generation: A Survey and Lessons from Semantic Parsing",
    "authors":[
      "Celine Lee",
      "Justin Emile Gottschlich",
      "Dan Roth Intel Labs",
      "U. Pennsylvania"
    ],
    "abstract":"With the growth of natural language processing techniques and demand for improved software engineering efficiency, there is an emerging interest in translating intention from human languages to programming languages. In this survey paper, we attempt to provide an overview of the growing body of research in this space. We begin by reviewing natural language semantic parsing techniques and draw parallels with program synthesis efforts. We then consider semantic parsing works from an evolutionary perspective, with specific analyses on neuro-symbolic methods, architecture, and supervision. We then analyze advancements in frameworks for semantic parsing for code generation. In closing, we present what we believe are some of the emerging open challenges in this domain.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-04-26",
    "citation_count":18,
    "reference_count":97,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2105-03317",
      "ArXiv":"2105.03317",
      "CorpusId":234096076
    }
  },
  {
    "paper_id":"13e538b94a1d97d0154709a977cca9352fee428d",
    "title":"Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog",
    "authors":[
      "Arun Babu",
      "Akshat Shrivastava",
      "Armen Aghajanyan",
      "Ahmed Aly",
      "Angela Fan",
      "Marjan Ghazvininejad"
    ],
    "abstract":"Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-11",
    "citation_count":21,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3166029516",
      "ArXiv":"2104.04923",
      "ACL":"2021.naacl-main.236",
      "DBLP":"conf\/naacl\/BabuSAAFG21",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.236",
      "CorpusId":233210669
    }
  },
  {
    "paper_id":"0258f9ffd26903fbcc50be4eb0e3f502dc556962",
    "title":"SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation",
    "authors":[
      "Luigi Procopio",
      "Rocco Tripodi",
      "Roberto Navigli"
    ],
    "abstract":"Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https:\/\/github.com\/SapienzaNLP\/sgl.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-06-01",
    "citation_count":27,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3167860749",
      "DBLP":"conf\/naacl\/ProcopioTN21",
      "ACL":"2021.naacl-main.30",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.30",
      "CorpusId":235097669
    }
  },
  {
    "paper_id":"6bb6a37a0754093035734d53ddc36df8803d4f47",
    "title":"Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder",
    "authors":[
      "Elman Mansimov",
      "Yi Zhang"
    ],
    "abstract":"We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for semantic parsing in task-oriented dialog. Our model consists of an encoder network that incrementally builds the semantic parse tree by predicting the non-terminal label and its positions in the linearized tree. At the generation time, the model constructs the semantic parse tree by recursively inserting the predicted non-terminal labels at the predicted positions until termination. RINE achieves state-of-the-art exact match accuracy on low- and high-resource versions of the conversational semantic parsing benchmark TOP, outperforming strong sequence-to-sequence models and transition-based parsers. We also show that our model design is applicable to nested named entity recognition task, where it performs on par with state-of-the-art approach designed for that task. Finally, we demonstrate that our approach is 2-3.5 times faster than the sequence-to-sequence model at inference time.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2021,
    "publication_date":"2021-09-09",
    "citation_count":16,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2109-04500",
      "ArXiv":"2109.04500",
      "DOI":"10.1609\/aaai.v36i10.21355",
      "CorpusId":237485223
    }
  },
  {
    "paper_id":"c91959c65108c2fe483fea297399c730ede4b191",
    "title":"A survey of syntactic-semantic parsing based on constituent and dependency structures",
    "authors":[
      "Meishan Zhang"
    ],
    "abstract":"Syntactic and semantic parsing has been investigated for decades, which is one primary topic in the natural language processing community. This article aims for a brief survey on this topic. The parsing community includes many tasks, which are difficult to be covered fully. Here we focus on two of the most popular formalizations of parsing: constituent parsing and dependency parsing. Constituent parsing is majorly targeted to syntactic analysis, and dependency parsing can handle both syntactic and semantic analysis. This article briefly reviews the representative models of constituent parsing and dependency parsing, and also dependency graph parsing with rich semantics. Besides, we also review the closely-related topics such as cross-domain, cross-lingual and joint parsing models, parser application as well as corpus development of parsing in the article.",
    "venue":"Science China Technological Sciences",
    "year":2020,
    "publication_date":"2020-06-19",
    "citation_count":39,
    "reference_count":270,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3088470625",
      "DBLP":"journals\/corr\/abs-2006-11056",
      "ArXiv":"2006.11056",
      "DOI":"10.1007\/s11431-020-1666-4",
      "CorpusId":219956190
    }
  },
  {
    "paper_id":"7e42377dbaa3af86bcba0ccd614c6c0a4a7bd6a3",
    "title":"Unsupervised Person Image Generation With Semantic Parsing Transformation",
    "authors":[
      "Sijie Song",
      "Wei Zhang",
      "Jiaying Liu",
      "Tao Mei"
    ],
    "abstract":"In this paper, we address unsupervised pose-guided person image generation, which is known challenging due to non-rigid deformation. Unlike previous methods learning a rock-hard direct mapping between human bodies, we propose a new pathway to decompose the hard mapping into two more accessible subtasks, namely, semantic parsing transformation and appearance generation. Firstly, a semantic generative network is proposed to transform between semantic parsing maps, in order to simplify the non-rigid deformation learning. Secondly, an appearance generative network learns to synthesize semantic-aware textures. Thirdly, we demonstrate that training our framework in an end-to-end manner further refines the semantic maps and final results accordingly. Our method is generalizable to other semantic-aware person image generation tasks, e.g., clothing texture transfer and controlled image manipulation. Experimental results demonstrate the superiority of our method on DeepFashion and Market-1501 datasets, especially in keeping the clothing attributes and better body shapes.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2019,
    "publication_date":"2019-04-06",
    "citation_count":106,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/SongZLM19",
      "ArXiv":"1904.03379",
      "MAG":"2933625230",
      "DOI":"10.1109\/CVPR.2019.00246",
      "CorpusId":102352602
    }
  },
  {
    "paper_id":"2fc1cfc75d6ba80846d64fbec424f6c35682f5d8",
    "title":"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing",
    "authors":[
      "Chen Liang",
      "Mohammad Norouzi",
      "Jonathan Berant",
      "Quoc V. Le",
      "N. Lao"
    ],
    "abstract":"We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at https:\/\/goo.gl\/TXBp4e",
    "venue":"Neural Information Processing Systems",
    "year":2018,
    "publication_date":"2018-07-01",
    "citation_count":136,
    "reference_count":70,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/LiangNBLL18",
      "ArXiv":"1807.02322",
      "MAG":"2891991579",
      "CorpusId":52300972
    }
  },
  {
    "paper_id":"735ce0447e459e13f89ae751b19323d76a2af786",
    "title":"Program Synthesis and Semantic Parsing with Learned Code Idioms",
    "authors":[
      "Richard Shin",
      "Miltiadis Allamanis",
      "Marc Brockschmidt",
      "Oleksandr Polozov"
    ],
    "abstract":"Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.",
    "venue":"Neural Information Processing Systems",
    "year":2019,
    "publication_date":"2019-06-26",
    "citation_count":90,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/ShinABP19",
      "MAG":"2955270045",
      "ArXiv":"1906.10816",
      "CorpusId":195658048
    }
  },
  {
    "paper_id":"1a272eb83fbf3082ca6d6009963186bb23254b03",
    "title":"Value-Agnostic Conversational Semantic Parsing",
    "authors":[
      "Emmanouil Antonios Platanios",
      "Adam Pauls",
      "Subhro Roy",
      "Yuchen Zhang",
      "Alexander Kyte",
      "Alan Guo",
      "Sam Thomson",
      "Jayant Krishnamurthy",
      "J. Wolfe",
      "Jacob Andreas",
      "D. Klein"
    ],
    "abstract":"Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":null,
    "citation_count":21,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.acl-long.284",
      "DBLP":"conf\/acl\/PlataniosPRZKGT20",
      "DOI":"10.18653\/v1\/2021.acl-long.284",
      "CorpusId":236460195
    }
  },
  {
    "paper_id":"913910c332f7c783180eab3a0830334d0dbbcbdc",
    "title":"ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN",
    "authors":[
      "David Samuel",
      "Milan Straka"
    ],
    "abstract":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http:\/\/www.github.com\/ufal\/perin.",
    "venue":"Conference on Computational Natural Language Learning",
    "year":2020,
    "publication_date":"2020-11-01",
    "citation_count":33,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3107811276",
      "ArXiv":"2011.00758",
      "DBLP":"conf\/conll\/SamuelS20",
      "ACL":"2020.conll-shared.5",
      "DOI":"10.18653\/v1\/2020.conll-shared.5",
      "CorpusId":226227071
    }
  },
  {
    "paper_id":"25f87536c57090d3c797e216fd2b871dbb54fbf0",
    "title":"Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study",
    "authors":[
      "Ziyu Yao",
      "Yu Su",
      "Huan Sun",
      "Wen-tau Yih"
    ],
    "abstract":"As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-10-01",
    "citation_count":84,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2970393840",
      "ACL":"D19-1547",
      "ArXiv":"1910.05389",
      "DBLP":"journals\/corr\/abs-1910-05389",
      "DOI":"10.18653\/v1\/D19-1547",
      "CorpusId":204509379
    }
  },
  {
    "paper_id":"2c1e874c3b67510a3215e535f5646b362de5bc89",
    "title":"Abstract Syntax Networks for Code Generation and Semantic Parsing",
    "authors":[
      "Maxim Rabinovich",
      "Mitchell Stern",
      "D. Klein"
    ],
    "abstract":"Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2017,
    "publication_date":"2017-04-01",
    "citation_count":374,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/RabinovichSK17",
      "ACL":"P17-1105",
      "MAG":"2610002206",
      "ArXiv":"1704.07535",
      "DOI":"10.18653\/v1\/p17-1105",
      "CorpusId":13529592
    }
  },
  {
    "paper_id":"467964144699f9626fd8b289646295bc44d7c744",
    "title":"Open-Domain Frame Semantic Parsing Using Transformers",
    "authors":[
      "Aditya Kalyanpur",
      "Or Biran",
      "Tom Breloff",
      "Jennifer Chu-Carroll",
      "Ariel Diertani",
      "Owen Rambow",
      "Mark Sammons"
    ],
    "abstract":"Frame semantic parsing is a complex problem which includes multiple underlying subtasks. Recent approaches have employed joint learning of subtasks (such as predicate and argument detection), and multi-task learning of related tasks (such as syntactic and semantic parsing). In this paper, we explore multi-task learning of all subtasks with transformer-based models. We show that a purely generative encoder-decoder architecture handily beats the previous state of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task approach achieves even better performance.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-10-21",
    "citation_count":23,
    "reference_count":29,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2010.10998",
      "MAG":"3093569337",
      "DBLP":"journals\/corr\/abs-2010-10998",
      "CorpusId":224819862
    }
  },
  {
    "paper_id":"8c8e24625b194a6569fd0e88102f2ecd7983c3c4",
    "title":"Don’t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding",
    "authors":[
      "Qile Zhu",
      "Haidar Khan",
      "Saleh Soltan",
      "Stephen Rawls",
      "Wael Hamza"
    ],
    "abstract":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.",
    "venue":"Conference on Computational Natural Language Learning",
    "year":2020,
    "publication_date":"2020-10-08",
    "citation_count":25,
    "reference_count":41,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2010.03714",
      "ACL":"2020.conll-1.40",
      "MAG":"3091803861",
      "DBLP":"journals\/corr\/abs-2010-03714",
      "DOI":"10.18653\/v1\/2020.conll-1.40",
      "CorpusId":222208998
    }
  },
  {
    "paper_id":"a1c4ce9de92338646c6ee93c7c2e5ee366784b1a",
    "title":"Benchmarking Meaning Representations in Neural Semantic Parsing",
    "authors":[
      "Jiaqi Guo",
      "Qian Liu",
      "Jian-Guang Lou",
      "Zhenwen Li",
      "Xueqing Liu",
      "Tao Xie",
      "Ting Liu"
    ],
    "abstract":"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work’s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose , a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets × four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https:\/\/github.com\/JasperGuo\/Unimer.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-11-01",
    "citation_count":29,
    "reference_count":58,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3106356412",
      "ACL":"2020.emnlp-main.118",
      "DBLP":"conf\/emnlp\/GuoLLLLXL20",
      "DOI":"10.18653\/v1\/2020.emnlp-main.118",
      "CorpusId":226262340
    }
  },
  {
    "paper_id":"4fb94857121be6ccbea7034a118b48e8fc06c121",
    "title":"Leveraging Semantic Parsing for Relation Linking over Knowledge Bases",
    "authors":[
      "Nandana Mihindukulasooriya",
      "Gaetano Rossiello",
      "Pavan Kapanipathi",
      "I. Abdelaziz",
      "Srinivas Ravishankar",
      "Mo Yu",
      "A. Gliozzo",
      "S. Roukos",
      "Alexander G. Gray"
    ],
    "abstract":"Knowledgebase question answering systems are heavily dependent on relation extraction and linking modules. However, the task of extracting and linking relations from text to knowledgebases faces two primary challenges; the ambiguity of natural language and lack of training data. To overcome these challenges, we present SLING, a relation linking framework which leverages semantic parsing using Abstract Meaning Representation (AMR) and distant supervision. SLING integrates multiple relation linking approaches that capture complementary signals such as linguistic cues, rich semantic representation, and information from the knowledgebase. The experiments on relation linking using three KBQA datasets; QALD-7, QALD-9, and LC-QuAD 1.0 demonstrate that the proposed approach achieves state-of-the-art performance on all benchmarks.",
    "venue":"International Workshop on the Semantic Web",
    "year":2020,
    "publication_date":"2020-09-16",
    "citation_count":25,
    "reference_count":26,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2009.07726",
      "MAG":"3107452004",
      "DBLP":"conf\/semweb\/Mihindukulasooriya20",
      "DOI":"10.1007\/978-3-030-62419-4_23",
      "CorpusId":221738906
    }
  },
  {
    "paper_id":"60d99e00f7f96efea3099b9491a93bb8060ff502",
    "title":"Fast Semantic Parsing with Well-typedness Guarantees",
    "authors":[
      "Matthias Lindemann",
      "Jonas Groschwitz",
      "Alexander Koller"
    ],
    "abstract":"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-09-15",
    "citation_count":24,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2020.emnlp-main.323",
      "DBLP":"journals\/corr\/abs-2009-07365",
      "MAG":"3102963613",
      "ArXiv":"2009.07365",
      "DOI":"10.18653\/v1\/2020.emnlp-main.323",
      "CorpusId":221739114
    }
  },
  {
    "paper_id":"bb95ce4b89c48ecb5989ebc180c155c178eef3d2",
    "title":"Memory-Based Semantic Parsing",
    "authors":[
      "Parag Jain",
      "Mirella Lapata"
    ],
    "abstract":"Abstract We present a memory-based model for context- dependent semantic parsing. Previous approaches focus on enabling the decoder to copy or modify the parse from the previous utterance, assuming there is a dependency between the current and previous parses. In this work, we propose to represent contextual information using an external memory. We learn a context memory controller that manages the memory by maintaining the cumulative meaning of sequential user utterances. We evaluate our approach on three semantic parsing benchmarks. Experimental results show that our model can better process context-dependent information and demonstrates improved performance without using task-specific decoders.",
    "venue":"Transactions of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-09-07",
    "citation_count":9,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2110-07358",
      "ArXiv":"2110.07358",
      "DOI":"10.1162\/tacl_a_00422",
      "CorpusId":238856853
    }
  },
  {
    "paper_id":"d88bbcf9aa534e6ff78643a0daa68fc75a7e5bdf",
    "title":"SParC: Cross-Domain Semantic Parsing in Context",
    "authors":[
      "Tao Yu",
      "Rui Zhang",
      "Michihiro Yasunaga",
      "Y. Tan",
      "Xi Victoria Lin",
      "Suyi Li",
      "H. Er",
      "Irene Z Li",
      "B. Pang",
      "Tao Chen",
      "Emily Ji",
      "Shreya Dixit",
      "David Proctor",
      "Sungrok Shim",
      "Jonathan Kraft",
      "Vincent Zhang",
      "Caiming Xiong",
      "R. Socher",
      "Dragomir R. Radev"
    ],
    "abstract":"We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https:\/\/yale-lily.github.io\/sparc.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-06-01",
    "citation_count":211,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2948465491",
      "DBLP":"conf\/acl\/YuZYTLLELPCJDPS19",
      "ArXiv":"1906.02285",
      "ACL":"P19-1443",
      "DOI":"10.18653\/v1\/P19-1443",
      "CorpusId":174802873
    }
  },
  {
    "paper_id":"9ead8b26ffc2be9d4f0e5409b2033b306a8bb012",
    "title":"Context Dependent Semantic Parsing: A Survey",
    "authors":[
      "Zhuang Li",
      "Lizhen Qu",
      "Gholamreza Haffari"
    ],
    "abstract":"Semantic parsing is the task of translating natural language utterances into machine-readable meaning representations. Currently, most semantic parsing methods are not able to utilize the contextual information (e.g. dialogue and comments history), which has a great potential to boost the semantic parsing systems. To address this issue, context dependent semantic parsing has recently drawn a lot of attention. In this survey, we investigate progress on the methods for the context dependent semantic parsing, together with the current datasets and tasks. We then point out open problems and challenges for future research in this area.",
    "venue":"International Conference on Computational Linguistics",
    "year":2020,
    "publication_date":"2020-11-01",
    "citation_count":20,
    "reference_count":56,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3108094909",
      "DBLP":"journals\/corr\/abs-2011-00797",
      "ACL":"2020.coling-main.226",
      "ArXiv":"2011.00797",
      "DOI":"10.18653\/V1\/2020.COLING-MAIN.226",
      "CorpusId":226227399
    }
  },
  {
    "paper_id":"41cdecbf34ecf3e43d2182c30e159d4efe91e2b7",
    "title":"Unpaired Person Image Generation With Semantic Parsing Transformation",
    "authors":[
      "Sijie Song",
      "Wei Zhang",
      "Jiaying Liu",
      "Zongming Guo",
      "Tao Mei"
    ],
    "abstract":"In this paper, we tackle the problem of pose-guided person image generation with unpaired data, which is a challenging problem due to non-rigid spatial deformation. Instead of learning a fixed mapping directly between human bodies as previous methods, we propose a new pathway to decompose a single fixed mapping into two subtasks, namely, semantic parsing transformation and appearance generation. First, to simplify the learning for non-rigid deformation, a semantic generative network is developed to transform semantic parsing maps between different poses. Second, guided by semantic parsing maps, we render the foreground and background image, respectively. A foreground generative network learns to synthesize semantic-aware textures, and another background generative network learns to predict missing background regions caused by pose changes. Third, we enable pseudo-label training with unpaired data, and demonstrate that end-to-end training of the overall network further refines the semantic map prediction and final results accordingly. Moreover, our method is generalizable to other person image generation tasks defined on semantic maps, e.g., clothing texture transfer, controlled image manipulation, and virtual try-on. Experimental results on DeepFashion and Market-1501 datasets demonstrate the superiority of our method, especially in keeping better body shapes and clothing attributes, as well as rendering structure-coherent backgrounds.",
    "venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year":2020,
    "publication_date":"2020-05-04",
    "citation_count":17,
    "reference_count":60,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "DBLP":"journals\/pami\/SongZLGM21",
      "MAG":"3021075672",
      "DOI":"10.1109\/TPAMI.2020.2992105",
      "CorpusId":218505054,
      "PubMed":"32365019"
    }
  },
  {
    "paper_id":"c45e5323cfc2dc83c0ee43b457b3eebc663c9e28",
    "title":"Broad-Coverage Semantic Parsing as Transduction",
    "authors":[
      "Sheng Zhang",
      "Xutai Ma",
      "Kevin Duh",
      "Benjamin Van Durme"
    ],
    "abstract":"We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks – AMR, SDP and UCCA – demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-09-05",
    "citation_count":74,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1909.02607",
      "DBLP":"journals\/corr\/abs-1909-02607",
      "MAG":"2971910751",
      "ACL":"D19-1392",
      "DOI":"10.18653\/v1\/D19-1392",
      "CorpusId":202233360
    }
  },
  {
    "paper_id":"2068825cabd94c951a0282ed731a8b8f2da1721c",
    "title":"StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing",
    "authors":[
      "Pengcheng Yin",
      "Chunting Zhou",
      "Junxian He",
      "Graham Neubig"
    ],
    "abstract":"Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2018,
    "publication_date":"2018-06-01",
    "citation_count":103,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2950420506",
      "DBLP":"journals\/corr\/abs-1806-07832",
      "ArXiv":"1806.07832",
      "ACL":"P18-1070",
      "DOI":"10.18653\/v1\/P18-1070",
      "CorpusId":49325612
    }
  },
  {
    "paper_id":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c",
    "title":"Reranking for Neural Semantic Parsing",
    "authors":[
      "Pengcheng Yin",
      "Graham Neubig"
    ],
    "abstract":"Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CoNaLa) and 2.9% in accuracy (Django), outperforming the best published neural parser results on all four datasets.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-07-01",
    "citation_count":70,
    "reference_count":31,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2949215742",
      "DBLP":"conf\/acl\/YinN19",
      "ACL":"P19-1447",
      "DOI":"10.18653\/v1\/P19-1447",
      "CorpusId":196206775
    }
  },
  {
    "paper_id":"a8e5b0d370f7adfafb6640047951393a1843c29e",
    "title":"A Pilot Study for Chinese SQL Semantic Parsing",
    "authors":[
      "Qingkai Min",
      "Yu Shi",
      "Yue Zhang"
    ],
    "abstract":"The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-09-29",
    "citation_count":67,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/MinSZ19",
      "ArXiv":"1909.13293",
      "ACL":"D19-1377",
      "MAG":"2970605129",
      "DOI":"10.18653\/v1\/D19-1377",
      "CorpusId":202783651
    }
  },
  {
    "paper_id":"676a7e362e283cb953135f506eb0a07a8b0be0ed",
    "title":"A Survey on Semantic Parsing",
    "authors":[
      "Aishwarya Kamath",
      "Rajarshi Das"
    ],
    "abstract":"A significant amount of information in today's world is stored in structured and semi-structured knowledge bases. Efficient and simple methods to query them are essential and must not be restricted to only those who have expertise in formal query languages. The field of semantic parsing deals with converting natural language utterances to logical forms that can be easily executed on a knowledge base. In this survey, we examine the various components of a semantic parsing system and discuss prominent work ranging from the initial rule based methods to the current neural approaches to program synthesis. We also discuss methods that operate using varying levels of supervision and highlight the key challenges involved in the learning of such systems.",
    "venue":"Conference on Automated Knowledge Base Construction",
    "year":2018,
    "publication_date":"2018-12-03",
    "citation_count":127,
    "reference_count":97,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2952781527",
      "DBLP":"journals\/corr\/abs-1812-00978",
      "ArXiv":"1812.00978",
      "DOI":"10.24432\/C5WC7D",
      "CorpusId":54181011
    }
  },
  {
    "paper_id":"5301f6320300ac87b42018da171f17e6b7842486",
    "title":"Semantic Parsing with Dual Learning",
    "authors":[
      "Ruisheng Cao",
      "Su Zhu",
      "Chen Liu",
      "Jieyu Li",
      "Kai Yu"
    ],
    "abstract":"Semantic parsing converts natural language queries into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-07-10",
    "citation_count":64,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1907.05343",
      "MAG":"2950869915",
      "DBLP":"journals\/corr\/abs-1907-05343",
      "ACL":"P19-1007",
      "DOI":"10.18653\/v1\/p19-1007",
      "CorpusId":195886068
    }
  },
  {
    "paper_id":"295065d942abca0711300b2b4c39829551060578",
    "title":"BERTScore: Evaluating Text Generation with BERT",
    "authors":[
      "Tianyi Zhang",
      "Varsha Kishore",
      "Felix Wu",
      "Kilian Q. Weinberger",
      "Yoav Artzi"
    ],
    "abstract":"We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
    "venue":"International Conference on Learning Representations",
    "year":2019,
    "publication_date":"2019-04-21",
    "citation_count":7024,
    "reference_count":105,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2936695845",
      "ArXiv":"1904.09675",
      "DBLP":"journals\/corr\/abs-1904-09675",
      "CorpusId":127986044
    }
  },
  {
    "paper_id":"ec45c3f0f88c8ce1deb5baa71c2c0e14ad64d249",
    "title":"Evaluating Text-to-Visual Generation with Image-to-Text Generation",
    "authors":[
      "Zhiqiu Lin",
      "Deepak Pathak",
      "Baiqi Li",
      "Jiayao Li",
      "Xide Xia",
      "Graham Neubig",
      "Pengchuan Zhang",
      "Deva Ramanan"
    ],
    "abstract":"Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a\"bag of words\", conflating prompts such as\"the horse is eating the grass\"with\"the grass is eating the horse\". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a\"Yes\"answer to a simple\"Does this figure show '{text}'?\"question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.",
    "venue":"European Conference on Computer Vision",
    "year":2024,
    "publication_date":"2024-04-01",
    "citation_count":287,
    "reference_count":94,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/eccv\/LinPLLXNZR24",
      "ArXiv":"2404.01291",
      "DOI":"10.48550\/arXiv.2404.01291",
      "CorpusId":268857167
    }
  },
  {
    "paper_id":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
    "title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    "authors":[
      "Sewon Min",
      "Kalpesh Krishna",
      "Xinxi Lyu",
      "M. Lewis",
      "Wen-tau Yih",
      "Pang Wei Koh",
      "Mohit Iyyer",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ],
    "abstract":"Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-05-23",
    "citation_count":917,
    "reference_count":78,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.14251",
      "DBLP":"conf\/emnlp\/MinKLLYKIZH23",
      "DOI":"10.48550\/arXiv.2305.14251",
      "CorpusId":258841470
    }
  },
  {
    "paper_id":"c57293882b2561e1ba03017902df9fc2f289dea2",
    "title":"Hierarchical Text-Conditional Image Generation with CLIP Latents",
    "authors":[
      "A. Ramesh",
      "Prafulla Dhariwal",
      "Alex Nichol",
      "Casey Chu",
      "Mark Chen"
    ],
    "abstract":"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-04-13",
    "citation_count":7978,
    "reference_count":66,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2204.06125",
      "DBLP":"journals\/corr\/abs-2204-06125",
      "DOI":"10.48550\/arXiv.2204.06125",
      "CorpusId":248097655
    }
  },
  {
    "paper_id":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
    "title":"LAION-5B: An open large-scale dataset for training next generation image-text models",
    "authors":[
      "Christoph Schuhmann",
      "R. Beaumont",
      "R. Vencu",
      "Cade Gordon",
      "Ross Wightman",
      "Mehdi Cherti",
      "Theo Coombes",
      "Aarush Katta",
      "Clayton Mullis",
      "Mitchell Wortsman",
      "P. Schramowski",
      "Srivatsa Kundurthy",
      "Katherine Crowson",
      "Ludwig Schmidt",
      "R. Kaczmarczyk",
      "J. Jitsev"
    ],
    "abstract":"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https:\/\/laion.ai\/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets\/",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-10-16",
    "citation_count":4299,
    "reference_count":109,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/SchuhmannBVGWCC22",
      "ArXiv":"2210.08402",
      "DOI":"10.48550\/arXiv.2210.08402",
      "CorpusId":252917726
    }
  },
  {
    "paper_id":"4ae52766028e69186052ea8f33a137fbbbdb986a",
    "title":"BLEURT: Learning Robust Metrics for Text Generation",
    "authors":[
      "Thibault Sellam",
      "Dipanjan Das",
      "Ankur P. Parikh"
    ],
    "abstract":"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-04-09",
    "citation_count":1678,
    "reference_count":52,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3035252911",
      "ArXiv":"2004.04696",
      "DBLP":"conf\/acl\/SellamDP20",
      "ACL":"2020.acl-main.704",
      "DOI":"10.18653\/v1\/2020.acl-main.704",
      "CorpusId":215548699
    }
  },
  {
    "paper_id":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
    "title":"Zero-Shot Text-to-Image Generation",
    "authors":[
      "A. Ramesh",
      "Mikhail Pavlov",
      "Gabriel Goh",
      "Scott Gray",
      "Chelsea Voss",
      "Alec Radford",
      "Mark Chen",
      "I. Sutskever"
    ],
    "abstract":"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
    "venue":"International Conference on Machine Learning",
    "year":2021,
    "publication_date":"2021-02-24",
    "citation_count":5745,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2102-12092",
      "MAG":"3170016573",
      "ArXiv":"2102.12092",
      "CorpusId":232035663
    }
  },
  {
    "paper_id":"7002ae048e4b8c9133a55428441e8066070995cb",
    "title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
    "authors":[
      "Alex Nichol",
      "Prafulla Dhariwal",
      "A. Ramesh",
      "Pranav Shyam",
      "Pamela Mishkin",
      "Bob McGrew",
      "I. Sutskever",
      "Mark Chen"
    ],
    "abstract":"Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https:\/\/github.com\/openai\/glide-text2im.",
    "venue":"International Conference on Machine Learning",
    "year":2021,
    "publication_date":"2021-12-20",
    "citation_count":4202,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2112.10741",
      "DBLP":"journals\/corr\/abs-2112-10741",
      "CorpusId":245335086
    }
  },
  {
    "paper_id":"a6a7724763d8adba466519489b0b9d209e7f2d15",
    "title":"BARTScore: Evaluating Generated Text as Text Generation",
    "authors":[
      "Weizhe Yuan",
      "Graham Neubig",
      "Pengfei Liu"
    ],
    "abstract":"A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to\/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https:\/\/github.com\/neulab\/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http:\/\/explainaboard.nlpedia.ai\/leaderboard\/task-meval\/ on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.",
    "venue":"Neural Information Processing Systems",
    "year":2021,
    "publication_date":"2021-06-22",
    "citation_count":942,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/YuanNL21",
      "ArXiv":"2106.11520",
      "CorpusId":235593404
    }
  },
  {
    "paper_id":"b7b7a549629bbe7fa325572339938980e77d155c",
    "title":"AnyText: Multilingual Visual Text Generation And Editing",
    "authors":[
      "Yuxiang Tuo",
      "Wangmeng Xiang",
      "Jun-Yan He",
      "Yifeng Geng",
      "Xuansong Xie"
    ],
    "abstract":"Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https:\/\/github.com\/tyxsspa\/AnyText to improve and promote the development of text generation technology.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-11-06",
    "citation_count":110,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2311-03054",
      "ArXiv":"2311.03054",
      "DOI":"10.48550\/arXiv.2311.03054",
      "CorpusId":265034144
    }
  },
  {
    "paper_id":"1386b8a11929cf02da291c56aca353e33bbc22ed",
    "title":"Diffusion-LM Improves Controllable Text Generation",
    "authors":[
      "Xiang Lisa Li",
      "John Thickstun",
      "Ishaan Gulrajani",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "abstract":"Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-05-27",
    "citation_count":1036,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/LiTGLH22",
      "ArXiv":"2205.14217",
      "DOI":"10.48550\/arXiv.2205.14217",
      "CorpusId":249192356
    }
  },
  {
    "paper_id":"44279244407a64431810f982be6d0c7da4429dd7",
    "title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
    "authors":[
      "Renqian Luo",
      "Liai Sun",
      "Yingce Xia",
      "Tao Qin",
      "Sheng Zhang",
      "Hoifung Poon",
      "Tie-Yan Liu"
    ],
    "abstract":"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
    "venue":"Briefings Bioinform.",
    "year":2022,
    "publication_date":"2022-09-24",
    "citation_count":1052,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science",
      "Medicine"
    ],
    "external_ids":{
      "ArXiv":"2210.10341",
      "DBLP":"journals\/bib\/LuoSXQZPL22",
      "DOI":"10.1093\/bib\/bbac409",
      "CorpusId":252542956,
      "PubMed":"36156661"
    }
  },
  {
    "paper_id":"b618998f9f3634331c8762342bbf110b74ad3fc0",
    "title":"LongLaMP: A Benchmark for Personalized Long-form Text Generation",
    "authors":[
      "Ishita Kumar",
      "Snigdha Viswanathan",
      "Sushrita Yerra",
      "Alireza Salemi",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Hanieh Deilamsalehy",
      "Xiang Chen",
      "Ruiyi Zhang",
      "Shubham Agarwal",
      "Nedim Lipka",
      "Hamed Zamani"
    ],
    "abstract":"Long-text generation is seemingly ubiquitous in real-world applications of large language models such as generating an email or writing a review. Despite the fundamental importance and prevalence of long-text generation in many practical applications, existing work on personalized generation has focused on the generation of very short text. To overcome these limitations, we study the problem of personalized long-text generation, that is, generating long-text that is personalized for a specific user while being practically useful for the vast majority of real-world applications that naturally require the generation of longer text. In this work, we demonstrate the importance of user-specific personalization for long-text generation tasks and develop the Long-text Language Model Personalization (LongLaMP) Benchmark. LongLaMP provides a comprehensive and diverse evaluation framework for personalized long-text generation. Extensive experiments on LongLaMP for zero-shot and fine-tuned language tasks demonstrate the effectiveness of the proposed benchmark and its utility for developing and evaluating techniques for personalized long-text generation across a wide variety of long-text generation tasks. The results highlight the importance of personalization across a wide variety of long-text generation tasks. Finally, we release the benchmark for others to use for this important problem.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-06-27",
    "citation_count":45,
    "reference_count":37,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2407-11016",
      "ArXiv":"2407.11016",
      "DOI":"10.48550\/arXiv.2407.11016",
      "CorpusId":271218187
    }
  },
  {
    "paper_id":"7fbf028793a5252941e358366f57837888fcbf11",
    "title":"Controllable Text Generation for Large Language Models: A Survey",
    "authors":[
      "Xun Liang",
      "Hanyu Wang",
      "Yezhaohui Wang",
      "Shichao Song",
      "Jiawei Yang",
      "Simin Niu",
      "Jie Hu",
      "Dan Liu",
      "Shunyu Yao",
      "Feiyu Xiong",
      "Zhiyu Li"
    ],
    "abstract":"In Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated high text generation quality. However, in real-world applications, LLMs must meet increasingly complex requirements. Beyond avoiding misleading or inappropriate content, LLMs are also expected to cater to specific user needs, such as imitating particular writing styles or generating text with poetic richness. These varied demands have driven the development of Controllable Text Generation (CTG) techniques, which ensure that outputs adhere to predefined control conditions--such as safety, sentiment, thematic consistency, and linguistic style--while maintaining high standards of helpfulness, fluency, and diversity. This paper systematically reviews the latest advancements in CTG for LLMs, offering a comprehensive definition of its core concepts and clarifying the requirements for control conditions and text quality. We categorize CTG tasks into two primary types: content control and attribute control. The key methods are discussed, including model retraining, fine-tuning, reinforcement learning, prompt engineering, latent space manipulation, and decoding-time intervention. We analyze each method's characteristics, advantages, and limitations, providing nuanced insights for achieving generation control. Additionally, we review CTG evaluation methods, summarize its applications across domains, and address key challenges in current research, including reduced fluency and practicality. We also propose several appeals, such as placing greater emphasis on real-world applications in future research. This paper aims to offer valuable guidance to researchers and developers in the field. Our reference list and Chinese version are open-sourced at https:\/\/github.com\/IAAR-Shanghai\/CTGSurvey.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-08-22",
    "citation_count":43,
    "reference_count":0,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2408-12599",
      "ArXiv":"2408.12599",
      "DOI":"10.48550\/arXiv.2408.12599",
      "CorpusId":271924120
    }
  },
  {
    "paper_id":"079e6a2003e8e6ef1d346fbc22daf4399eaf6a4e",
    "title":"HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models",
    "authors":[
      "Haoran Que",
      "Feiyu Duan",
      "Liqun He",
      "Yutao Mou",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Wenge Rong",
      "Z. Wang",
      "Jian Yang",
      "Ge Zhang",
      "Junran Peng",
      "Zhaoxiang Zhang",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "abstract":"In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (e.g., long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are not well investigated. Therefore, we introduce the Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive, in-the-wild, and open-ended benchmark to evaluate LLMs' performance in generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation. Besides, we propose Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation method that significantly reduces the time and effort required for human evaluation while maintaining a high correlation with human evaluation. We have conducted extensive experiments across around 30 mainstream LLMs and observed that the current LLMs lack long text generation capabilities. Specifically, first, regardless of whether the instructions include explicit or implicit length constraints, we observe that most LLMs cannot generate text that is longer than 4000 words. Second, we observe that while some LLMs can generate longer text, many issues exist (e.g., severe repetition and quality degradation). Third, to demonstrate the effectiveness of HelloEval, we compare HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge methods, which show that HelloEval has the highest correlation with human evaluation. We release our code in https:\/\/github.com\/Quehry\/HelloBench.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-09-24",
    "citation_count":32,
    "reference_count":100,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2409-16191",
      "ArXiv":"2409.16191",
      "DOI":"10.48550\/arXiv.2409.16191",
      "CorpusId":272832029
    }
  },
  {
    "paper_id":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744",
    "title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
    "authors":[
      "Nataniel Ruiz",
      "Yuanzhen Li",
      "Varun Jampani",
      "Y. Pritch",
      "Michael Rubinstein",
      "Kfir Aberman"
    ],
    "abstract":"Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https:\/\/dreambooth.github.io\/",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2022,
    "publication_date":"2022-08-25",
    "citation_count":3529,
    "reference_count":78,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2208.12242",
      "DBLP":"conf\/cvpr\/RuizLJPRA23",
      "DOI":"10.1109\/CVPR52729.2023.02155",
      "CorpusId":251800180
    }
  },
  {
    "paper_id":"289533e16e509d0ba8f499a371b4e470f3e492de",
    "title":"SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
    "authors":[
      "A. Hou",
      "Jingyu (Jack) Zhang",
      "Tianxing He",
      "Yichen Wang",
      "Yung-Sung Chuang",
      "Hongwei Wang",
      "Lingfeng Shen",
      "Benjamin Van Durme",
      "Daniel Khashabi",
      "Yulia Tsvetkov"
    ],
    "abstract":"Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a “bigram paraphrase” attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2024,
    "publication_date":null,
    "citation_count":91,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2024.naacl-long.226",
      "DBLP":"conf\/naacl\/HouZHWCWSDKT24",
      "DOI":"10.18653\/v1\/2024.naacl-long.226",
      "CorpusId":263831179
    }
  },
  {
    "paper_id":"250430f92bc02b99161dcf915d13eb102b3f1bf2",
    "title":"Large Language Model (LLM) AI text generation detection based on transformer deep learning algorithm",
    "authors":[
      "Yuhong Mo",
      "Hao Qin",
      "Yushan Dong",
      "Ziyi Zhu",
      "Zhenglin Li"
    ],
    "abstract":"In this paper, a tool for detecting LLM AI text generation is developed based on the Transformer model, aiming to improve the accuracy of AI text generation detection and provide reference for subsequent research. Firstly the text is Unicode normalised, converted to lowercase form, characters other than non-alphabetic characters and punctuation marks are removed by regular expressions, spaces are added around punctuation marks, first and last spaces are removed, consecutive ellipses are replaced with single spaces and the text is connected using the specified delimiter. Next remove non-alphabetic characters and extra whitespace characters, replace multiple consecutive whitespace characters with a single space and again convert to lowercase form. The deep learning model combines layers such as LSTM, Transformer and CNN for text classification or sequence labelling tasks. The training and validation sets show that the model loss decreases from 0.127 to 0.005 and accuracy increases from 94.96 to 99.8, indicating that the model has good detection and classification ability for AI generated text. The test set confusion matrix and accuracy show that the model has 99% prediction accuracy for AI-generated text, with a precision of 0.99, a recall of 1, and an f1 score of 0.99, achieving a very high classification accuracy. Looking forward, it has the prospect of wide application in the field of AI text detection.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-04-06",
    "citation_count":60,
    "reference_count":10,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2405.06652",
      "DBLP":"journals\/corr\/abs-2405-06652",
      "DOI":"10.48550\/arXiv.2405.06652",
      "CorpusId":269757069
    }
  },
  {
    "paper_id":"a54204cbbf2b117cb472fd0557ef13d1b3d8bead",
    "title":"Attribute First, then Generate: Locally-attributable Grounded Text Generation",
    "authors":[
      "Aviv Slobodkin",
      "Eran Hirsch",
      "Arie Cattan",
      "Tal Schuster",
      "Ido Dagan"
    ],
    "abstract":"Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named\"Attribute First, then Generate\", breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (\"select first\") and then conditioning the generation process on them (\"then generate\"), we ensure these segments also act as the output's fine-grained attributions (\"select\"becomes\"attribute\"). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2024,
    "publication_date":"2024-03-25",
    "citation_count":42,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/SlobodkinHCSD24",
      "ArXiv":"2403.17104",
      "DOI":"10.48550\/arXiv.2403.17104",
      "CorpusId":268692231
    }
  },
  {
    "paper_id":"6860189c0c7c99ebc35b36ec37310823022336c9",
    "title":"Generative Steganography Based on Long Readable Text Generation",
    "authors":[
      "Yi Cao",
      "Zhili Zhou",
      "Chinmay Chakraborty",
      "Meimin Wang",
      "Q. M. J. Wu",
      "Xingming Sun",
      "K. Yu"
    ],
    "abstract":"Text steganography has received a lot of attention in the application of covert communication. How to ensure desirable capacity and imperceptibility has become a key issue in text steganography. There are two typical approaches, i.e., text-selection-based steganography and text-generation-based steganography. However, the text-selection-based approaches generally have the very low hidden capacity and are not applicable in practical scenarios. Although the text-generation-based approaches can embed secret messages with higher capacity during text generation, they are prone to semantic incoherence and semantic errors when generating long texts. To address the abovementioned issues, this article proposes a novel text steganography based on long readable text generation. It first determines the topic of the stego-text according to the scenarios of the communication parties. Then, the plug and play language model (PPLM) is explored to generate the long readable stego-text conforming to the topic with semantic coherency. A given secret message is hidden during text generation by selecting proper words in an established embeddable candidate word pool (ECWP). Establishing the ECWP prevents the language model (LM) from selecting words with low probability in the text generation, thereby avoiding the generation of low-quality or even grammatically incorrect stego-text. Experimental results show that the proposed approach significantly increases hidden capacity while maintaining good imperceptibility compared with the existing approaches.",
    "venue":"IEEE Transactions on Computational Social Systems",
    "year":2024,
    "publication_date":"2024-08-01",
    "citation_count":35,
    "reference_count":62,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tcss\/CaoZCWWSY24",
      "DOI":"10.1109\/TCSS.2022.3174013",
      "CorpusId":248937326
    }
  },
  {
    "paper_id":"69144d537f90f214d5b07a7c79121d16afd7da16",
    "title":"DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
    "authors":[
      "Shansan Gong",
      "Mukai Li",
      "Jiangtao Feng",
      "Zhiyong Wu",
      "Lingpeng Kong"
    ],
    "abstract":"Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive\/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https:\/\/github.com\/Shark-NLP\/DiffuSeq}",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-10-17",
    "citation_count":427,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/GongLF0K23",
      "ArXiv":"2210.08933",
      "DOI":"10.48550\/arXiv.2210.08933",
      "CorpusId":252917661
    }
  },
  {
    "paper_id":"88b62496cbc52072bfa8f4b29d172b0477b701bc",
    "title":"Contrastive Decoding: Open-ended Text Generation as Optimization",
    "authors":[
      "Xiang Lisa Li",
      "Ari Holtzman",
      "Daniel Fried",
      "Percy Liang",
      "Jason Eisner",
      "Tatsunori Hashimoto",
      "Luke Zettlemoyer",
      "M. Lewis"
    ],
    "abstract":"Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-10-27",
    "citation_count":480,
    "reference_count":39,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/LiHFLEHZL23",
      "ArXiv":"2210.15097",
      "ACL":"2023.acl-long.687",
      "DOI":"10.48550\/arXiv.2210.15097",
      "CorpusId":253157949
    }
  },
  {
    "paper_id":"cb596bffc5c5042c254058b62317a57fa156fea4",
    "title":"Unifying Vision-and-Language Tasks via Text Generation",
    "authors":[
      "Jaemin Cho",
      "Jie Lei",
      "Hao Tan",
      "Mohit Bansal"
    ],
    "abstract":"Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https:\/\/github.com\/j-min\/VL-T5",
    "venue":"International Conference on Machine Learning",
    "year":2021,
    "publication_date":"2021-02-05",
    "citation_count":598,
    "reference_count":86,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2102.02779",
      "DBLP":"journals\/corr\/abs-2102-02779",
      "CorpusId":231802355
    }
  },
  {
    "paper_id":"041f5dbfcd07a3369ac44a6b902ee4b145eccf2b",
    "title":"Towards a Unified Multi-Dimensional Evaluator for Text Generation",
    "authors":[
      "Ming Zhong",
      "Yang Liu",
      "Da Yin",
      "Yuning Mao",
      "Yizhu Jiao",
      "Peng Liu",
      "Chenguang Zhu",
      "Heng Ji",
      "Jiawei Han"
    ],
    "abstract":"Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data, and all pre-trained evaluators are available at https:\/\/github.com\/maszhongming\/UniEval.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2022,
    "publication_date":"2022-10-13",
    "citation_count":316,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2210.07197",
      "ACL":"2022.emnlp-main.131",
      "DBLP":"journals\/corr\/abs-2210-07197",
      "DOI":"10.18653\/v1\/2022.emnlp-main.131",
      "CorpusId":252873117
    }
  },
  {
    "paper_id":"02f033482b8045c687316ef81ba7aaae9f0a2e1c",
    "title":"DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    "authors":[
      "Alisa Liu",
      "Maarten Sap",
      "Ximing Lu",
      "Swabha Swayamdipta",
      "Chandra Bhagavatula",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "abstract":"Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and\/or “anti-expert” LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-05-07",
    "citation_count":425,
    "reference_count":53,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/LiuSLSBSC20",
      "ArXiv":"2105.03023",
      "ACL":"2021.acl-long.522",
      "DOI":"10.18653\/v1\/2021.acl-long.522",
      "CorpusId":235313967
    }
  },
  {
    "paper_id":"b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
    "title":"FUDGE: Controlled Text Generation With Future Discriminators",
    "authors":[
      "Kevin Yang",
      "D. Klein"
    ],
    "abstract":"We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G’s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor’s outputs to adjust G’s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks — couplet completion in poetry, topic control in language generation, and formality change in machine translation — and observe gains in all three tasks.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-11",
    "citation_count":376,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/YangK21",
      "ArXiv":"2104.05218",
      "MAG":"3169017236",
      "ACL":"2021.naacl-main.276",
      "DOI":"10.18653\/v1\/2021.naacl-main.276",
      "CorpusId":233210709
    }
  },
  {
    "paper_id":"5406129d9d7d00dc310671c43597101b0ee93629",
    "title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
    "authors":[
      "Rinon Gal",
      "Yuval Alaluf",
      "Y. Atzmon",
      "Or Patashnik",
      "Amit H. Bermano",
      "Gal Chechik",
      "D. Cohen-Or"
    ],
    "abstract":"Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new\"words\"in the embedding space of a frozen text-to-image model. These\"words\"can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https:\/\/textual-inversion.github.io",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-08-02",
    "citation_count":2305,
    "reference_count":93,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2208.01618",
      "DBLP":"journals\/corr\/abs-2208-01618",
      "DOI":"10.48550\/arXiv.2208.01618",
      "CorpusId":251253049
    }
  },
  {
    "paper_id":"3fc6184c72fd55b67409ef87493c333f15a33180",
    "title":"CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
    "authors":[
      "Hao He",
      "Yinghao Xu",
      "Yuwei Guo",
      "Gordon Wetzstein",
      "Bo Dai",
      "Hongsheng Li",
      "Ceyuan Yang"
    ],
    "abstract":"Controllability plays a crucial role in video generation, as it allows users to create and edit content more precisely. Existing models, however, lack control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for video diffusion models. Our approach explores effective camera trajectory parameterization along with a plug-and-play camera pose control module that is trained on top of a video diffusion model, leaving other modules of the base model untouched. Moreover, a comprehensive study on the effect of various training datasets is conducted, suggesting that videos with diverse camera distributions and similar appearance to the base model indeed enhance controllability and generalization. Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise camera control with different video generation models, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-04-02",
    "citation_count":223,
    "reference_count":72,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2404-02101",
      "ArXiv":"2404.02101",
      "DOI":"10.48550\/arXiv.2404.02101",
      "CorpusId":268857272
    }
  },
  {
    "paper_id":"41b796b026a1d322de6ef0b280d3e2e68eee65bd",
    "title":"Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
    "authors":[
      "Xin Cheng",
      "Di Luo",
      "Xiuying Chen",
      "Lemao Liu",
      "Dongyan Zhao",
      "Rui Yan"
    ],
    "abstract":"With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-03",
    "citation_count":135,
    "reference_count":118,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-02437",
      "ArXiv":"2305.02437",
      "DOI":"10.48550\/arXiv.2305.02437",
      "CorpusId":258479968
    }
  },
  {
    "paper_id":"48f4f65df8eb4c435dcb14851f876c270ce2cfd5",
    "title":"Controlled Text Generation with Natural Language Instructions",
    "authors":[
      "Wangchunshu Zhou",
      "Y. Jiang",
      "Ethan Gotlieb Wilcox",
      "Ryan Cotterell",
      "Mrinmaya Sachan"
    ],
    "abstract":"Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCTG is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, InstructCTG allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-04-27",
    "citation_count":109,
    "reference_count":59,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2304-14293",
      "ArXiv":"2304.14293",
      "DOI":"10.48550\/arXiv.2304.14293",
      "CorpusId":258352203
    }
  },
  {
    "paper_id":"460609e217fd59eaa34f5e11a820661f8ec8d7b6",
    "title":"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback",
    "authors":[
      "Wenda Xu",
      "Danqing Wang",
      "Liangming Pan",
      "Zhenqiao Song",
      "Markus Freitag",
      "William Yang Wang",
      "Lei Li"
    ],
    "abstract":"The field of automatic evaluation of text generation made tremendous progress in the last few years. In particular, since the advent of neural metrics, like COMET, BLEURT and SEScore2, the newest generation of metrics show a high correlation with human judgment. Unfortunately, quality scores generated with neural metrics are not interpretable and it is unclear which part of the generation output is criticized by the metrics. To address this limitation, we present I NSTRUCT S CORE , an open-source, explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment. We evaluate I NSTRUCT S CORE on the WMT22 Zh-En translation task, where our 7B model surpasses other LLM-based baselines, including those based on 175B GPT3. Impressively, our I NSTRUCT S CORE , even without direct super-vision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which was fine-tuned on human ratings. 1",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":null,
    "citation_count":121,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/emnlp\/XuWPSFWL23",
      "DOI":"10.48550\/arXiv.2305.14282",
      "CorpusId":258841553
    }
  },
  {
    "paper_id":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba",
    "title":"GlyphControl: Glyph Conditional Control for Visual Text Generation",
    "authors":[
      "Yukang Yang",
      "Dongnan Gui",
      "Yuhui Yuan",
      "Haisong Ding",
      "Hang-Rui Hu",
      "Kai Chen"
    ],
    "abstract":"Recently, there has been an increasing interest in developing diffusion-based text-to-image generative models capable of generating coherent and well-formed visual text. In this paper, we propose a novel and efficient approach called GlyphControl to address this task. Unlike existing methods that rely on character-aware text encoders like ByT5 and require retraining of text-to-image models, our approach leverages additional glyph conditional information to enhance the performance of the off-the-shelf Stable-Diffusion model in generating accurate visual text. By incorporating glyph instructions, users can customize the content, location, and size of the generated text according to their specific requirements. To facilitate further research in visual text generation, we construct a training benchmark dataset called LAION-Glyph. We evaluate the effectiveness of our approach by measuring OCR-based metrics, CLIP score, and FID of the generated visual text. Our empirical evaluations demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of our method.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-29",
    "citation_count":102,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/YangGYLDH023",
      "ArXiv":"2305.18259",
      "DOI":"10.48550\/arXiv.2305.18259",
      "CorpusId":258960586
    }
  },
  {
    "paper_id":"54b6e5dcef733c151adef0ac06430f63cb301a36",
    "title":"AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "authors":[
      "Tong Wu",
      "Zhihao Fan",
      "Xiao Liu",
      "Yeyun Gong",
      "Yelong Shen",
      "Jian Jiao",
      "Haitao Zheng",
      "Juntao Li",
      "Zhongyu Wei",
      "Jian Guo",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "abstract":"Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results. Our code is available at https:\/\/github.com\/microsoft\/ProphetNet\/tree\/master\/AR-diffusion.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-16",
    "citation_count":103,
    "reference_count":46,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.09515",
      "DBLP":"journals\/corr\/abs-2305-09515",
      "DOI":"10.48550\/arXiv.2305.09515",
      "CorpusId":258714669
    }
  },
  {
    "paper_id":"1f898d66acabff511a3871b82799aa73c0055402",
    "title":"A Reparameterized Discrete Diffusion Model for Text Generation",
    "authors":[
      "Lin Zheng",
      "Jianbo Yuan",
      "Lei Yu",
      "Lingpeng Kong"
    ],
    "abstract":"This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.",
    "venue":"arXiv.org",
    "year":2023,
    "publication_date":"2023-02-11",
    "citation_count":107,
    "reference_count":99,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-05737",
      "ArXiv":"2302.05737",
      "DOI":"10.48550\/arXiv.2302.05737",
      "CorpusId":256826865
    }
  },
  {
    "paper_id":"e04a80263d252a3d8a382ba37a249b9345620570",
    "title":"Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
    "authors":[
      "Sumanth Dathathri",
      "Andrea Madotto",
      "Janice Lan",
      "Jane Hung",
      "Eric Frank",
      "Piero Molino",
      "J. Yosinski",
      "Rosanne Liu"
    ],
    "abstract":"Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.",
    "venue":"International Conference on Learning Representations",
    "year":2019,
    "publication_date":"2019-09-25",
    "citation_count":1064,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/DathathriMLHFMY20",
      "ArXiv":"1912.02164",
      "MAG":"2993398598",
      "CorpusId":208617790
    }
  },
  {
    "paper_id":"a7ae51ab170abe719a3b354e80ec4a693cf2d42e",
    "title":"Artificial intelligence, text generation tools and ChatGPT – does digital watermarking offer a solution?",
    "authors":[
      "Thomas A. Lancaster"
    ],
    "abstract":"Text generation tools, often presented as a form of generative artificial intelligence, have the potential to pose a threat to the integrity of the educational system. They can be misused to afford students marks and qualifications that they do not deserve. The emergence of recent tools, such as ChatGPT, appear to have left the educational community unprepared, despite the fact that the computer science community has been working to develop and improve such tools for years. This paper provides an introduction to text generation tools intended for a non-specialist audience, discussing the types of assessments that students can outsource, showing the type of prompts that can be used to generate text, and illustrating one possible watermarking technique that may allow generated text to be detected. A small-scale study into watermarking suggests that this technique is feasible and show technical promise but should not be relied on as a solution to widespread use of artificial intelligence based tools by students. Alternative solutions are needed, including encouraging the educational community to work with artificial intelligence rather than against it. As such, the paper concludes by discussing seven potential areas for further exploration.",
    "venue":"International Journal for Educational Integrity",
    "year":2023,
    "publication_date":"2023-06-05",
    "citation_count":72,
    "reference_count":16,
    "fields_of_study":[

    ],
    "external_ids":{
      "DOI":"10.1007\/s40979-023-00131-6",
      "CorpusId":259066129
    }
  },
  {
    "paper_id":"d238a9770d24d0725656ef6cf4789afebf2126e7",
    "title":"TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
    "authors":[
      "Dongfu Jiang",
      "Yishan Li",
      "Ge Zhang",
      "Wenhao Huang",
      "Bill Yuchen Lin",
      "Wenhu Chen"
    ],
    "abstract":"We present TIGERScore, a \\textbf{T}rained metric that follows \\textbf{I}nstruction \\textbf{G}uidance to perform \\textbf{E}xplainable, and \\textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output $\\rightarrow$ error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task. All the resourced are released in our project website: \\url{https:\/\/tiger-ai-lab.github.io\/TIGERScore\/}.",
    "venue":"Trans. Mach. Learn. Res.",
    "year":2023,
    "publication_date":"2023-10-01",
    "citation_count":80,
    "reference_count":54,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2310.00752",
      "DBLP":"journals\/tmlr\/JiangLZHLC24",
      "DOI":"10.48550\/arXiv.2310.00752",
      "CorpusId":263334281
    }
  },
  {
    "paper_id":"e5d0857feca845b474b89565d513ff599629851d",
    "title":"Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    "authors":[
      "H. Deng",
      "Colin Raffel"
    ],
    "abstract":"While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2023,
    "publication_date":"2023-10-14",
    "citation_count":68,
    "reference_count":34,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2310.09520",
      "DBLP":"journals\/corr\/abs-2310-09520",
      "DOI":"10.18653\/v1\/2023.emnlp-main.721",
      "CorpusId":264146589
    }
  },
  {
    "paper_id":"c1ffa773f29e567e13b00495767f8c42e2bb44b2",
    "title":"A Contrastive Framework for Neural Text Generation",
    "authors":[
      "Yixuan Su",
      "Tian Lan",
      "Yan Wang",
      "Dani Yogatama",
      "Lingpeng Kong",
      "Nigel Collier"
    ],
    "abstract":"Text generation is of great importance to many natural language processing applications. However, maximization-based decoding methods (e.g. beam search) of neural language models often lead to degenerate solutions -- the generated text is unnatural and contains undesirable repetitions. Existing approaches introduce stochasticity via sampling or modify training objectives to decrease probabilities of certain tokens (e.g., unlikelihood training). However, they often lead to solutions that lack coherence. In this work, we show that an underlying reason for model degeneration is the anisotropic distribution of token representations. We present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model's representation space, and (ii) a decoding method -- contrastive search -- to encourage diversity while maintaining coherence in the generated text. Extensive experiments and analyses on three benchmarks from two languages demonstrate that our proposed approach significantly outperforms current state-of-the-art text generation methods as evaluated by both human and automatic metrics.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-02-13",
    "citation_count":283,
    "reference_count":92,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/SuLWYKC22",
      "ArXiv":"2202.06417",
      "CorpusId":246823043
    }
  },
  {
    "paper_id":"c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
    "title":"ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation",
    "authors":[
      "Zhengyi Wang",
      "Cheng Lu",
      "Yikai Wang",
      "Fan Bao",
      "Chongxuan Li",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract":"Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https:\/\/ml.cs.tsinghua.edu.cn\/prolificdreamer\/",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-25",
    "citation_count":1081,
    "reference_count":57,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/nips\/Wang00BL0023",
      "ArXiv":"2305.16213",
      "DOI":"10.48550\/arXiv.2305.16213",
      "CorpusId":258887357
    }
  },
  {
    "paper_id":"18b75ea107ed166d7120c12c162b94f02e20b417",
    "title":"COLLIE: Systematic Construction of Constrained Text Generation Tasks",
    "authors":[
      "Shunyu Yao",
      "Howard Chen",
      "Austin W. Hanjie",
      "Runzhe Yang",
      "Karthik Narasimhan"
    ],
    "abstract":"Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-07-17",
    "citation_count":51,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2307.08689",
      "DBLP":"journals\/corr\/abs-2307-08689",
      "DOI":"10.48550\/arXiv.2307.08689",
      "CorpusId":259936996
    }
  },
  {
    "paper_id":"75ddc4fb91332f95222d74449d96b9f7c8f976c7",
    "title":"Nationality Bias in Text Generation",
    "authors":[
      "Pranav Narayanan Venkit",
      "Sanjana Gautam",
      "Ruchi Panchanadikar",
      "Ting-Hao 'Kenneth' Huang",
      "Shomir Wilson"
    ],
    "abstract":"Little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social NLP models. This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms. We generate stories using GPT-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country’s economic status impacts the sentiment of the stories. To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering. Our results show that GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same.",
    "venue":"Conference of the European Chapter of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-02-05",
    "citation_count":73,
    "reference_count":33,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-02463",
      "ArXiv":"2302.02463",
      "ACL":"2023.eacl-main.9",
      "DOI":"10.48550\/arXiv.2302.02463",
      "CorpusId":256616034
    }
  },
  {
    "paper_id":"0119a57cf88ef16e6dc291252fae340bb6b3953c",
    "title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
    "authors":[
      "Bill Yuchen Lin",
      "Ming Shen",
      "Wangchunshu Zhou",
      "Pei Zhou",
      "Chandra Bhagavatula",
      "Yejin Choi",
      "Xiang Ren"
    ],
    "abstract":"Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man throws a frisbee and his dog catches it”). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6% v.s. 63.5% in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9% to 78.4 in dev accuracy) by generating additional context.",
    "venue":"Findings",
    "year":2020,
    "publication_date":"2020-02-14",
    "citation_count":383,
    "reference_count":95,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3023710830",
      "DBLP":"conf\/emnlp\/LinZSZBCR20",
      "ACL":"2020.findings-emnlp.165",
      "DOI":"10.18653\/v1\/2020.findings-emnlp.165",
      "CorpusId":218500588
    }
  },
  {
    "paper_id":"13b8060acc3db1fc555f6e55368f6d02899a1698",
    "title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation",
    "authors":[
      "Eve Fleisig",
      "Aubrie Amstutz",
      "Chad Atalla",
      "Su Lin Blodgett",
      "Hal Daumé",
      "Alexandra Olteanu",
      "Emily Sheng",
      "Dan Vann",
      "Hanna M. Wallach"
    ],
    "abstract":"It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer specification of dataset coverage, and accounting for annotator disagreement and harms that are context-dependent. FairPrism’s annotations include the extent of stereotyping and demeaning harms, the demographic groups targeted, and appropriateness for different applications. The annotations also include specific harms that occur in interactive contexts and harms that raise normative concerns when the “speaker” is an AI system. Due to its precision and granularity, FairPrism can be used to diagnose (1) the types of fairness-related harms that AI text generation systems cause, and (2) the potential limitations of mitigation methods, both of which we illustrate through case studies. Finally, the process we followed to develop FairPrism offers a recipe for building improved datasets for measuring and mitigating harms caused by AI systems.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":null,
    "citation_count":34,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/acl\/FleisigAABDOSVW23",
      "ACL":"2023.acl-long.343",
      "DOI":"10.18653\/v1\/2023.acl-long.343",
      "CorpusId":259092939
    }
  },
  {
    "paper_id":"47fd2e73a9be04ed2186c03d8f88d5c87f64e4e4",
    "title":"ToTTo: A Controlled Table-To-Text Generation Dataset",
    "authors":[
      "Ankur P. Parikh",
      "Xuezhi Wang",
      "Sebastian Gehrmann",
      "Manaal Faruqui",
      "Bhuwan Dhingra",
      "Diyi Yang",
      "Dipanjan Das"
    ],
    "abstract":"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2020,
    "publication_date":"2020-04-29",
    "citation_count":408,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"3022814719",
      "DBLP":"journals\/corr\/abs-2004-14373",
      "ArXiv":"2004.14373",
      "ACL":"2020.emnlp-main.89",
      "DOI":"10.18653\/v1\/2020.emnlp-main.89",
      "CorpusId":216641852
    }
  },
  {
    "paper_id":"023edab4738690444e3924e224c2641017a0d794",
    "title":"Quark: Controllable Text Generation with Reinforced Unlearning",
    "authors":[
      "Ximing Lu",
      "S. Welleck",
      "Liwei Jiang",
      "Jack Hessel",
      "Lianhui Qin",
      "Peter West",
      "Prithviraj Ammanabrolu",
      "Yejin Choi"
    ],
    "abstract":"Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-05-26",
    "citation_count":251,
    "reference_count":99,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2205.13636",
      "DBLP":"journals\/corr\/abs-2205-13636",
      "DOI":"10.48550\/arXiv.2205.13636",
      "CorpusId":249152301
    }
  },
  {
    "paper_id":"912df62f362eb0d42a3393898818196d6a6fd57a",
    "title":"Controlled Text Generation via Language Model Arithmetic",
    "authors":[
      "Jasper Dekoninck",
      "Marc Fischer",
      "Luca Beurer-Kellner",
      "Martin T. Vechev"
    ],
    "abstract":"As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https:\/\/github.com\/eth-sri\/language-model-arithmetic.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-11-24",
    "citation_count":51,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2311-14479",
      "ArXiv":"2311.14479",
      "DOI":"10.48550\/arXiv.2311.14479",
      "CorpusId":265445161
    }
  },
  {
    "paper_id":"6b5fc164c4f21e4a4f151df60bfd5e32b061a903",
    "title":"InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation",
    "authors":[
      "Haofan Wang",
      "Matteo Spinelli",
      "Qixun Wang",
      "Xu Bai",
      "Zekui Qin",
      "Anthony Chen"
    ],
    "abstract":"Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at https:\/\/github.com\/InstantStyle\/InstantStyle.",
    "venue":"arXiv.org",
    "year":2024,
    "publication_date":"2024-04-03",
    "citation_count":140,
    "reference_count":26,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2404-02733",
      "ArXiv":"2404.02733",
      "DOI":"10.48550\/arXiv.2404.02733",
      "CorpusId":268876474
    }
  },
  {
    "paper_id":"ca18c18ddd730e4d690431ad6c65035d0f41aed6",
    "title":"Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
    "authors":[
      "Yu Fu",
      "Deyi Xiong",
      "Yue Dong"
    ],
    "abstract":"To mitigate potential risks associated with language models (LMs), recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. In this paper, we show that watermarking algorithms designed for LMs cannot be seamlessly applied to conditional text generation (CTG) tasks without a notable decline in downstream task performance. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation with the input context. Compared to the baseline watermarks, our proposed watermark yields significant improvements in both automatic and human evaluations across various text generation models, including BART and Flan-T5, for CTG tasks such as summarization and data-to-text generation. Meanwhile, it maintains detection ability with higher z-scores but lower AUC scores, suggesting the presence of a detection paradox that poses additional challenges for watermarking CTG.",
    "venue":"AAAI Conference on Artificial Intelligence",
    "year":2023,
    "publication_date":"2023-07-25",
    "citation_count":50,
    "reference_count":48,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/aaai\/FuXD24",
      "ArXiv":"2307.13808",
      "DOI":"10.48550\/arXiv.2307.13808",
      "CorpusId":260164516
    }
  },
  {
    "paper_id":"5fa10872ef8037853ff7c8baf5f77fb55a918eca",
    "title":"Diffusion Models for Non-autoregressive Text Generation: A Survey",
    "authors":[
      "Yifan Li",
      "Kun Zhou",
      "Wayne Xin Zhao",
      "Ji-rong Wen"
    ],
    "abstract":"Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude this paper. Our survey aims to provide researchers with a systematic reference of related research on text diffusion models for NAR generation. We also demonstrate our collection of text diffusion models at https:\/\/github.com\/RUCAIBox\/Awesome-Text-Diffusion-Models.",
    "venue":"International Joint Conference on Artificial Intelligence",
    "year":2023,
    "publication_date":"2023-03-12",
    "citation_count":48,
    "reference_count":63,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2303-06574",
      "ArXiv":"2303.06574",
      "DOI":"10.48550\/arXiv.2303.06574",
      "CorpusId":257496277
    }
  },
  {
    "paper_id":"168d3bfab5cf0ed356c51eb6eaa18654d575a419",
    "title":"Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning",
    "authors":[
      "Chujie Zheng",
      "Pei Ke",
      "Zheng Zhang",
      "Minlie Huang"
    ],
    "abstract":"It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Click outperforms strong baselines of controllable text generation and demonstrate the superiority of Click's sample construction strategy.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2023,
    "publication_date":"2023-06-06",
    "citation_count":43,
    "reference_count":43,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2306.03350",
      "DBLP":"conf\/acl\/ZhengK0H23",
      "DOI":"10.48550\/arXiv.2306.03350",
      "CorpusId":259088837
    }
  },
  {
    "paper_id":"c3b0da01017870729a7a83b94e7787e5105cbc32",
    "title":"Learning to Rewrite Prompts for Personalized Text Generation",
    "authors":[
      "Cheng Li",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Weize Kong",
      "Michael Bendersky"
    ],
    "abstract":"Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.",
    "venue":"The Web Conference",
    "year":2023,
    "publication_date":"2023-09-29",
    "citation_count":39,
    "reference_count":34,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/www\/00120MKB24",
      "ArXiv":"2310.00152",
      "DOI":"10.1145\/3589334.3645408",
      "CorpusId":263333908
    }
  },
  {
    "paper_id":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
    "title":"Make-A-Video: Text-to-Video Generation without Text-Video Data",
    "authors":[
      "Uriel Singer",
      "Adam Polyak",
      "Thomas Hayes",
      "Xiaoyue Yin",
      "Jie An",
      "Songyang Zhang",
      "Qiyuan Hu",
      "Harry Yang",
      "Oron Ashual",
      "Oran Gafni",
      "Devi Parikh",
      "Sonal Gupta",
      "Yaniv Taigman"
    ],
    "abstract":"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",
    "venue":"International Conference on Learning Representations",
    "year":2022,
    "publication_date":"2022-09-29",
    "citation_count":1701,
    "reference_count":51,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/SingerPH00ZHYAG23",
      "ArXiv":"2209.14792",
      "CorpusId":252595919
    }
  },
  {
    "paper_id":"86be5c90c4128ec59b1c320a16996bb5de68624e",
    "title":"Texygen: A Benchmarking Platform for Text Generation Models",
    "authors":[
      "Yaoming Zhu",
      "Sidi Lu",
      "Lei Zheng",
      "Jiaxian Guo",
      "Weinan Zhang",
      "Jun Wang",
      "Yong Yu"
    ],
    "abstract":"We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.",
    "venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year":2018,
    "publication_date":"2018-02-06",
    "citation_count":764,
    "reference_count":17,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "MAG":"2785896739",
      "DBLP":"conf\/sigir\/ZhuLZGZWY18",
      "ArXiv":"1802.01886",
      "DOI":"10.1145\/3209978.3210080",
      "CorpusId":3636178
    }
  },
  {
    "paper_id":"53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
    "title":"Neural Text Generation with Unlikelihood Training",
    "authors":[
      "S. Welleck",
      "Ilia Kulikov",
      "Stephen Roller",
      "Emily Dinan",
      "Kyunghyun Cho",
      "J. Weston"
    ],
    "abstract":"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",
    "venue":"International Conference on Learning Representations",
    "year":2019,
    "publication_date":"2019-08-12",
    "citation_count":636,
    "reference_count":37,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "MAG":"2968297680",
      "DBLP":"conf\/iclr\/WelleckKRDCW20",
      "ArXiv":"1908.04319",
      "CorpusId":199551982
    }
  },
  {
    "paper_id":"635cb6fb865e86c108c5d1d895aeac0e759eb199",
    "title":"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "authors":[
      "Wei Zhao",
      "Maxime Peyrard",
      "Fei Liu",
      "Yang Gao",
      "Christian M. Meyer",
      "Steffen Eger"
    ],
    "abstract":"A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2019,
    "publication_date":"2019-08-14",
    "citation_count":639,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"1909.02622",
      "DBLP":"journals\/corr\/abs-1909-02622",
      "MAG":"2971753232",
      "ACL":"D19-1053",
      "DOI":"10.18653\/v1\/D19-1053",
      "CorpusId":202540033
    }
  },
  {
    "paper_id":"e6770e3f5e74210c6863aaeed527ac4c1da419d7",
    "title":"A Survey on Retrieval-Augmented Text Generation",
    "authors":[
      "Huayang Li",
      "Yixuan Su",
      "Deng Cai",
      "Yan Wang",
      "Lemao Liu"
    ],
    "abstract":"Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-02-02",
    "citation_count":251,
    "reference_count":85,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2202-01110",
      "ArXiv":"2202.01110",
      "CorpusId":246472929
    }
  },
  {
    "paper_id":"f12a6168ed8de1aee69fee51b469b1aecd5f903e",
    "title":"Factuality Enhanced Language Models for Open-Ended Text Generation",
    "authors":[
      "Nayeon Lee",
      "Wei Ping",
      "Peng Xu",
      "M. Patwary",
      "M. Shoeybi",
      "Bryan Catanzaro"
    ],
    "abstract":"Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation. We design the FactualityPrompts test set and metrics to measure the factuality of LM generations. Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions. In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step. We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors. We release our code and FactualityPrompts benchmark at: https:\/\/github.com\/nayeon7lee\/FactualityPrompt.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-06-09",
    "citation_count":267,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2206.04624",
      "DBLP":"conf\/nips\/LeePXPFSC22",
      "DOI":"10.48550\/arXiv.2206.04624",
      "CorpusId":249538460
    }
  },
  {
    "paper_id":"be8e58320203a92bfacc1a1f95f6e65f3ee4fa5c",
    "title":"A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
    "authors":[
      "Hanqing Zhang",
      "Haolin Song",
      "Shaoyu Li",
      "Ming Zhou",
      "Dawei Song"
    ],
    "abstract":"Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
    "venue":"ACM Computing Surveys",
    "year":2022,
    "publication_date":"2022-01-14",
    "citation_count":276,
    "reference_count":219,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/csur\/ZhangSLZS24",
      "ArXiv":"2201.05337",
      "DOI":"10.1145\/3617680",
      "CorpusId":245986550
    }
  },
  {
    "paper_id":"39ba6d541d94132b816938e7e16b1e8fd49c2fd9",
    "title":"Training-Free Consistent Text-to-Image Generation",
    "authors":[
      "Yoad Tewel",
      "Omri Kaduri",
      "Rinon Gal",
      "Yoni Kasten",
      "Lior Wolf",
      "Gal Chechik",
      "Y. Atzmon"
    ],
    "abstract":"Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
    "venue":"ACM Transactions on Graphics",
    "year":2024,
    "publication_date":"2024-02-05",
    "citation_count":99,
    "reference_count":55,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2402.03286",
      "DBLP":"journals\/corr\/abs-2402-03286",
      "DOI":"10.1145\/3658157",
      "CorpusId":267412997
    }
  },
  {
    "paper_id":"0c181f508ec9de8e48f62523ba8a9bcb1f51f83a",
    "title":"Pre-Trained Language Models for Text Generation: A Survey",
    "authors":[
      "Junyi Li",
      "Tianyi Tang",
      "Wayne Xin Zhao",
      "J. Nie",
      "Ji-rong Wen"
    ],
    "abstract":"Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (PLMs). Text generation based on PLMs is viewed as a promising approach in both academia and industry. In this article, we provide a survey on the utilization of PLMs in text generation. We begin with introducing two key aspects of applying PLMs to text generation: (1) how to design an effective PLM to serve as the generation model; and (2) how to effectively optimize PLMs given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges that have arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on PLMs. Finally, we highlight the future research directions which will further improve these PLMs for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on PLMs.",
    "venue":"ACM Computing Surveys",
    "year":2022,
    "publication_date":"2022-01-14",
    "citation_count":232,
    "reference_count":324,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2201.05273",
      "DBLP":"journals\/csur\/LiTZNW24",
      "DOI":"10.1145\/3649449",
      "CorpusId":260435365
    }
  },
  {
    "paper_id":"994a1ce6677b496bd3c0c63aceafc6556005e994",
    "title":"GLIGEN: Open-Set Grounded Text-to-Image Generation",
    "authors":[
      "Yuheng Li",
      "Haotian Liu",
      "Qingyang Wu",
      "Fangzhou Mu",
      "Jianwei Yang",
      "Jianfeng Gao",
      "Chunyuan Li",
      "Yong Jae Lee"
    ],
    "abstract":"Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2023,
    "publication_date":"2023-01-17",
    "citation_count":750,
    "reference_count":80,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/cvpr\/LiLWMYGLL23",
      "ArXiv":"2301.07093",
      "DOI":"10.1109\/CVPR52729.2023.02156",
      "CorpusId":255942528
    }
  },
  {
    "paper_id":"e23cb51f50f749320b9122fb5f75113b4d192c0a",
    "title":"Evaluation of Text Generation: A Survey",
    "authors":[
      "Asli Celikyilmaz",
      "Elizabeth Clark",
      "Jianfeng Gao"
    ],
    "abstract":"The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models. We then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.",
    "venue":"arXiv.org",
    "year":2020,
    "publication_date":"2020-06-26",
    "citation_count":411,
    "reference_count":329,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2006-14799",
      "ArXiv":"2006.14799",
      "MAG":"3037013468",
      "CorpusId":220128348
    }
  },
  {
    "paper_id":"cf694df964caa156ec306b45d3a3127533cb458f",
    "title":"Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation",
    "authors":[
      "Yuval Kirstain",
      "Adam Polyak",
      "Uriel Singer",
      "Shahbuland Matiana",
      "Joe Penna",
      "Omer Levy"
    ],
    "abstract":"The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-02",
    "citation_count":605,
    "reference_count":20,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2305.01569",
      "DBLP":"journals\/corr\/abs-2305-01569",
      "DOI":"10.48550\/arXiv.2305.01569",
      "CorpusId":258437096
    }
  },
  {
    "paper_id":"1b2355c3c674b26a977768a91a164384ad51bbb1",
    "title":"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
    "authors":[
      "Jiazheng Xu",
      "Xiao Liu",
      "Yuchen Wu",
      "Yuxuan Tong",
      "Qinkai Li",
      "Ming Ding",
      "Jie Tang",
      "Yuxiao Dong"
    ],
    "abstract":"We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at \\url{https:\/\/github.com\/THUDM\/ImageReward}.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-04-12",
    "citation_count":667,
    "reference_count":70,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2304.05977",
      "DBLP":"conf\/nips\/XuLWTLDTD23",
      "DOI":"10.48550\/arXiv.2304.05977",
      "CorpusId":258079316
    }
  },
  {
    "paper_id":"21e3937cceb3320bdcdd4727b9e2b83b6529de8e",
    "title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation",
    "authors":[
      "Kepan Nan",
      "Rui Xie",
      "Penghao Zhou",
      "Tiehan Fan",
      "Zhenheng Yang",
      "Zhijie Chen",
      "Xiang Li",
      "Jian Yang",
      "Ying Tai"
    ],
    "abstract":"Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V generation. Furthermore, we curate 433K 1080p videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition video generation. Additionally, we propose a novel Multi-modal Video Diffusion Transformer (MVDiT) capable of mining both structure information from visual tokens and semantic information from text tokens. Extensive experiments and ablation studies verify the superiority of OpenVid-1M over previous datasets and the effectiveness of our MVDiT.",
    "venue":"International Conference on Learning Representations",
    "year":2024,
    "publication_date":"2024-07-02",
    "citation_count":156,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2407-02371",
      "ArXiv":"2407.02371",
      "DOI":"10.48550\/arXiv.2407.02371",
      "CorpusId":270877903
    }
  },
  {
    "paper_id":"4fff661078543f6ffb9fe2c0c04829a877f5cfa2",
    "title":"Next-Generation Database Interfaces: A Survey of LLM-Based Text-to-SQL",
    "authors":[
      "Zijin Hong",
      "Zheng Yuan",
      "Qinggang Zhang",
      "Hao Chen",
      "Junnan Dong",
      "Feiran Huang",
      "Xiao Huang"
    ],
    "abstract":"Generating accurate SQL from users’ natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restrict the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summary and discuss the remaining challenges in this field and suggest expectations for future research directions.",
    "venue":"IEEE Transactions on Knowledge and Data Engineering",
    "year":2024,
    "publication_date":"2024-06-12",
    "citation_count":125,
    "reference_count":193,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2406-08426",
      "ArXiv":"2406.08426",
      "DOI":"10.1109\/TKDE.2025.3609486",
      "CorpusId":270391628
    }
  },
  {
    "paper_id":"21a77ed349c8621d0a0ef8407eb744e3de3b13c5",
    "title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text",
    "authors":[
      "Roberto Henschel",
      "Levon Khachatryan",
      "Daniil Hayrapetyan",
      "Hayk Poghosyan",
      "Vahram Tadevosyan",
      "Zhangyang Wang",
      "Shant Navasardyan",
      "Humphrey Shi"
    ],
    "abstract":"Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, simplifying the process of producing diverse and individual content. Current methods excel in generating short videos (up to 16s), but produce hard-cuts when naively extended to long video synthesis. To overcome these limitations, we present StreamingT2V, an autoregressive method that generates long videos of up to 2 minutes or longer with seamless transitions. The key components are: (i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the preceding chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a longterm memory block called appearance preservation module (APM), which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that allows for the autoregressive application of a video enhancer on videos of indefinite length, ensuring consistency across chunks. Experiments show that StreamingT2V produces more motion, while competing methods suffer from video stagnation when applied naively in an autoregressive fashion. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator, surpassing competitors in both consistency and motion.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2024,
    "publication_date":"2024-03-21",
    "citation_count":128,
    "reference_count":75,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2403-14773",
      "ArXiv":"2403.14773",
      "DOI":"10.1109\/CVPR52734.2025.00245",
      "CorpusId":268667009
    }
  },
  {
    "paper_id":"395de0bd3837fdf4b4b5e5f04835bcc69c279481",
    "title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    "authors":[
      "M. Lewis",
      "Yinhan Liu",
      "Naman Goyal",
      "Marjan Ghazvininejad",
      "Abdel-rahman Mohamed",
      "Omer Levy",
      "Veselin Stoyanov",
      "Luke Zettlemoyer"
    ],
    "abstract":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2019,
    "publication_date":"2019-10-29",
    "citation_count":11756,
    "reference_count":36,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "MAG":"2982399380",
      "ACL":"2020.acl-main.703",
      "DBLP":"journals\/corr\/abs-1910-13461",
      "ArXiv":"1910.13461",
      "DOI":"10.18653\/v1\/2020.acl-main.703",
      "CorpusId":204960716
    }
  },
  {
    "paper_id":"4a6a65968a8eb8c09ffb57a7774ddabb596565b1",
    "title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics",
    "authors":[
      "Lianhui Qin",
      "S. Welleck",
      "Daniel Khashabi",
      "Yejin Choi"
    ],
    "abstract":"Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-02-23",
    "citation_count":172,
    "reference_count":76,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2202-11705",
      "ArXiv":"2202.11705",
      "CorpusId":247058662
    }
  },
  {
    "paper_id":"3f19484f941f209f45a51b1b69160e24e9b9dc99",
    "title":"GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
    "authors":[
      "Tong Wu",
      "Guandao Yang",
      "Zhibing Li",
      "Kai Zhang",
      "Ziwei Liu",
      "Leonidas J. Guibas",
      "Dahua Lin",
      "Gordon Wetzstein"
    ],
    "abstract":"Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. These metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. Conducting user preference studies is an alternative that offers both adaptability and human-aligned results. User studies, however, can be very ex-pensive to scale. This paper presents an automatic, ver-satile, and human-aligned evaluation metric for text-to-3D generative models. To this end, we first develop a prompt generator using GPT-4V to generate evaluating prompts, which serve as input to compare text-to-3D models. We further design a method instructing GPT-4V to compare two 3D assets according to user-defined crite-ria. Finally, we use these pairwise comparison results to assign these models Elo ratings. Experimental results suggest our metric strongly aligns with human preference across different evaluation criteria. Our code is available at https:\/\/github.com\/3DTopia\/GPTEval3D.",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2024,
    "publication_date":"2024-01-08",
    "citation_count":126,
    "reference_count":73,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2401.04092",
      "DBLP":"conf\/cvpr\/WuYLZLGLW24",
      "DOI":"10.1109\/CVPR52733.2024.02098",
      "CorpusId":266844463
    }
  },
  {
    "paper_id":"0b9770a377b3f96cef9f268cee1791d39a0d4893",
    "title":"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
    "authors":[
      "Xiaochuang Han",
      "Sachin Kumar",
      "Yulia Tsvetkov"
    ],
    "abstract":"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-10-31",
    "citation_count":128,
    "reference_count":98,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2210-17432",
      "ACL":"2023.acl-long.647",
      "ArXiv":"2210.17432",
      "DOI":"10.48550\/arXiv.2210.17432",
      "CorpusId":253237701
    }
  },
  {
    "paper_id":"fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b",
    "title":"AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
    "authors":[
      "Haohe Liu",
      "Zehua Chen",
      "Yiitan Yuan",
      "Xinhao Mei",
      "Xubo Liu",
      "Danilo P. Mandic",
      "Wenwu Wang",
      "Mark D. Plumbley"
    ],
    "abstract":"Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https:\/\/audioldm.github.io.",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-01-29",
    "citation_count":642,
    "reference_count":65,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "ArXiv":"2301.12503",
      "DBLP":"journals\/corr\/abs-2301-12503",
      "DOI":"10.48550\/arXiv.2301.12503",
      "CorpusId":256390486
    }
  },
  {
    "paper_id":"dc0c132b273456b288a785414db2fa72edf87b1a",
    "title":"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing",
    "authors":[
      "Dongxu Li",
      "Junnan Li",
      "Steven C. H. Hoi"
    ],
    "abstract":"Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Code and models will be released at https:\/\/github.com\/salesforce\/LAVIS\/tree\/main\/projects\/blip-diffusion. Project page at https:\/\/dxli94.github.io\/BLIP-Diffusion-website\/.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-05-24",
    "citation_count":429,
    "reference_count":40,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2305-14720",
      "ArXiv":"2305.14720",
      "DOI":"10.48550\/arXiv.2305.14720",
      "CorpusId":258865473
    }
  },
  {
    "paper_id":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe",
    "title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
    "authors":[
      "Jiahui Yu",
      "Yuanzhong Xu",
      "Jing Yu Koh",
      "Thang Luong",
      "Gunjan Baid",
      "Zirui Wang",
      "Vijay Vasudevan",
      "Alexander Ku",
      "Yinfei Yang",
      "Burcu Karagol Ayan",
      "Ben Hutchinson",
      "Wei Han",
      "Zarana Parekh",
      "Xin Li",
      "Han Zhang",
      "Jason Baldridge",
      "Yonghui Wu"
    ],
    "abstract":"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https:\/\/parti.research.google\/ for high-resolution images.",
    "venue":"Trans. Mach. Learn. Res.",
    "year":2022,
    "publication_date":"2022-06-22",
    "citation_count":1303,
    "reference_count":115,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/tmlr\/YuXKLBWVKYAHHPLZBW22",
      "ArXiv":"2206.10789",
      "DOI":"10.48550\/arXiv.2206.10789",
      "CorpusId":249926846
    }
  },
  {
    "paper_id":"2a3213cb3c755f036d5dfec7261d726a819c78c1",
    "title":"Muse: Text-To-Image Generation via Masked Generative Transformers",
    "authors":[
      "Huiwen Chang",
      "Han Zhang",
      "Jarred Barber",
      "AJ Maschinot",
      "José Lezama",
      "Lu Jiang",
      "Ming Yang",
      "K. Murphy",
      "W. Freeman",
      "Michael Rubinstein",
      "Yuanzhen Li",
      "Dilip Krishnan"
    ],
    "abstract":"We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https:\/\/muse-model.github.io",
    "venue":"International Conference on Machine Learning",
    "year":2023,
    "publication_date":"2023-01-02",
    "citation_count":657,
    "reference_count":87,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icml\/ChangZBML00MFRL23",
      "ArXiv":"2301.00704",
      "DOI":"10.48550\/arXiv.2301.00704",
      "CorpusId":255372955
    }
  },
  {
    "paper_id":"e15900cf7c93d4b6e45a12fe3534840c910467e1",
    "title":"ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation",
    "authors":[
      "Yuxiang Wei",
      "Yabo Zhang",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Lei Zhang",
      "W. Zuo"
    ],
    "abstract":"In addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. In this paper, we instead propose a learning-based encoder, which consists of a global and a local mapping networks for fast and accurate customized text-to-image generation. In specific, the global mapping network projects the hierarchical features of a given image into multiple \"new\" words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with existing optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables high-fidelity inversion and more robust editability with a significantly faster encoding process. Our code is publicly available at https:\/\/github.com\/csyxwei\/ELITE.",
    "venue":"IEEE International Conference on Computer Vision",
    "year":2023,
    "publication_date":"2023-02-27",
    "citation_count":403,
    "reference_count":45,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2302-13848",
      "ArXiv":"2302.13848",
      "DOI":"10.1109\/ICCV51070.2023.01461",
      "CorpusId":257219968
    }
  },
  {
    "paper_id":"2c6ac935c826002976722ca8d3319f691975687e",
    "title":"Self-conditioned Embedding Diffusion for Text Generation",
    "authors":[
      "Robin Strudel",
      "Corentin Tallec",
      "Florent Altch'e",
      "Yilun Du",
      "Yaroslav Ganin",
      "A. Mensch",
      "Will Grathwohl",
      "Nikolay Savinov",
      "S. Dieleman",
      "L. Sifre",
      "Rémi Leblond"
    ],
    "abstract":"Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-11-08",
    "citation_count":100,
    "reference_count":35,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2211.04236",
      "DBLP":"journals\/corr\/abs-2211-04236",
      "DOI":"10.48550\/arXiv.2211.04236",
      "CorpusId":253397773
    }
  },
  {
    "paper_id":"86891d00499eebe86d3f1e39143d412addf2652b",
    "title":"DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation",
    "authors":[
      "Seongmin Hong",
      "Seungjae Moon",
      "Junsoo Kim",
      "Sungjae Lee",
      "Minsub Kim",
      "Dongsoo Lee",
      "Joo-Young Kim"
    ],
    "abstract":"Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pretrained Transformer (GPT) has achieved remarkable performance in text generation, or natural language generation (NLG), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. The conventional platforms such as GPU are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. Therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation. In this paper, we present DFX, a multi-FPGA acceleration appliance that executes GPT-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. DFX uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. Its compute cores operate on custom instructions and provide GPT-2 operations end-to-end. We implement the proposed hardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the channels of the high bandwidth memory (HBM) and the maximum number of compute resources for high hardware efficiency. DFX achieves 5.58$\\times$ speedup and 3.99$\\times$ energy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is also 8.21$\\times$ more cost-effective than the GPU appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.",
    "venue":"Micro",
    "year":2022,
    "publication_date":"2022-08-21",
    "citation_count":105,
    "reference_count":53,
    "fields_of_study":[
      "Engineering",
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/micro\/HongMKLKLK22",
      "ArXiv":"2209.10797",
      "DOI":"10.1109\/MICRO56248.2022.00051",
      "CorpusId":252439113
    }
  },
  {
    "paper_id":"05bcf9999525656cfaa59bc71f8572d771ff3776",
    "title":"Language Models Can See: Plugging Visual Controls in Text Generation",
    "authors":[
      "Yixuan Su",
      "Tian Lan",
      "Yahui Liu",
      "Fangyu Liu",
      "Dani Yogatama",
      "Yan Wang",
      "Lingpeng Kong",
      "Nigel Collier"
    ],
    "abstract":"Generative language models (LMs) such as GPT-2\/3 can be prompted to generate text with remarkable quality. While they are designed for text-prompted generation, it remains an open question how the generation process could be guided by modalities beyond text such as images. In this work, we propose a training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP), for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC is a simple yet efficient plug-and-play framework, which directly combines an off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP) for image-grounded text generation. During decoding, MAGIC influences the generation of the LM by introducing a CLIP-induced score, called magic score, which regularizes the generated result to be semantically related to a given image while being coherent to the previously generated context. Notably, the proposed decoding scheme does not involve any gradient update operation, therefore being computationally efficient. On the challenging task of zero-shot image captioning, MAGIC outperforms the state-of-the-art method by notable margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework and is theoretically compatible with any text generation tasks that incorporate image grounding. In the experiments, we showcase that it is also capable of performing visually grounded story generation given both an image and a text prompt.",
    "venue":"arXiv.org",
    "year":2022,
    "publication_date":"2022-05-05",
    "citation_count":110,
    "reference_count":95,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2205.02655",
      "DBLP":"journals\/corr\/abs-2205-02655",
      "DOI":"10.48550\/arXiv.2205.02655",
      "CorpusId":248525064
    }
  },
  {
    "paper_id":"a9e00c216ce69325a15fd139da0624978e54058a",
    "title":"Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
    "authors":[
      "Matt Le",
      "Apoorv Vyas",
      "Bowen Shi",
      "B. Karrer",
      "Leda Sari",
      "Rashel Moritz",
      "Mary Williamson",
      "Vimal Manohar",
      "Yossi Adi",
      "Jay Mahadeokar",
      "Wei-Ning Hsu"
    ],
    "abstract":"Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https:\/\/voicebox.metademolab.com}.",
    "venue":"Neural Information Processing Systems",
    "year":2023,
    "publication_date":"2023-06-23",
    "citation_count":402,
    "reference_count":82,
    "fields_of_study":[
      "Computer Science",
      "Engineering"
    ],
    "external_ids":{
      "ArXiv":"2306.15687",
      "DBLP":"journals\/corr\/abs-2306-15687",
      "DOI":"10.48550\/arXiv.2306.15687",
      "CorpusId":259275061
    }
  },
  {
    "paper_id":"6e3f8187f8fef3e11578a73f32da07d33dbf8235",
    "title":"DART: Open-Domain Structured Data Record to Text Generation",
    "authors":[
      "Dragomir R. Radev",
      "Rui Zhang",
      "Amrit Rau",
      "Abhinand Sivaprasad",
      "Chia-Hsuan Hsieh",
      "Nazneen Rajani",
      "Xiangru Tang",
      "Aadit Vyas",
      "Neha Verma",
      "P. Krishna",
      "Yangxiaokang Liu",
      "Nadia Irwanto",
      "Jessica Pan",
      "Faiaz Rahman",
      "A. Zaidi",
      "Murori Mutuma",
      "Yasin Tarabar",
      "Ankit Gupta",
      "Tao Yu",
      "Y. Tan",
      "Xi Victoria Lin",
      "Caiming Xiong",
      "R. Socher"
    ],
    "abstract":"We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https:\/\/github.com\/Yale-LILY\/dart.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2020,
    "publication_date":"2020-07-06",
    "citation_count":213,
    "reference_count":103,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/naacl\/NanRZRSHTVVKLIP21",
      "MAG":"3170806096",
      "ArXiv":"2007.02871",
      "ACL":"2021.naacl-main.37",
      "DOI":"10.18653\/V1\/2021.NAACL-MAIN.37",
      "CorpusId":220364230
    }
  },
  {
    "paper_id":"735bf29aaf13c9420653e271db37614be55154d7",
    "title":"Recent Advances in Retrieval-Augmented Text Generation",
    "authors":[
      "Deng Cai",
      "Yan Wang",
      "Lemao Liu",
      "Shuming Shi"
    ],
    "abstract":"Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.",
    "venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year":2022,
    "publication_date":"2022-07-06",
    "citation_count":94,
    "reference_count":25,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/sigir\/0002WL022",
      "DOI":"10.1145\/3477495.3532682",
      "CorpusId":250340214
    }
  },
  {
    "paper_id":"e5f9cec0538f01e6e0435892746fa47a8c1068ca",
    "title":"Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries",
    "authors":[
      "Hai Dang",
      "Karim Benharrak",
      "Florian Lehmann",
      "Daniel Buschek"
    ],
    "abstract":"We propose a text editor to help users plan, structure and reflect on their writing process. It provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summarization. Summary levels range from full text, to selected (central) sentences, down to a collection of keywords. To understand how users interact with this system during writing, we conducted two user studies (N=4 and N=8) in which people wrote analytic essays about a given topic and article. As a key finding, the summaries gave users an external perspective on their writing and helped them to revise the content and scope of their drafted paragraphs. People further used the tool to quickly gain an overview of the text and developed strategies to integrate insights from the automated summaries. More broadly, this work explores and highlights the value of designing AI tools for writers, with Natural Language Processing (NLP) capabilities that go beyond direct text generation and correction.",
    "venue":"ACM Symposium on User Interface Software and Technology",
    "year":2022,
    "publication_date":"2022-08-19",
    "citation_count":106,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/uist\/DangBLB22",
      "ArXiv":"2208.09323",
      "DOI":"10.1145\/3526113.3545672",
      "CorpusId":251710484
    }
  },
  {
    "paper_id":"cb648d482dbd1e6ad0b0f4da43aca71c06538d4f",
    "title":"Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
    "authors":[
      "Zheng-Wen Lin",
      "Yeyun Gong",
      "Yelong Shen",
      "Tong Wu",
      "Zhihao Fan",
      "Chen Lin",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "abstract":"In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN\/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https:\/\/github.com\/microsoft\/ProphetNet\/tree\/master\/GENIE.",
    "venue":"International Conference on Machine Learning",
    "year":2022,
    "publication_date":"2022-12-22",
    "citation_count":88,
    "reference_count":47,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/icml\/LinGSWFLDC23",
      "ArXiv":"2212.11685",
      "CorpusId":257019559
    }
  },
  {
    "paper_id":"19af8292ff3cc10aad6f190490f0d34691658179",
    "title":"Investigating Pretrained Language Models for Graph-to-Text Generation",
    "authors":[
      "Leonardo F. R. Ribeiro",
      "Martin Schmitt",
      "Hinrich Schütze",
      "Iryna Gurevych"
    ],
    "abstract":"Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.",
    "venue":"NLP4CONVAI",
    "year":2020,
    "publication_date":"2020-07-16",
    "citation_count":224,
    "reference_count":77,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2021.nlp4convai-1.20",
      "ArXiv":"2007.08426",
      "MAG":"3042876905",
      "DBLP":"journals\/corr\/abs-2007-08426",
      "DOI":"10.18653\/v1\/2021.nlp4convai-1.20",
      "CorpusId":220546439
    }
  },
  {
    "paper_id":"6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
    "title":"Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
    "authors":[
      "Jin Xu",
      "Xiaojiang Liu",
      "Jianhao Yan",
      "Deng Cai",
      "Huayang Li",
      "Jian Li"
    ],
    "abstract":"While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \\textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \\textbf{DITTO} (Pseu\\underline{D}o-Repet\\underline{IT}ion Penaliza\\underline{T}i\\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN\/DailyMail) demonstrate the generality and effectiveness of our method.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-06-06",
    "citation_count":88,
    "reference_count":44,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2206-02369",
      "ArXiv":"2206.02369",
      "DOI":"10.48550\/arXiv.2206.02369",
      "CorpusId":249395390
    }
  },
  {
    "paper_id":"6f709278506813d04a074e6fa20188cce9bb927b",
    "title":"LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching",
    "authors":[
      "Yixun Liang",
      "Xin Yang",
      "Jiantao Lin",
      "Haodong Li",
      "Xiaogang Xu",
      "Yingcong Chen"
    ],
    "abstract":"The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across var-ious real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency. Our code is available at: EnVision-Research\/LucidDreamer",
    "venue":"Computer Vision and Pattern Recognition",
    "year":2023,
    "publication_date":"2023-11-19",
    "citation_count":258,
    "reference_count":50,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2311-11284",
      "ArXiv":"2311.11284",
      "DOI":"10.1109\/CVPR52733.2024.00623",
      "CorpusId":265295106
    }
  },
  {
    "paper_id":"823cacd5255f3897a8d29f29a7c7cb8f978bd928",
    "title":"CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks",
    "authors":[
      "Xuanli He",
      "Qiongkai Xu",
      "Yi Zeng",
      "Lingjuan Lyu",
      "Fangzhao Wu",
      "Jiwei Li",
      "R. Jia"
    ],
    "abstract":"Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, a recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, \\cater can effectively identify the IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-09-19",
    "citation_count":85,
    "reference_count":75,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2209-08773",
      "ArXiv":"2209.08773",
      "DOI":"10.48550\/arXiv.2209.08773",
      "CorpusId":252367820
    }
  },
  {
    "paper_id":"3af134a559a618b3185390646d49d1d4e7ffab45",
    "title":"Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model",
    "authors":[
      "Jiahao Li",
      "Hao Tan",
      "Kai Zhang",
      "Zexiang Xu",
      "Fujun Luan",
      "Yinghao Xu",
      "Yicong Hong",
      "Kalyan Sunkavalli",
      "Greg Shakhnarovich",
      "Sai Bi"
    ],
    "abstract":"Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low-quality results due to the scarcity of 3D training data. In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor. Through extensive experiments, we demonstrate that our method can generate diverse 3D assets of high visual quality within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. Our project webpage: https:\/\/jiahao.ai\/instant3d\/.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-11-10",
    "citation_count":349,
    "reference_count":90,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2311-06214",
      "ArXiv":"2311.06214",
      "DOI":"10.48550\/arXiv.2311.06214",
      "CorpusId":265128529
    }
  },
  {
    "paper_id":"369b449415d50387fba048bbd4d26ee890df84b5",
    "title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
    "authors":[
      "Yi Wang",
      "Yinan He",
      "Yizhuo Li",
      "Kunchang Li",
      "Jiashuo Yu",
      "X. Ma",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Ping Luo",
      "Ziwei Liu",
      "Yali Wang",
      "Limin Wang",
      "Y. Qiao"
    ],
    "abstract":"This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.",
    "venue":"International Conference on Learning Representations",
    "year":2023,
    "publication_date":"2023-07-13",
    "citation_count":362,
    "reference_count":84,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"conf\/iclr\/WangH00YML0C00024",
      "ArXiv":"2307.06942",
      "DOI":"10.48550\/arXiv.2307.06942",
      "CorpusId":259847783
    }
  },
  {
    "paper_id":"c845494445f3bfa01d8245a4759b144e27aa3788",
    "title":"A Survey of Knowledge-enhanced Text Generation",
    "authors":[
      "W. Yu",
      "Wenhao Yu",
      "Chenguang Zhu",
      "Zaitang Li",
      "Zhiting Hu",
      "Qingyun Wang",
      "Heng Ji",
      "Meng Jiang"
    ],
    "abstract":"The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.",
    "venue":"ACM Computing Surveys",
    "year":2020,
    "publication_date":"2020-10-09",
    "citation_count":311,
    "reference_count":237,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2010-04389",
      "MAG":"3092288641",
      "ArXiv":"2010.04389",
      "DOI":"10.1145\/3512467",
      "CorpusId":222272210
    }
  },
  {
    "paper_id":"21adf4285eb85f4a50106b84906f41c2bd68d510",
    "title":"A Causal Lens for Controllable Text Generation",
    "authors":[
      "Zhiting Hu",
      "Erran L. Li"
    ],
    "abstract":"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.",
    "venue":"Neural Information Processing Systems",
    "year":2022,
    "publication_date":"2022-01-22",
    "citation_count":68,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science",
      "Mathematics"
    ],
    "external_ids":{
      "ArXiv":"2201.09119",
      "DBLP":"conf\/nips\/HuL21",
      "CorpusId":243843906
    }
  },
  {
    "paper_id":"304cf21da84961469ac9f43405df187441832b61",
    "title":"NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics",
    "authors":[
      "Ximing Lu",
      "S. Welleck",
      "Peter West",
      "Liwei Jiang",
      "Jungo Kasai",
      "Daniel Khashabi",
      "Ronan Le Bras",
      "Lianhui Qin",
      "Youngjae Yu",
      "Rowan Zellers",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "abstract":"The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A^* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-12-16",
    "citation_count":174,
    "reference_count":67,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2022.naacl-main.57",
      "DBLP":"conf\/naacl\/LuWWJKKBQYZSC22",
      "ArXiv":"2112.08726",
      "DOI":"10.18653\/v1\/2022.naacl-main.57",
      "CorpusId":245218671
    }
  },
  {
    "paper_id":"7845bfb55f5ce573b87d77bb76d4d38829b37620",
    "title":"TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
    "authors":[
      "Adaku Uchendu",
      "Zeyu Ma",
      "Thai Le",
      "Rui Zhang",
      "Dongwon Lee"
    ],
    "abstract":"Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called\"Turing Test\"problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https:\/\/turingbench.ist.psu.edu\/",
    "venue":"Conference on Empirical Methods in Natural Language Processing",
    "year":2021,
    "publication_date":"2021-09-27",
    "citation_count":152,
    "reference_count":78,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2109-13296",
      "ArXiv":"2109.13296",
      "DOI":"10.18653\/v1\/2021.findings-emnlp.172",
      "CorpusId":237589233
    }
  },
  {
    "paper_id":"6e0b6ba5cae954a0643baeb00167965e88458fc3",
    "title":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
    "authors":[
      "Tianxing He",
      "Jingyu (Jack) Zhang",
      "Tianle Wang",
      "Sachin Kumar",
      "Kyunghyun Cho",
      "James R. Glass",
      "Yulia Tsvetkov"
    ],
    "abstract":"In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https:\/\/github.com\/cloudygoose\/blindspot_nlg.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-12-20",
    "citation_count":56,
    "reference_count":89,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2212.10020",
      "DBLP":"journals\/corr\/abs-2212-10020",
      "ACL":"2023.acl-long.674",
      "DOI":"10.48550\/arXiv.2212.10020",
      "CorpusId":254877323
    }
  },
  {
    "paper_id":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5",
    "title":"Pretrained Language Models for Text Generation: A Survey",
    "authors":[
      "Junyi Li",
      "Tianyi Tang",
      "Wayne Xin Zhao",
      "Ji-rong Wen"
    ],
    "abstract":"Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.",
    "venue":"arXiv.org",
    "year":2021,
    "publication_date":"2021-05-21",
    "citation_count":204,
    "reference_count":61,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ArXiv":"2105.10311",
      "DBLP":"journals\/corr\/abs-2105-10311",
      "CorpusId":235125595
    }
  },
  {
    "paper_id":"c6bf48f25e0a65d64d658b47326de5922ea7dd44",
    "title":"A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
    "authors":[
      "Tianyu Liu",
      "Yizhe Zhang",
      "C. Brockett",
      "Yi Mao",
      "Zhifang Sui",
      "Weizhu Chen",
      "W. Dolan"
    ],
    "abstract":"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
    "venue":"Annual Meeting of the Association for Computational Linguistics",
    "year":2021,
    "publication_date":"2021-04-18",
    "citation_count":176,
    "reference_count":64,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "DBLP":"journals\/corr\/abs-2104-08704",
      "ArXiv":"2104.08704",
      "ACL":"2022.acl-long.464",
      "DOI":"10.18653\/v1\/2022.acl-long.464",
      "CorpusId":233296648
    }
  },
  {
    "paper_id":"47b23ebaeb60184569c22aa484167bc32f914fe3",
    "title":"Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning",
    "authors":[
      "Fei Wang",
      "Zhewei Xu",
      "Pedro A. Szekely",
      "Muhao Chen"
    ],
    "abstract":"Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.",
    "venue":"North American Chapter of the Association for Computational Linguistics",
    "year":2022,
    "publication_date":"2022-05-08",
    "citation_count":55,
    "reference_count":49,
    "fields_of_study":[
      "Computer Science"
    ],
    "external_ids":{
      "ACL":"2022.naacl-main.371",
      "DBLP":"conf\/naacl\/WangXSC22",
      "ArXiv":"2205.03972",
      "DOI":"10.48550\/arXiv.2205.03972",
      "CorpusId":248571764
    }
  }
]