paper_id,title,abstract,year,venue,original_length,cleaned_length,word_count,token_count,compression_ratio,cleaned_text,tokens
s2_369b449415d50387fba048bbd4d26ee890df84b5,InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation,"This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. ...",2023,International Conference on Learning Representations,1453,1453,181,187,1.0,"internvid: a large-scale video-text dataset for multimodal understanding and generation this paper introduces internvid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. the internvid dataset contains over 7 million videos lasting nearly 760k hours, yielding 234m video clips accompanied by detailed descriptions of total 4.1b words. our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (llm), thereby showcasing its efficacy in learning video-language representation at scale. specifically, we utilize a multi-scale approach to generate video-related descriptions. furthermore, we introduce viclip, a video-text representation learning model based on vit-l. learned on internvid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. they are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. these proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.","['internvid', 'large', 'scale', 'video', 'text', 'dataset', 'for', 'multimodal', 'understanding', 'and', 'generation', 'this', 'paper', 'introduces', 'internvid', 'large', 'scale', 'video', 'centric', 'multimodal']"
s2_c845494445f3bfa01d8245a4759b144e27aa3788,A Survey of Knowledge-enhanced Text Generation,"The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging ...",2020,ACM Computing Surveys,1329,1329,197,198,1.0,"a survey of knowledge-enhanced text generation the goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. it is one of the most important yet challenging tasks in natural language processing (nlp). various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. however, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. to address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. this research topic is known as knowledge-enhanced text generation. in this survey, we present a comprehensive review of the research on this topic over the past five years. the main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. this survey can have broad audiences, researchers and practitioners, in academia and industry.","['survey', 'of', 'knowledge', 'enhanced', 'text', 'generation', 'the', 'goal', 'of', 'text', 'to', 'text', 'generation', 'is', 'to', 'make', 'machines', 'express', 'like', 'human']"
s2_21adf4285eb85f4a50106b84906f41c2bd68d510,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing te...",2022,Neural Information Processing Systems,1243,1243,172,164,1.0,"a causal lens for controllable text generation controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). this paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. a direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. we treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. we then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.","['causal', 'lens', 'for', 'controllable', 'text', 'generation', 'controllable', 'text', 'generation', 'concerns', 'two', 'fundamental', 'tasks', 'of', 'wide', 'applications', 'namely', 'generating', 'text', 'of']"
s2_304cf21da84961469ac9f43405df187441832b61,NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics,"The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requi...",2021,North American Chapter of the Association for Computational Linguistics,1362,1345,170,179,0.9875183553597651,"neurologic a*esque decoding: constrained text generation with lookahead heuristics the dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. drawing inspiration from the a^* search algorithm, we propose neurologic a*esque, a decoding algorithm that incorporates heuristic estimates of future cost. we develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. to enable constrained generation, we build on neurologic decoding , combining its flexibility in incorporating logical constraints with a*esque estimates of future constraint satisfaction. our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. the improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. neurologic a*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.","['neurologic', 'esque', 'decoding', 'constrained', 'text', 'generation', 'with', 'lookahead', 'heuristics', 'the', 'dominant', 'paradigm', 'for', 'neural', 'text', 'generation', 'is', 'left', 'to', 'right']"
s2_7845bfb55f5ce573b87d77bb76d4d38829b37620,TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation,"Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need t...",2021,Conference on Empirical Methods in Natural Language Processing,1425,1392,187,173,0.9768421052631578,"turingbench: a benchmark environment for turing test in the age of neural text generation recent progress in generative language models has enabled machines to generate astonishingly realistic texts. while there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). however, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called""turing test""problem for neural text generation methods. in this work, we present the turingbench benchmark environment, which is comprised of (1) a dataset with 200k human- or machine-generated samples across 20 labels {human, gpt-1, gpt-2_small, gpt-2_medium, gpt-2_large, gpt-2_xl, gpt-2_pytorch, gpt-3, grover_base, grover_large, grover_mega, ctrl, xlm, xlnet_base, xlnet_large, fair_wmt19, fair_wmt20, transformer_xl, pplm_distil, pplm_gpt2}, (2) two benchmark tasks -- i.e., turing test (tt) and authorship attribution (aa), and (3) a website with leaderboards. our preliminary experimental results using turingbench show that fair_wmt20 and gpt-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest f1 score by five state-of-the-art tt detection models. the turingbench is available at:","['turingbench', 'benchmark', 'environment', 'for', 'turing', 'test', 'in', 'the', 'age', 'of', 'neural', 'text', 'generation', 'recent', 'progress', 'in', 'generative', 'language', 'models', 'has']"
s2_6e0b6ba5cae954a0643baeb00167965e88458fc3,On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,"In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a...",2022,Annual Meeting of the Association for Computational Linguistics,1065,1019,154,151,0.9568075117370892,"on the blind spots of model-based evaluation metrics for text generation in this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. we examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. for example, we find that bertscore is confused by truncation errors in summarization, and mauve (built on top of gpt-2) is insensitive to errors at the beginning or middle of generations. further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. we have released our code and data at","['on', 'the', 'blind', 'spots', 'of', 'model', 'based', 'evaluation', 'metrics', 'for', 'text', 'generation', 'in', 'this', 'work', 'we', 'explore', 'useful', 'but', 'often']"
s2_d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5,Pretrained Language Models for Text Generation: A Survey,Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation mo...,2021,arXiv.org,972,972,146,145,1.0,"pretrained language models for text generation: a survey text generation has become one of the most important yet challenging tasks in natural language processing (nlp). the resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (plms). in this paper, we present an overview of the major advances achieved in the topic of plms for text generation. as the preliminaries, we present the general task definition and briefly describe the mainstream architectures of plms for text generation. as the core content, we discuss how to adapt existing plms to model different input data and satisfy special properties in the generated text. we further summarize several important fine-tuning strategies for text generation. finally, we present several future directions and conclude this paper. our survey aims to provide text generation researchers a synthesis and pointer to related research.","['pretrained', 'language', 'models', 'for', 'text', 'generation', 'survey', 'text', 'generation', 'has', 'become', 'one', 'of', 'the', 'most', 'important', 'yet', 'challenging', 'tasks', 'in']"
s2_c6bf48f25e0a65d64d658b47326de5922ea7dd44,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually att...",2021,Annual Meeting of the Association for Computational Linguistics,1197,1197,160,167,1.0,"a token-level reference-free hallucination detection benchmark for free-form text generation large pretrained generative models like gpt-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. however ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. as a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named hades (hallucination detection dataset). to create this dataset, we first perturb a large number of text segments extracted from english language wikipedia, and then verify these with crowd-sourced annotations. to mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. we conduct comprehensive data analyses and create multiple baseline models.","['token', 'level', 'reference', 'free', 'hallucination', 'detection', 'benchmark', 'for', 'free', 'form', 'text', 'generation', 'large', 'pretrained', 'generative', 'models', 'like', 'gpt', 'often', 'suffer']"
s2_735bf29aaf13c9420653e271db37614be55154d7,Recent Advances in Retrieval-Augmented Text Generation,"Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims ...",2022,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,669,669,84,91,1.0,"recent advances in retrieval-augmented text generation recently retrieval-augmented text generation has achieved state-of-the-art performance in many nlp tasks and has attracted increasing attention of the nlp and ir community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. it firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.","['recent', 'advances', 'in', 'retrieval', 'augmented', 'text', 'generation', 'recently', 'retrieval', 'augmented', 'text', 'generation', 'has', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'in']"
s2_e5f9cec0538f01e6e0435892746fa47a8c1068ca,Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries,"We propose a text editor to help users plan, structure and reflect on their writing process. It provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summa...",2022,ACM Symposium on User Interface Software and Technology,1084,1084,164,159,1.0,"beyond text generation: supporting writers with continuous automatic text summaries we propose a text editor to help users plan, structure and reflect on their writing process. it provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summarization. summary levels range from full text, to selected (central) sentences, down to a collection of keywords. to understand how users interact with this system during writing, we conducted two user studies (n=4 and n=8) in which people wrote analytic essays about a given topic and article. as a key finding, the summaries gave users an external perspective on their writing and helped them to revise the content and scope of their drafted paragraphs. people further used the tool to quickly gain an overview of the text and developed strategies to integrate insights from the automated summaries. more broadly, this work explores and highlights the value of designing ai tools for writers, with natural language processing (nlp) capabilities that go beyond direct text generation and correction.","['beyond', 'text', 'generation', 'supporting', 'writers', 'with', 'continuous', 'automatic', 'text', 'summaries', 'we', 'propose', 'text', 'editor', 'to', 'help', 'users', 'plan', 'structure', 'and']"
s2_cb648d482dbd1e6ad0b0f4da43aca71c06538d4f,Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise,"In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists o...",2022,International Conference on Machine Learning,1127,1068,149,150,0.9476486246672582,"text generation with diffusion language models: a pre-training approach with continuous paragraph denoise in this paper, we introduce a novel diffusion language model pre-training framework for text generation, which we call genie. genie is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. to pre-train genie on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. we evaluate genie on four downstream text generation benchmarks, namely xsum, cnn/dailymail, gigaword, and commongen. our experimental results show that genie achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. the code and models of genie are available at","['text', 'generation', 'with', 'diffusion', 'language', 'models', 'pre', 'training', 'approach', 'with', 'continuous', 'paragraph', 'denoise', 'in', 'this', 'paper', 'we', 'introduce', 'novel', 'diffusion']"
s2_19af8292ff3cc10aad6f190490f0d34691658179,Investigating Pretrained Language Models for Graph-to-Text Generation,"Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adapti...",2020,NLP4CONVAI,1205,1205,173,180,1.0,"investigating pretrained language models for graph-to-text generation graph-to-text generation aims to generate fluent texts from graph-based data. in this paper, we investigate two recent pretrained language models (plms) and analyze the impact of different task-adaptive pretraining strategies for plms in graph-to-text generation. we present a study across three graph domains: meaning representations, wikipedia knowledge graphs (kgs) and scientific kgs. we show that approaches based on plms bart and t5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. we report new state-of-the-art bleu scores of 49.72 on amr-ldc2017t10, 59.70 on webnlg, and 25.66 on agenda datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. in an extensive analysis, we identify possible reasons for the plms’ success on graph-to-text tasks. our findings suggest that the plms benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.","['investigating', 'pretrained', 'language', 'models', 'for', 'graph', 'to', 'text', 'generation', 'graph', 'to', 'text', 'generation', 'aims', 'to', 'generate', 'fluent', 'texts', 'from', 'graph']"
s2_6151ee4af6a3fe78f2df7c605598cd9e02b23c5b,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,"While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maxi...",2022,Neural Information Processing Systems,1795,1795,228,238,1.0,"learning to break the loop: analyzing and mitigating repetitions for neural text generation while large-scale neural language models, such as gpt2 and bart, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\textit{e.g.}, greedy search). this phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\% in wikitext-103). to investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. through our quantitative experiments, we find that 1) language models have a preference to repeat the previous sentence; 2) the sentence-level repetitions have a \textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) the sentences with higher initial probabilities usually have a stronger self-reinforcement effect. motivated by our findings, we propose a simple and effective training method \textbf{ditto} (pseu\underline{d}o-repet\underline{it}ion penaliza\underline{t}i\underline{o}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. although our method is motivated by mitigating repetitions, experiments show that ditto not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. extensive experiments on open-ended text generation (wikitext-103) and text summarization (cnn/dailymail) demonstrate the generality and effectiveness of our method.","['learning', 'to', 'break', 'the', 'loop', 'analyzing', 'and', 'mitigating', 'repetitions', 'for', 'neural', 'text', 'generation', 'while', 'large', 'scale', 'neural', 'language', 'models', 'such']"
s2_6f709278506813d04a074e6fa20188cce9bb927b,LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching,"The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across var-ious real-world scenarios....",2023,Computer Vision and Pattern Recognition,1175,1175,151,161,1.0,"luciddreamer: towards high-fidelity text-to-3d generation via interval score matching the recent advancements in text-to-3d generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3d assets across var-ious real-world scenarios. while recent advancements in text-to-3d generation have shown promise, they often fall short in rendering detailed and high-quality 3d models. this problem is especially prevalent as many methods base themselves on score distillation sampling (sds). this paper identifies a notable deficiency in sds, that it brings inconsistent and low-quality updating direction for the 3d model, causing the over-smoothing effect. to address this, we propose a novel approach called interval score matching (ism). ism employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. furthermore, we incorporate 3d gaussian splatting into our text-to-3d generation pipeline. extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency. our code is available at: envision-research/luciddreamer","['luciddreamer', 'towards', 'high', 'fidelity', 'text', 'to', 'generation', 'via', 'interval', 'score', 'matching', 'the', 'recent', 'advancements', 'in', 'text', 'to', 'generation', 'mark', 'significant']"
s2_823cacd5255f3897a8d29f29a7c7cb8f978bd928,CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks,"Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, a recent work has introduced...",2022,Neural Information Processing Systems,1564,1564,228,226,1.0,"cater: intellectual property protection on text generation apis via conditional watermarks previous works have validated that text generation apis can be stolen through imitation attacks, causing ip violations. in order to protect the ip of text generation apis, a recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. however, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. to address this drawback, in this paper, we propose a novel conditional watermarking framework (cater) for protecting the ip of text generation apis. an optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. theoretically, we prove that it is infeasible for even the savviest attacker (they know how cater works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. in addition, \cater can effectively identify the ip infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim apis. we envision our work as a milestone for stealthily protecting the ip of text generation apis.","['cater', 'intellectual', 'property', 'protection', 'on', 'text', 'generation', 'apis', 'via', 'conditional', 'watermarks', 'previous', 'works', 'have', 'validated', 'that', 'text', 'generation', 'apis', 'can']"
s2_3af134a559a618b3185390646d49d1d4e7ffab45,Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model,"Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low ...",2023,International Conference on Learning Representations,1161,1131,165,165,0.9741602067183462,"instant3d: fast text-to-3d with sparse-view generation and large reconstruction model text-to-3d with diffusion models has achieved remarkable progress in recent years. however, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and janus problems, or are feed-forward methods that generate low-quality results due to the scarcity of 3d training data. in this paper, we propose instant3d, a novel method that generates high-quality and diverse 3d assets from text prompts in a feed-forward manner. we adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2d text-to-image diffusion model, and then directly regresses the nerf from the generated images with a novel transformer-based sparse-view reconstructor. through extensive experiments, we demonstrate that our method can generate diverse 3d assets of high visual quality within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. our project webpage:","['fast', 'text', 'to', 'with', 'sparse', 'view', 'generation', 'and', 'large', 'reconstruction', 'model', 'text', 'to', 'with', 'diffusion', 'models', 'has', 'achieved', 'remarkable', 'progress']"
s2_1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world kno...",2022,Trans. Mach. Learn. Res.,1473,1442,203,213,0.9789545145960624,"scaling autoregressive models for content-rich text-to-image generation we present the pathways autoregressive text-to-image (parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. this strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. our approach is simple: first, parti uses a transformer-based image tokenizer, vit-vqgan, to encode images as sequences of discrete tokens. second, we achieve consistent quality improvements by scaling the encoder-decoder transformer model up to 20b parameters, with a new state-of-the-art zero-shot fid score of 7.23 and finetuned fid score of 3.22 on ms-coco. our detailed analysis on localized narratives as well as partiprompts (p2), a new holistic benchmark of over 1600 english prompts, demonstrate the effectiveness of parti across a wide variety of categories and difficulty aspects. we also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. see for high-resolution images.","['scaling', 'autoregressive', 'models', 'for', 'content', 'rich', 'text', 'to', 'image', 'generation', 'we', 'present', 'the', 'pathways', 'autoregressive', 'text', 'to', 'image', 'parti', 'model']"
s2_2a3213cb3c755f036d5dfec7261d726a819c78c1,Muse: Text-To-Image Generation via Masked Generative Transformers,"We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is...",2023,International Conference on Machine Learning,1407,1378,205,206,0.9793887704335466,"muse: text-to-image generation via masked generative transformers we present muse, a text-to-image transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (llm), muse is trained to predict randomly masked image tokens. compared to pixel-space diffusion models, such as imagen and dall-e 2, muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as parti, muse is more efficient due to the use of parallel decoding. the use of a pre-trained llm enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. our 900m parameter model achieves a new sota on cc3m, with an fid score of 6.06. the muse 3b parameter model achieves an fid of 7.88 on zero-shot coco evaluation, along with a clip score of 0.32. muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. more results are available at","['muse', 'text', 'to', 'image', 'generation', 'via', 'masked', 'generative', 'transformers', 'we', 'present', 'muse', 'text', 'to', 'image', 'transformer', 'model', 'that', 'achieves', 'state']"
s2_e15900cf7c93d4b6e45a12fe3534840c910467e1,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,"In addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. Existing works generally learn such concepts in...",2023,IEEE International Conference on Computer Vision,1339,1305,181,184,0.9746079163554892,"elite: encoding visual concepts into textual embeddings for customized text-to-image generation in addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. in this paper, we instead propose a learning-based encoder, which consists of a global and a local mapping networks for fast and accurate customized text-to-image generation. in specific, the global mapping network projects the hierarchical features of a given image into multiple ""new"" words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). in the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. we compare our method with existing optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables high-fidelity inversion and more robust editability with a significantly faster encoding process. our code is publicly available at","['elite', 'encoding', 'visual', 'concepts', 'into', 'textual', 'embeddings', 'for', 'customized', 'text', 'to', 'image', 'generation', 'in', 'addition', 'to', 'the', 'unprecedented', 'ability', 'in']"
s2_2c6ac935c826002976722ca8d3319f691975687e,Self-conditioned Embedding Diffusion for Text Generation,"Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens ...",2022,arXiv.org,1002,1002,140,139,1.0,"self-conditioned embedding diffusion for text generation can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? to circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. we propose self-conditioned embedding diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.","['self', 'conditioned', 'embedding', 'diffusion', 'for', 'text', 'generation', 'can', 'continuous', 'diffusion', 'models', 'bring', 'the', 'same', 'performance', 'breakthrough', 'on', 'natural', 'language', 'they']"
s2_86891d00499eebe86d3f1e39143d412addf2652b,DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation,"Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pretrained Transformer (GPT) has achieved ...",2022,Micro,1832,1832,259,262,1.0,"dfx: a low-latency multi-fpga appliance for accelerating transformer-based text generation transformer is a deep learning language model widely used for natural language processing (nlp) services in datacenters. among transformer models, generative pretrained transformer (gpt) has achieved remarkable performance in text generation, or natural language generation (nlg), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. the conventional platforms such as gpu are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation. in this paper, we present dfx, a multi-fpga acceleration appliance that executes gpt-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. dfx uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. its compute cores operate on custom instructions and provide gpt-2 operations end-to-end. we implement the proposed hardware architecture on four xilinx alveo u280 fpgas and utilize all of the channels of the high bandwidth memory (hbm) and the maximum number of compute resources for high hardware efficiency. dfx achieves 5.58$\times$ speedup and 3.99$\times$ energy efficiency over four nvidia v100 gpus on the modern gpt-2 model. dfx is also 8.21$\times$ more cost-effective than the gpu appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.","['dfx', 'low', 'latency', 'multi', 'fpga', 'appliance', 'for', 'accelerating', 'transformer', 'based', 'text', 'generation', 'transformer', 'is', 'deep', 'learning', 'language', 'model', 'widely', 'used']"
s2_05bcf9999525656cfaa59bc71f8572d771ff3776,Language Models Can See: Plugging Visual Controls in Text Generation,"Generative language models (LMs) such as GPT-2/3 can be prompted to generate text with remarkable quality. While they are designed for text-prompted generation, it remains an open question how the gen...",2022,arXiv.org,1615,1615,236,239,1.0,"language models can see: plugging visual controls in text generation generative language models (lms) such as gpt-2/3 can be prompted to generate text with remarkable quality. while they are designed for text-prompted generation, it remains an open question how the generation process could be guided by modalities beyond text such as images. in this work, we propose a training-free framework, called magic (image-guided text generation with clip), for plugging in visual controls in the generation process and enabling lms to perform multimodal tasks (e.g., image captioning) in a zero-shot manner. magic is a simple yet efficient plug-and-play framework, which directly combines an off-the-shelf lm (i.e., gpt-2) and an image-text matching model (i.e., clip) for image-grounded text generation. during decoding, magic influences the generation of the lm by introducing a clip-induced score, called magic score, which regularizes the generated result to be semantically related to a given image while being coherent to the previously generated context. notably, the proposed decoding scheme does not involve any gradient update operation, therefore being computationally efficient. on the challenging task of zero-shot image captioning, magic outperforms the state-of-the-art method by notable margins with a nearly 27 times decoding speedup. magic is a flexible framework and is theoretically compatible with any text generation tasks that incorporate image grounding. in the experiments, we showcase that it is also capable of performing visually grounded story generation given both an image and a text prompt.","['language', 'models', 'can', 'see', 'plugging', 'visual', 'controls', 'in', 'text', 'generation', 'generative', 'language', 'models', 'lms', 'such', 'as', 'gpt', 'can', 'be', 'prompted']"
s2_a9e00c216ce69325a15fd139da0624978e54058a,Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale,"Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks no...",2023,Neural Information Processing Systems,1277,1243,183,190,0.9733750978856696,"voicebox: text-guided multilingual universal speech generation at scale large-scale generative models such as gpt and dall-e have revolutionized the research community. these models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. in contrast, speech generative models are still primitive in terms of scale and task generalization. in this paper, we present voicebox, the most versatile text-guided generative model for speech at scale. voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50k hours of speech that are not filtered or enhanced. similar to gpt, voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. in particular, voicebox outperforms the state-of-the-art zero-shot tts model vall-e on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. audio samples can be found in \url{","['voicebox', 'text', 'guided', 'multilingual', 'universal', 'speech', 'generation', 'at', 'scale', 'large', 'scale', 'generative', 'models', 'such', 'as', 'gpt', 'and', 'dall', 'have', 'revolutionized']"
s2_6e3f8187f8fef3e11578a73f32da07d33dbf8235,DART: Open-Domain Structured Data Record to Text Generation,"We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables...",2020,North American Chapter of the Association for Computational Linguistics,1136,1101,156,162,0.9691901408450704,"dart: open-domain structured data record to text generation we present dart, an open domain structured data record to text generation dataset with over 82k instances (darts). data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. to this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. we present systematic evaluation on dart as well as new state-of-the-art results on webnlg 2017 to show that dart (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. our data and code can be found at","['dart', 'open', 'domain', 'structured', 'data', 'record', 'to', 'text', 'generation', 'we', 'present', 'dart', 'an', 'open', 'domain', 'structured', 'data', 'record', 'to', 'text']"
s2_4fff661078543f6ffb9fe2c0c04829a877f5cfa2,Next-Generation Database Interfaces: A Survey of LLM-Based Text-to-SQL,"Generating accurate SQL from users’ natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehe...",2024,IEEE Transactions on Knowledge and Data Engineering,1570,1570,203,222,1.0,"next-generation database interfaces: a survey of llm-based text-to-sql generating accurate sql from users’ natural language questions (text-to-sql) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and sql generation. traditional text-to-sql systems, which combine human engineering and deep neural networks, have made significant progress. subsequently, pre-trained language models (plms) have been developed for text-to-sql tasks, achieving promising results. however, as modern databases and user questions grow more complex, plms with a limited parameter size often produce incorrect sql. this necessitates more sophisticated and tailored optimization methods, which restrict the application of plm-based systems. recently, large language models (llms) have shown significant capabilities in natural language understanding as model scale increases. thus, integrating llm-based solutions can bring unique opportunities, improvements, and solutions to text-to-sql research. in this survey, we provide a comprehensive review of existing llm-based text-to-sql studies. specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-sql. next, we introduce the datasets and metrics designed to evaluate text-to-sql systems. subsequently, we present a systematic analysis of recent advances in llm-based text-to-sql. finally, we make a summary and discuss the remaining challenges in this field and suggest expectations for future research directions.","['next', 'generation', 'database', 'interfaces', 'survey', 'of', 'llm', 'based', 'text', 'to', 'sql', 'generating', 'accurate', 'sql', 'from', 'users', 'natural', 'language', 'questions', 'text']"
s2_21a77ed349c8621d0a0ef8407eb744e3de3b13c5,"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text","Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, simplifying the process of producing diverse and individual content. Current methods excel in...",2024,Computer Vision and Pattern Recognition,1501,1501,207,204,1.0,"streamingt2v: consistent, dynamic, and extendable long video generation from text text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, simplifying the process of producing diverse and individual content. current methods excel in generating short videos (up to 16s), but produce hard-cuts when naively extended to long video synthesis. to overcome these limitations, we present streamingt2v, an autoregressive method that generates long videos of up to 2 minutes or longer with seamless transitions. the key components are: (i) a short-term memory block called conditional attention module (cam), which conditions the current generation on the features extracted from the preceding chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a longterm memory block called appearance preservation module (apm), which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that allows for the autoregressive application of a video enhancer on videos of indefinite length, ensuring consistency across chunks. experiments show that streamingt2v produces more motion, while competing methods suffer from video stagnation when applied naively in an autoregressive fashion. thus, we propose with streamingt2v a high-quality seamless text-to-long video generator, surpassing competitors in both consistency and motion.","['consistent', 'dynamic', 'and', 'extendable', 'long', 'video', 'generation', 'from', 'text', 'text', 'to', 'video', 'diffusion', 'models', 'enable', 'the', 'generation', 'of', 'high', 'quality']"
s2_395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct...",2019,Annual Meeting of the Association for Computational Linguistics,1384,1384,199,200,1.0,"bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension we present bart, a denoising autoencoder for pretraining sequence-to-sequence models. bart is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. it uses a standard tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing bert (due to the bidirectional encoder), gpt (with the left-to-right decoder), and other recent pretraining schemes. we evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. bart is particularly effective when fine tuned for text generation but also works well for comprehension tasks. it matches the performance of roberta on glue and squad, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 rouge. bart also provides a 1.1 bleu increase over a back-translation system for machine translation, with only target language pretraining. we also replicate other pretraining schemes within the bart framework, to understand their effect on end-task performance.","['bart', 'denoising', 'sequence', 'to', 'sequence', 'pre', 'training', 'for', 'natural', 'language', 'generation', 'translation', 'and', 'comprehension', 'we', 'present', 'bart', 'denoising', 'autoencoder', 'for']"
s2_4a6a65968a8eb8c09ffb57a7774ddabb596565b1,COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics,"Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are...",2022,Neural Information Processing Systems,1163,1163,150,157,1.0,"cold decoding: energy-based constrained text generation with langevin dynamics many applications of text generation require incorporating different constraints to control the semantics or style of generated text. these constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). in this paper, we present energy-based constrained decoding with langevin dynamics (cold), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. cold decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.","['cold', 'decoding', 'energy', 'based', 'constrained', 'text', 'generation', 'with', 'langevin', 'dynamics', 'many', 'applications', 'of', 'text', 'generation', 'require', 'incorporating', 'different', 'constraints', 'to']"
s2_3f19484f941f209f45a51b1b69160e24e9b9dc99,GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation,"Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the ass...",2024,Computer Vision and Pattern Recognition,1231,1193,171,177,0.9691307879772543,"gpt-4v(ision) is a human-aligned evaluator for text-to-3d generation despite recent advances in text-to-3d generative methods, there is a notable absence of reliable evaluation metrics. existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. these metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. conducting user preference studies is an alternative that offers both adaptability and human-aligned results. user studies, however, can be very ex-pensive to scale. this paper presents an automatic, ver-satile, and human-aligned evaluation metric for text-to-3d generative models. to this end, we first develop a prompt generator using gpt-4v to generate evaluating prompts, which serve as input to compare text-to-3d models. we further design a method instructing gpt-4v to compare two 3d assets according to user-defined crite-ria. finally, we use these pairwise comparison results to assign these models elo ratings. experimental results suggest our metric strongly aligns with human preference across different evaluation criteria. our code is available at","['gpt', 'ision', 'is', 'human', 'aligned', 'evaluator', 'for', 'text', 'to', 'generation', 'despite', 'recent', 'advances', 'in', 'text', 'to', 'generative', 'methods', 'there', 'is']"
s2_0b9770a377b3f96cef9f268cee1791d39a0d4893,SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive langu...",2022,Annual Meeting of the Association for Computational Linguistics,1192,1192,156,168,1.0,"ssd-lm: semi-autoregressive simplex-based diffusion language model for text generation and modular control despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. in this work, we present ssd-lm—a diffusion-based language model with two key design choices. first, ssd-lm is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. we evaluate ssd-lm on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive gpt-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. on controlled text generation, ssd-lm also outperforms competitive baselines, with an extra advantage in modularity.","['ssd', 'lm', 'semi', 'autoregressive', 'simplex', 'based', 'diffusion', 'language', 'model', 'for', 'text', 'generation', 'and', 'modular', 'control', 'despite', 'the', 'growing', 'success', 'of']"
s2_fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b,AudioLDM: Text-to-Audio Generation with Latent Diffusion Models,"Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with h...",2023,International Conference on Machine Learning,1202,1174,162,166,0.9767054908485857,"audioldm: text-to-audio generation with latent diffusion models text-to-audio (tta) system has recently gained attention for its ability to synthesize general audio based on text descriptions. however, previous studies in tta have limited generation quality with high computational costs. in this study, we propose audioldm, a tta system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (clap) latents. the pretrained clap models enable us to train ldms with audio embedding while providing text embedding as a condition during sampling. by learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, audioldm is advantageous in both generation quality and computational efficiency. trained on audiocaps with a single gpu, audioldm achieves state-of-the-art tta performance measured by both objective and subjective metrics (e.g., frechet distance). moreover, audioldm is the first tta system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. our implementation and demos are available at","['audioldm', 'text', 'to', 'audio', 'generation', 'with', 'latent', 'diffusion', 'models', 'text', 'to', 'audio', 'tta', 'system', 'has', 'recently', 'gained', 'attention', 'for', 'its']"
s2_dc0c132b273456b288a785414db2fa72edf87b1a,BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing,Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subjec...,2023,Neural Information Processing Systems,1493,1372,179,195,0.9189551239115874,"blip-diffusion: pre-trained subject representation for controllable text-to-image generation and editing subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. to overcome these limitations, we introduce blip-diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. unlike other subject-driven generation models, blip-diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. we first pre-train the multimodal encoder following blip-2 to produce visual representation aligned with the text. then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. compared with previous methods such as dreambooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. we also demonstrate that blip-diffusion can be flexibly combined with existing techniques such as controlnet and prompt-to-prompt to enable novel subject-driven generation and editing applications. code and models will be released at project page at","['blip', 'diffusion', 'pre', 'trained', 'subject', 'representation', 'for', 'controllable', 'text', 'to', 'image', 'generation', 'and', 'editing', 'subject', 'driven', 'text', 'to', 'image', 'generation']"
s2_f12a6168ed8de1aee69fee51b469b1aecd5f903e,Factuality Enhanced Language Models for Open-Ended Text Generation,"Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generati...",2022,Neural Information Processing Systems,1446,1398,201,201,0.966804979253112,"factuality enhanced language models for open-ended text generation pretrained language models (lms) are susceptible to generate text with nonfactual information. in this work, we measure and improve the factual accuracy of large-scale lms for open-ended text generation. we design the factualityprompts test set and metrics to measure the factuality of lm generations. based on that, we study the factual accuracy of lms with parameter sizes ranging from 126m to 530b. interestingly, we find that larger lms are more factual than smaller ones, although a previous study suggests that larger lms can be less truthful in terms of misconceptions. in addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step. we propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., wikipedia). we propose a factuality-enhanced training method that uses topicprefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors. we release our code and factualityprompts benchmark at:","['factuality', 'enhanced', 'language', 'models', 'for', 'open', 'ended', 'text', 'generation', 'pretrained', 'language', 'models', 'lms', 'are', 'susceptible', 'to', 'generate', 'text', 'with', 'nonfactual']"
s2_be8e58320203a92bfacc1a1f95f6e65f3ee4fa5c,A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models,Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that be...,2022,ACM Computing Surveys,1623,1623,242,243,1.0,"a survey of controllable text generation using transformer-based pre-trained language models controllable text generation (ctg) is an emerging area in the field of natural language generation (nlg). it is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. in recent years, methods using large-scale pre-trained language models (plms), in particular the widely used transformer-based plms, have become a new paradigm of nlg, allowing generation of more diverse and fluent text. however, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. to this end, controllable text generation using transformer-based plms has become a rapidly growing yet challenging new research hotspot. a diverse range of approaches have emerged in the past 3 to 4 years, targeting different ctg tasks that require different types of controlled constraints. in this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. finally, we discuss the challenges that the field is facing, and put forward various promising future directions. to the best of our knowledge, this is the first survey article to summarize the state-of-the-art ctg techniques from the perspective of transformer-based plms. we hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.","['survey', 'of', 'controllable', 'text', 'generation', 'using', 'transformer', 'based', 'pre', 'trained', 'language', 'models', 'controllable', 'text', 'generation', 'ctg', 'is', 'an', 'emerging', 'area']"
s2_39ba6d541d94132b816938e7e16b1e8fd49c2fd9,Training-Free Consistent Text-to-Image Generation,"Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the...",2024,ACM Transactions on Graphics,1383,1383,178,190,1.0,"training-free consistent text-to-image generation text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. however, using these models to consistently portray the same subject across diverse prompts remains challenging. existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. these methods require lengthy persubject optimization or large-scale pre-training. moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. here, we present consistory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. we introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. we compare consistory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. finally, consistory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.","['training', 'free', 'consistent', 'text', 'to', 'image', 'generation', 'text', 'to', 'image', 'models', 'offer', 'new', 'level', 'of', 'creative', 'flexibility', 'by', 'allowing', 'users']"
s2_0c181f508ec9de8e48f62523ba8a9bcb1f51f83a,Pre-Trained Language Models for Text Generation: A Survey,"Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural g...",2022,ACM Computing Surveys,1317,1317,210,206,1.0,"pre-trained language models for text generation: a survey text generation aims to produce plausible and readable text in human language from input data. the resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (plms). text generation based on plms is viewed as a promising approach in both academia and industry. in this article, we provide a survey on the utilization of plms in text generation. we begin with introducing two key aspects of applying plms to text generation: (1) how to design an effective plm to serve as the generation model; and (2) how to effectively optimize plms given the reference text and to ensure that the generated texts satisfy special text properties. then, we show the major challenges that have arisen in these aspects, as well as possible solutions for them. we also include a summary of various useful resources and typical text generation applications based on plms. finally, we highlight the future research directions which will further improve these plms for text generation. this comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on plms.","['pre', 'trained', 'language', 'models', 'for', 'text', 'generation', 'survey', 'text', 'generation', 'aims', 'to', 'produce', 'plausible', 'and', 'readable', 'text', 'in', 'human', 'language']"
s2_994a1ce6677b496bd3c0c63aceafc6556005e994,GLIGEN: Open-Set Grounded Text-to-Image Generation,"Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-L...",2023,Computer Vision and Pattern Recognition,960,960,132,145,1.0,"gligen: open-set grounded text-to-image generation large-scale text-to-image diffusion models have made amazing advances. however, the status quo is to use text input alone, which can impede controllability. in this work, we propose gligen, grounded-language-to-image generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. to preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. gligen's zero-shot performance on coco and lvis outperforms existing supervised layout-to-image baselines by a large margin.","['gligen', 'open', 'set', 'grounded', 'text', 'to', 'image', 'generation', 'large', 'scale', 'text', 'to', 'image', 'diffusion', 'models', 'have', 'made', 'amazing', 'advances', 'however']"
s2_e23cb51f50f749320b9122fb5f75113b4d192c0a,Evaluation of Text Generation: A Survey,The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centr...,2020,arXiv.org,683,683,104,101,1.0,"evaluation of text generation: a survey the paper surveys evaluation methods of natural language generation (nlg) systems that have been developed in the last few years. we group nlg evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. for each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed nlg tasks and neural nlg models. we then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.","['evaluation', 'of', 'text', 'generation', 'survey', 'the', 'paper', 'surveys', 'evaluation', 'methods', 'of', 'natural', 'language', 'generation', 'nlg', 'systems', 'that', 'have', 'been', 'developed']"
s2_cf694df964caa156ec306b45d3a3127533cb458f,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,"The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create ...",2023,Neural Information Processing Systems,1085,1085,155,167,1.0,"pick-a-pic: an open dataset of user preferences for text-to-image generation the ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. to address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. using this web app we build pick-a-pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. we leverage this dataset to train a clip-based scoring function, pickscore, which exhibits superhuman performance on the task of predicting human preferences. then, we test pickscore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. therefore, we recommend using pickscore for evaluating future text-to-image generation models, and using pick-a-pic prompts as a more relevant dataset than ms-coco. finally, we demonstrate how pickscore can enhance existing text-to-image models via ranking.","['pick', 'pic', 'an', 'open', 'dataset', 'of', 'user', 'preferences', 'for', 'text', 'to', 'image', 'generation', 'the', 'ability', 'to', 'collect', 'large', 'dataset', 'of']"
s2_1b2355c3c674b26a977768a91a164384ad51bbb1,ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation,"We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human prefe...",2023,Neural Information Processing Systems,943,905,125,127,0.9597030752916225,"imagereward: learning and evaluating human preferences for text-to-image generation we present a comprehensive solution to learn and improve text-to-image models from human preference feedback. to begin with, we build imagereward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. in human evaluation, imagereward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. on top of it, we propose reward feedback learning (refl), a direct tuning algorithm to optimize diffusion models against a scorer. both automatic and human evaluation support refl's advantages over compared methods. all code and datasets are provided at \url{","['imagereward', 'learning', 'and', 'evaluating', 'human', 'preferences', 'for', 'text', 'to', 'image', 'generation', 'we', 'present', 'comprehensive', 'solution', 'to', 'learn', 'and', 'improve', 'text']"
s2_5fa10872ef8037853ff7c8baf5f77fb55a918eca,Diffusion Models for Non-autoregressive Text Generation: A Survey,"Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accur...",2023,International Joint Conference on Artificial Intelligence,1332,1273,186,186,0.9557057057057057,"diffusion models for non-autoregressive text generation: a survey non-autoregressive (nar) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. recently, diffusion models, a class of latent variable generative models, have been introduced into nar text generation, showing an improved text generation quality. in this survey, we review the recent progress in diffusion models for nar text generation. as the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for nar generation. as the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. moreover, we discuss the utilization of pre-trained language models (plms) for text diffusion models and introduce optimization techniques for text data. finally, we discuss several promising directions and conclude this paper. our survey aims to provide researchers with a systematic reference of related research on text diffusion models for nar generation. we also demonstrate our collection of text diffusion models at","['diffusion', 'models', 'for', 'non', 'autoregressive', 'text', 'generation', 'survey', 'non', 'autoregressive', 'nar', 'text', 'generation', 'has', 'attracted', 'much', 'attention', 'in', 'the', 'field']"
s2_168d3bfab5cf0ed356c51eb6eaa18654d575a419,Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning,"It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce ...",2023,Annual Meeting of the Association for Computational Linguistics,963,963,126,127,1.0,"click: controllable text generation with sequence likelihood contrastive learning it has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. we introduce click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. it employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). it also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. on the tasks of language detoxification, sentiment steering, and repetition reduction, we show that click outperforms strong baselines of controllable text generation and demonstrate the superiority of click's sample construction strategy.","['click', 'controllable', 'text', 'generation', 'with', 'sequence', 'likelihood', 'contrastive', 'learning', 'it', 'has', 'always', 'been', 'an', 'important', 'yet', 'challenging', 'problem', 'to', 'control']"
s2_c3b0da01017870729a7a83b94e7787e5105cbc32,Learning to Rewrite Prompts for Personalized Text Generation,"Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular ...",2023,The Web Conference,1704,1704,253,251,1.0,"learning to rewrite prompts for personalized text generation facilitated by large language models (llms), personalized text generation has become a rapidly growing research direction. most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the llms to generate personalized text. we consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through apis. under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the llm, a procedure that is usually done manually. in this paper, we propose a novel method to automatically revise prompts for personalized text generation. the proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. the prompt rewriter employs a training paradigm that chains together supervised learning (sl) and reinforcement learning (rl), where sl reduces the search space of rl and rl facilitates end-to-end training of the rewriter. using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. in-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.","['learning', 'to', 'rewrite', 'prompts', 'for', 'personalized', 'text', 'generation', 'facilitated', 'by', 'large', 'language', 'models', 'llms', 'personalized', 'text', 'generation', 'has', 'become', 'rapidly']"
s2_1e33716e8820b867d5a8aaebab44c2d3135ea4ac,Make-A-Video: Text-to-Video Generation without Text-Video Data,We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world l...,2022,International Conference on Learning Representations,1391,1391,202,209,1.0,"make-a-video: text-to-video generation without text-video data we propose make-a-video -- an approach for directly translating the tremendous recent progress in text-to-image (t2i) generation to text-to-video (t2v). our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. make-a-video has three advantages: (1) it accelerates training of the t2v model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. we design a simple yet effective way to build on t2i models with novel and effective spatial-temporal modules. first, we decompose the full temporal u-net and attention tensors and approximate them in space and time. second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides t2v. in all aspects, spatial and temporal resolution, faithfulness to text, and quality, make-a-video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.","['make', 'video', 'text', 'to', 'video', 'generation', 'without', 'text', 'video', 'data', 'we', 'propose', 'make', 'video', 'an', 'approach', 'for', 'directly', 'translating', 'the']"
s2_86be5c90c4128ec59b1c320a16996bb5de68624e,Texygen: A Benchmarking Platform for Text Generation Models,"We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set o...",2018,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,521,521,77,74,1.0,"texygen: a benchmarking platform for text generation models we introduce texygen, a benchmarking platform to support research on open-domain text generation models. texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. the texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.","['texygen', 'benchmarking', 'platform', 'for', 'text', 'generation', 'models', 'we', 'introduce', 'texygen', 'benchmarking', 'platform', 'to', 'support', 'research', 'on', 'open', 'domain', 'text', 'generation']"
s2_53a77e8f73f2ca422d6e38fa9ecc490231ac044c,Neural Text Generation with Unlikelihood Training,"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull...",2019,International Conference on Learning Representations,1246,1246,185,183,1.0,"neural text generation with unlikelihood training neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. in particular, standard likelihood training and decoding leads to dull and repetitive outputs. while some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. in this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. we propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. we show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. according to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.","['neural', 'text', 'generation', 'with', 'unlikelihood', 'training', 'neural', 'text', 'generation', 'is', 'key', 'tool', 'in', 'natural', 'language', 'applications', 'but', 'it', 'is', 'well']"
s2_635cb6fb865e86c108c5d1d895aeac0e759eb199,MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance,A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surfac...,2019,Conference on Empirical Methods in Natural Language Processing,977,977,139,136,1.0,"moverscore: text generation evaluating with contextualized embeddings and earth mover distance a robust evaluation metric has a profound impact on the development of text generation systems. a desirable metric compares system output against references based on their semantics rather than surface forms. in this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. we validate our new metric, namely moverscore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. our findings suggest that metrics combining contextualized representations with a distance measure perform the best. such metrics also demonstrate strong generalization capability across tasks. for ease-of-use we make our metrics available as web service.","['moverscore', 'text', 'generation', 'evaluating', 'with', 'contextualized', 'embeddings', 'and', 'earth', 'mover', 'distance', 'robust', 'evaluation', 'metric', 'has', 'profound', 'impact', 'on', 'the', 'development']"
s2_e6770e3f5e74210c6863aaeed527ac4c1da419d7,A Survey on Retrieval-Augmented Text Generation,"Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generati...",2022,arXiv.org,773,773,95,101,1.0,"a survey on retrieval-augmented text generation recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many nlp tasks. this paper aims to conduct a survey about retrieval-augmented text generation. it firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. finally, it points out some important directions on top of recent methods to facilitate future research.","['survey', 'on', 'retrieval', 'augmented', 'text', 'generation', 'recently', 'retrieval', 'augmented', 'text', 'generation', 'attracted', 'increasing', 'attention', 'of', 'the', 'computational', 'linguistics', 'community', 'compared']"
s2_75ddc4fb91332f95222d74449d96b9f7c8f976c7,Nationality Bias in Text Generation,"Little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social NLP models. This paper exami...",2023,Conference of the European Chapter of the Association for Computational Linguistics,841,841,120,120,1.0,"nationality bias in text generation little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social nlp models. this paper examines how a text generation model, gpt-2, accentuates pre-existing societal biases about country-based demonyms. we generate stories using gpt-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country’s economic status impacts the sentiment of the stories. to reduce the propagation of biases through large language models (llm), we explore the debiasing method of adversarial triggering. our results show that gpt-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same.","['nationality', 'bias', 'in', 'text', 'generation', 'little', 'attention', 'is', 'placed', 'on', 'analyzing', 'nationality', 'bias', 'in', 'language', 'models', 'especially', 'when', 'nationality', 'is']"
s2_0119a57cf88ef16e6dc291252fae340bb6b3953c,CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning,"Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose r...",2020,Findings,1511,1511,202,187,1.0,"commongen: a constrained text generation challenge for generative commonsense reasoning recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. however, building machines with commonsense to compose realistically plausible sentences remains challenging. in this paper, we present a constrained text generation task, commongen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man throws a frisbee and his dog catches it”). the commongen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. experiments show that there is a large gap between state-of-the-art text generation models (e.g., t5) and human performance (31.6% v.s. 63.5% in spice metric). furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as commonsenseqa (76.9% to 78.4 in dev accuracy) by generating additional context.","['commongen', 'constrained', 'text', 'generation', 'challenge', 'for', 'generative', 'commonsense', 'reasoning', 'recently', 'large', 'scale', 'pre', 'trained', 'language', 'models', 'have', 'demonstrated', 'impressive', 'performance']"
