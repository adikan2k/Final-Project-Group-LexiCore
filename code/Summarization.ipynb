{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9-T7XIXhbK3",
        "outputId": "e5b4b4e7-42df-4f59-8109-34732ce210fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project-Group-LexiCore'...\n",
            "remote: Enumerating objects: 415, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 415 (delta 8), reused 1 (delta 1), pack-reused 384 (from 2)\u001b[K\n",
            "Receiving objects: 100% (415/415), 319.87 MiB | 16.90 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n",
            "/content/Final-Project-Group-LexiCore\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/adikan2k/Final-Project-Group-LexiCore.git\n",
        "%cd Final-Project-Group-LexiCore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout trisha-singh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2u6DJshtZC",
        "outputId": "87404f97-e3ed-4976-892c-0ec14f860ebd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating files: 100% (55/55), done.\n",
            "Branch 'trisha-singh' set up to track remote branch 'trisha-singh' from 'origin'.\n",
            "Switched to a new branch 'trisha-singh'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK2ymETLhyFN",
        "outputId": "dec5efe6-2afa-400e-e521-871f5cc195a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'500 Samples'  'Final Group Presentation'     notebooks   requirements.txt\n",
            " code\t       'Final Group Project Report'   Outputs\t  simple_demo.py\n",
            " data\t        LICENSE\t\t\t      README.md   src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwNTxfVh1hn",
        "outputId": "4a67259e-9c6e-45d2-a619-db07290575d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings  processed  raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"data/processed/cleaned_papers.parquet\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "DH7bHak4h2sl",
        "outputId": "5e00503f-329f-45e9-a241-7f5e307e198e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 14)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         paper_id  \\\n",
              "0                              arxiv_2511.15703v2   \n",
              "1                              arxiv_2511.16613v1   \n",
              "2                              arxiv_2511.20992v1   \n",
              "3                              arxiv_2511.15120v1   \n",
              "4  s2orc_e31c6664182777a24b19ef09993ded07d1ed326e   \n",
              "\n",
              "                                               title  \\\n",
              "0  Think Visually, Reason Textually: Vision-Langu...   \n",
              "1  Rate-optimal community detection near the KS t...   \n",
              "2  Dataset Poisoning Attacks on Behavioral Clonin...   \n",
              "3  Neural Networks Learn Generic Multi-Index Mode...   \n",
              "4  Pyramid: A Layered Model for Nested Named Enti...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  [Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuha...   \n",
              "1  [Jingqiu Ding, Yiding Hua, Kasper Lindberg, Da...   \n",
              "2  [Akansha Kalra, Soumil Datta, Ethan Gilmore, D...   \n",
              "3  [Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. ...   \n",
              "4         [Jue Wang, Lidan Shou, Ke Chen, Gang Chen]   \n",
              "\n",
              "                                   original_abstract  \\\n",
              "0  Abstract reasoning from minimal examples remai...   \n",
              "1  We study community detection in the \\emph{symm...   \n",
              "2  Behavior Cloning (BC) is a popular framework f...   \n",
              "3  In deep learning, a central issue is to unders...   \n",
              "4  This paper presents Pyramid, a novel layered m...   \n",
              "\n",
              "                                        cleaned_text language  \\\n",
              "0  abstract reasoning from minimal examples remai...       en   \n",
              "1  we study community detection in the emphsymmet...       en   \n",
              "2  behavior cloning bc is a popular framework for...       en   \n",
              "3  in deep learning, a central issue is to unders...       en   \n",
              "4  this paper presents pyramid, a novel layered m...       en   \n",
              "\n",
              "                                           sentences  num_sentences  \\\n",
              "0  [Abstract reasoning from minimal examples rema...              1   \n",
              "1  [We study community detection in the emphsymme...              1   \n",
              "2  [Behavior Cloning BC is a popular framework fo...              1   \n",
              "3  [In deep learning, a central issue is to under...              1   \n",
              "4  [This paper presents Pyramid, a novel layered ...              1   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [abstract, reasoning, minimal, example, remain...   \n",
              "1  [study, community, detection, emphsymmetric, s...   \n",
              "2  [behavior, clone, popular, framework, train, s...   \n",
              "3  [deep, learning, central, issue, understand, n...   \n",
              "4  [paper, present, pyramid, novel, layered, mode...   \n",
              "\n",
              "                                      processed_text  num_tokens source  year  \\\n",
              "0  abstract reasoning minimal example remain core...         173  arxiv  2025   \n",
              "1  study community detection emphsymmetric stocha...         133  arxiv  2025   \n",
              "2  behavior clone popular framework train sequent...         127  arxiv  2025   \n",
              "3  deep learning central issue understand neural ...         126  arxiv  2025   \n",
              "4  paper present pyramid novel layered model nest...          90  s2orc  2020   \n",
              "\n",
              "                                               venue  \n",
              "0                                              arXiv  \n",
              "1                                              arXiv  \n",
              "2                                              arXiv  \n",
              "3                                              arXiv  \n",
              "4  Annual Meeting of the Association for Computat...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c354056-93b9-42c1-ab63-3319cd246846\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>original_abstract</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>language</th>\n",
              "      <th>sentences</th>\n",
              "      <th>num_sentences</th>\n",
              "      <th>tokens</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>num_tokens</th>\n",
              "      <th>source</th>\n",
              "      <th>year</th>\n",
              "      <th>venue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>arxiv_2511.15703v2</td>\n",
              "      <td>Think Visually, Reason Textually: Vision-Langu...</td>\n",
              "      <td>[Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuha...</td>\n",
              "      <td>Abstract reasoning from minimal examples remai...</td>\n",
              "      <td>abstract reasoning from minimal examples remai...</td>\n",
              "      <td>en</td>\n",
              "      <td>[Abstract reasoning from minimal examples rema...</td>\n",
              "      <td>1</td>\n",
              "      <td>[abstract, reasoning, minimal, example, remain...</td>\n",
              "      <td>abstract reasoning minimal example remain core...</td>\n",
              "      <td>173</td>\n",
              "      <td>arxiv</td>\n",
              "      <td>2025</td>\n",
              "      <td>arXiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>arxiv_2511.16613v1</td>\n",
              "      <td>Rate-optimal community detection near the KS t...</td>\n",
              "      <td>[Jingqiu Ding, Yiding Hua, Kasper Lindberg, Da...</td>\n",
              "      <td>We study community detection in the \\emph{symm...</td>\n",
              "      <td>we study community detection in the emphsymmet...</td>\n",
              "      <td>en</td>\n",
              "      <td>[We study community detection in the emphsymme...</td>\n",
              "      <td>1</td>\n",
              "      <td>[study, community, detection, emphsymmetric, s...</td>\n",
              "      <td>study community detection emphsymmetric stocha...</td>\n",
              "      <td>133</td>\n",
              "      <td>arxiv</td>\n",
              "      <td>2025</td>\n",
              "      <td>arXiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arxiv_2511.20992v1</td>\n",
              "      <td>Dataset Poisoning Attacks on Behavioral Clonin...</td>\n",
              "      <td>[Akansha Kalra, Soumil Datta, Ethan Gilmore, D...</td>\n",
              "      <td>Behavior Cloning (BC) is a popular framework f...</td>\n",
              "      <td>behavior cloning bc is a popular framework for...</td>\n",
              "      <td>en</td>\n",
              "      <td>[Behavior Cloning BC is a popular framework fo...</td>\n",
              "      <td>1</td>\n",
              "      <td>[behavior, clone, popular, framework, train, s...</td>\n",
              "      <td>behavior clone popular framework train sequent...</td>\n",
              "      <td>127</td>\n",
              "      <td>arxiv</td>\n",
              "      <td>2025</td>\n",
              "      <td>arXiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>arxiv_2511.15120v1</td>\n",
              "      <td>Neural Networks Learn Generic Multi-Index Mode...</td>\n",
              "      <td>[Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. ...</td>\n",
              "      <td>In deep learning, a central issue is to unders...</td>\n",
              "      <td>in deep learning, a central issue is to unders...</td>\n",
              "      <td>en</td>\n",
              "      <td>[In deep learning, a central issue is to under...</td>\n",
              "      <td>1</td>\n",
              "      <td>[deep, learning, central, issue, understand, n...</td>\n",
              "      <td>deep learning central issue understand neural ...</td>\n",
              "      <td>126</td>\n",
              "      <td>arxiv</td>\n",
              "      <td>2025</td>\n",
              "      <td>arXiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s2orc_e31c6664182777a24b19ef09993ded07d1ed326e</td>\n",
              "      <td>Pyramid: A Layered Model for Nested Named Enti...</td>\n",
              "      <td>[Jue Wang, Lidan Shou, Ke Chen, Gang Chen]</td>\n",
              "      <td>This paper presents Pyramid, a novel layered m...</td>\n",
              "      <td>this paper presents pyramid, a novel layered m...</td>\n",
              "      <td>en</td>\n",
              "      <td>[This paper presents Pyramid, a novel layered ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[paper, present, pyramid, novel, layered, mode...</td>\n",
              "      <td>paper present pyramid novel layered model nest...</td>\n",
              "      <td>90</td>\n",
              "      <td>s2orc</td>\n",
              "      <td>2020</td>\n",
              "      <td>Annual Meeting of the Association for Computat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c354056-93b9-42c1-ab63-3319cd246846')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c354056-93b9-42c1-ab63-3319cd246846 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c354056-93b9-42c1-ab63-3319cd246846');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3a725e4e-a8f6-4129-bc44-64e04101587c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a725e4e-a8f6-4129-bc44-64e04101587c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3a725e4e-a8f6-4129-bc44-64e04101587c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"arxiv_2511.21378v1\",\n          \"s2orc_ea8c46e193d5121e440daf96edfd15a47151c293\",\n          \"arxiv_2511.21188v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data\",\n          \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\",\n          \"AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.\",\n          \"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.\",\n          \"Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., \\\"shape\\\", \\\"color\\\"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. to address these limitations, we propose adaptive and aggressive rejection aar, a novel method that dynamically excludes anomalies using a modified z-score and gaussian mixture model-based thresholds. aar effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. extensive experiments on two image datasets and thirty tabular datasets demonstrate that aar outperforms the state-of-the-art method by 0.041 auroc. by providing a scalable and reliable solution, aar enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.\",\n          \"generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. while promising, this approach requires to use models with billions of parameters, which are expensive to train and query. in this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. we obtain state-of-the-art results on the natural questions and triviaqa open benchmarks. interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. this is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.\",\n          \"existing prompt learning methods, which are built upon clip models, leverage textual tokens as anchors to guide the learnable soft tokens. this guidance improves clip generalizations. however, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. to address this limitation, we propose anchoropt, a dynamic anchor-based prompt learning framework. specifically, anchoropt introduces dynamism in two key dimensions i anchor values eschew handcrafted explicit textual tokens e.g., shape, color, instead learning dynamically from task-specific data and ii the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. training occurs in two stages we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. as a plug-and-play module, anchoropt integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. code is publicly available at\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_sentences\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"handle contaminate datum pose critical challenge anomaly detection traditional model assume train purely normal datum conventional method mitigate contamination rely fix contamination ratio discrepancy assume actual ratio severely degrade performance especially noisy environment normal abnormal data distribution overlap address limitation propose adaptive aggressive rejection aar novel method dynamically exclude anomaly use modify score gaussian mixture model base threshold aar effectively balance trade preserve normal datum exclude anomaly integrate hard soft rejection strategy extensive experiment two image dataset thirty tabular dataset demonstrate aar outperform state art method auroc provide scalable reliable solution aar enhance robustness contaminate dataset pave way broad real world application domain security healthcare\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 22,\n        \"max\": 237,\n        \"num_unique_values\": 137,\n        \"samples\": [\n          144\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"s2orc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2009,\n        \"max\": 2025,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          2023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"venue\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 56,\n        \"samples\": [\n          \"arXiv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy transformers sentence-transformers faiss-cpu rank-bm25 lime --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp3Ni6BfiibA",
        "outputId": "931c5e25-0c8d-457e-bdf6-a0ac80fabee3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m256.0/275.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"CWD before:\", os.getcwd())\n",
        "%cd /content/Final-Project-Group-LexiCore\n",
        "!git checkout trisha-singh\n",
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twURs_ifijV4",
        "outputId": "587b5665-b5bd-40d9-a948-979bb77ea663"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD before: /content/Final-Project-Group-LexiCore\n",
            "/content/Final-Project-Group-LexiCore\n",
            "Already on 'trisha-singh'\n",
            "Your branch is up to date with 'origin/trisha-singh'.\n",
            "On branch trisha-singh\n",
            "Your branch is up to date with 'origin/trisha-singh'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code/summarization.py\n",
        "\"\"\"\n",
        " Summarization Engine\n",
        "\n",
        "Provides:\n",
        "- extractive_summary_textrank\n",
        "- abstractive_summary_bart\n",
        "- generate_summaries (1-sentence, 3-sentence, 5-bullet)\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "def extractive_summary_textrank(text: str, num_sentences: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Extractive summary using TextRank (Sumy).\n",
        "\n",
        "    Args:\n",
        "        text: input document (abstract)\n",
        "        num_sentences: how many sentences to keep\n",
        "\n",
        "    Returns:\n",
        "        Summary as a single string.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"No text available.\"\n",
        "\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        summarizer = TextRankSummarizer()\n",
        "        summary_sents = summarizer(parser.document, num_sentences)\n",
        "        sentences = [str(s) for s in summary_sents]\n",
        "        if not sentences:\n",
        "            return \"Summary unavailable (text too short).\"\n",
        "        return \" \".join(sentences)\n",
        "    except Exception as e:\n",
        "        return f\"Extractive summary error: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "_bart_summarizer = None\n",
        "\n",
        "def _get_bart_pipeline():\n",
        "    global _bart_summarizer\n",
        "    if _bart_summarizer is None:\n",
        "        _bart_summarizer = pipeline(\n",
        "            \"summarization\",\n",
        "            model=\"facebook/bart-large-cnn\"\n",
        "        )\n",
        "    return _bart_summarizer\n",
        "\n",
        "\n",
        "def abstractive_summary_bart(\n",
        "    text: str,\n",
        "    max_length: int = 120,\n",
        "    min_length: int = 30,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Abstractive summary using BART.\n",
        "\n",
        "    Args:\n",
        "        text: input document\n",
        "        max_length, min_length: summary length control\n",
        "\n",
        "    Returns:\n",
        "        Summary string.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"No text available.\"\n",
        "\n",
        "    try:\n",
        "        summarizer = _get_bart_pipeline()\n",
        "        truncated = text[:1024]   # BART input limit\n",
        "        out = summarizer(\n",
        "            truncated,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            do_sample=False,\n",
        "        )\n",
        "        return out[0][\"summary_text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Abstractive summary error: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_summaries(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Generates:\n",
        "    - one_sentence: short abstractive summary\n",
        "    - three_sentence: extractive summary (3 sentences)\n",
        "    - five_bullets: list of bullet strings\n",
        "\n",
        "    Returns:\n",
        "        dict with keys: one_sentence, three_sentence, five_bullets\n",
        "    \"\"\"\n",
        "    one_sentence = abstractive_summary_bart(\n",
        "        text, max_length=40, min_length=15\n",
        "    )\n",
        "    three_sentence = extractive_summary_textrank(\n",
        "        text, num_sentences=3\n",
        "    )\n",
        "\n",
        "    long_sum = abstractive_summary_bart(\n",
        "        text, max_length=160, min_length=40\n",
        "    )\n",
        "\n",
        "    raw_bullets = [s.strip() for s in long_sum.split(\".\") if s.strip()]\n",
        "    raw_bullets = raw_bullets[:5]\n",
        "    bullets: List[str] = [f\"• {b}\" for b in raw_bullets]\n",
        "\n",
        "    return {\n",
        "        \"one_sentence\": one_sentence,\n",
        "        \"three_sentence\": three_sentence,\n",
        "        \"five_bullets\": bullets,\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcCfNCUWitWf",
        "outputId": "5fe9c222-fde0-461c-9313-a6106fbf1c6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing code/summarization.py\n"
          ]
        }
      ]
    }
  ]
}