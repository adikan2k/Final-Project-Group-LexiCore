{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/adikan2k/Final-Project-Group-LexiCore.git\n",
        "\n",
        "%cd /content/Final-Project-Group-LexiCore\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGC7ArYH3DFG",
        "outputId": "f1b41f26-3cee-47df-d98d-2e113159d49c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project-Group-LexiCore'...\n",
            "remote: Enumerating objects: 439, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 439 (delta 23), reused 18 (delta 9), pack-reused 384 (from 2)\u001b[K\n",
            "Receiving objects: 100% (439/439), 319.90 MiB | 30.02 MiB/s, done.\n",
            "Resolving deltas: 100% (181/181), done.\n",
            "/content/Final-Project-Group-LexiCore\n",
            " code  'Final Group Presentation'     LICENSE\n",
            " data  'Final Group Project Report'   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_pipeline.py\n",
        "\"\"\"\n",
        "Final Pipeline Orchestrator\n",
        "\n",
        "This script:\n",
        "  1. Loads the cleaned corpus (output of Day 1+2).\n",
        "  2. Runs retrieval over title + abstract.\n",
        "  3. Calls the summarization engine (generate_summaries).\n",
        "  4. Saves a final digest JSON.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "CODE_DIR = ROOT / \"code\"\n",
        "sys.path.append(str(CODE_DIR))\n",
        "\n",
        "try:\n",
        "\n",
        "    from summarization import generate_summaries  # type: ignore\n",
        "except Exception:\n",
        "\n",
        "    def generate_summaries(text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Fallback summary used only if real summarization.py is not available.\"\"\"\n",
        "        snippet = (text or \"\").strip()\n",
        "        one_sentence = snippet[:250]\n",
        "        return {\n",
        "            \"one_sentence\": one_sentence,\n",
        "            \"three_sentence\": one_sentence,\n",
        "            \"five_bullets\": [\n",
        "                f\"- {one_sentence}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "def load_corpus() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned paper corpus produced by Day 1/2.\n",
        "\n",
        "    Tries both:\n",
        "      data/processed/cleaned_paper.parquet\n",
        "      data/processed/cleaned_papers.parquet\n",
        "    because naming sometimes differs.\n",
        "    \"\"\"\n",
        "    base = ROOT / \"data\" / \"processed\"\n",
        "    candidates = [\n",
        "        base / \"cleaned_paper.parquet\",\n",
        "        base / \"cleaned_papers.parquet\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for path in candidates:\n",
        "        try:\n",
        "            print(f\"Trying to load: {path}\")\n",
        "            df = pd.read_parquet(path)\n",
        "            print(f\"Loaded {len(df)} rows from {path.name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not load cleaned parquet. Tried {', '.join(str(c) for c in candidates)}.\\n\"\n",
        "        f\"Last error: {last_err}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def simple_retrieval(query: str, df: pd.DataFrame, top_k: int = 5) -> List[int]:\n",
        "    \"\"\"\n",
        "    Very simple keyword retrieval over title + abstract.\n",
        "\n",
        "    (If you later want to plug in BM25/FAISS hybrid, replace this\n",
        "     with calls to your Day-2 retrieval engine.)\n",
        "    \"\"\"\n",
        "    text_series = (\n",
        "        df.get(\"title\", \"\").fillna(\"\").astype(str)\n",
        "        + \" \"\n",
        "        + df.get(\"original_abstract\", df.get(\"abstract\", \"\")).fillna(\"\").astype(str)\n",
        "    )\n",
        "\n",
        "    mask = text_series.str.contains(query, case=False, na=False)\n",
        "    indices = df[mask].index.tolist()\n",
        "\n",
        "    if not indices:\n",
        "        print(\"No exact keyword matches found; falling back to first top_k papers.\")\n",
        "        indices = df.index.tolist()\n",
        "\n",
        "    return indices[:top_k]\n",
        "\n",
        "\n",
        "def build_digest(query: str, df: pd.DataFrame, top_k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Build digest object:\n",
        "      - retrieval to get top_k docs\n",
        "      - summarization for each abstract\n",
        "    \"\"\"\n",
        "    indices = simple_retrieval(query, df, top_k)\n",
        "    papers_out = []\n",
        "\n",
        "    for rank, idx in enumerate(indices, start=1):\n",
        "        row = df.loc[idx]\n",
        "        abstract = row.get(\"original_abstract\", row.get(\"abstract\", \"\"))\n",
        "\n",
        "        summaries = generate_summaries(str(abstract))\n",
        "\n",
        "        papers_out.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"paper_id\": row.get(\"paper_id\", int(idx)),\n",
        "                \"title\": row.get(\"title\", \"Untitled\"),\n",
        "                \"venue\": row.get(\"venue\"),\n",
        "                \"year\": int(row[\"year\"]) if \"year\" in row and pd.notna(row[\"year\"]) else None,\n",
        "                \"summaries\": summaries,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"num_results\": len(papers_out),\n",
        "        \"papers\": papers_out,\n",
        "    }\n",
        "\n",
        "\n",
        "def save_digest(digest: Dict[str, Any], output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Save digest to JSON, converting numpy/pandas types to plain Python types.\n",
        "    \"\"\"\n",
        "    folder = os.path.dirname(output_path)\n",
        "    if folder:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    def convert(o):\n",
        "        if isinstance(o, (np.integer,)):\n",
        "            return int(o)\n",
        "        if isinstance(o, (np.floating,)):\n",
        "            return float(o)\n",
        "        if isinstance(o, np.ndarray):\n",
        "            return o.tolist()\n",
        "        return str(o)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(digest, f, indent=2, default=convert)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    output_path: str = \"Outputs/pipeline_digest.json\",\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline:\n",
        "      1. Load corpus\n",
        "      2. Retrieve top_k docs\n",
        "      3. Summarize each\n",
        "      4. Save digest JSON\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    print(f\" Running pipeline for query: '{query}'\")\n",
        "    print(f\"   top_k       = {top_k}\")\n",
        "    print(f\"   output_path = {output_path}\")\n",
        "\n",
        "    print(\"\\n[1] Loading corpus...\")\n",
        "    df = load_corpus()\n",
        "    print(f\"    Loaded {len(df)} papers.\")\n",
        "\n",
        "    print(\"\\n[2] Building digest (retrieval + summarization)...\")\n",
        "    digest = build_digest(query, df, top_k=top_k)\n",
        "\n",
        "    print(\"\\n[3] Saving digest JSON...\")\n",
        "    save_digest(digest, output_path)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"\\n Done in {elapsed:.2f} seconds.\")\n",
        "    print(f\"   Papers in digest: {digest['num_results']}\")\n",
        "    print(f\"   JSON saved to:    {output_path}\")\n",
        "\n",
        "    return digest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run Day 3 digest pipeline.\")\n",
        "    parser.add_argument(\n",
        "        \"--query\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Search query, e.g. 'transformer models for NLP'\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top_k\",\n",
        "        type=int,\n",
        "        default=5,\n",
        "        help=\"Number of papers to include in the digest\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        type=str,\n",
        "        default=\"Outputs/pipeline_digest.json\",\n",
        "        help=\"Output path for JSON digest\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run_pipeline(\n",
        "        query=args.query,\n",
        "        top_k=args.top_k,\n",
        "        output_path=args.output,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybUDweHY3Occ",
        "outputId": "dde18b18-c58e-41a6-a04c-44f08eddb4be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Final-Project-Group-LexiCore\n",
        "\n",
        "!python run_pipeline.py --query \"transformer models\" --top_k 3 --output Outputs/transformer_pipeline_digest.json\n",
        "\n",
        "print(\"\\nFiles in Outputs/:\")\n",
        "!ls Outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMqAURa3jw_",
        "outputId": "d7ddc9d6-eea3-4660-f353-aea4bbb31d3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Final-Project-Group-LexiCore\n",
            " Running pipeline for query: 'transformer models'\n",
            "   top_k       = 3\n",
            "   output_path = Outputs/transformer_pipeline_digest.json\n",
            "\n",
            "[1] Loading corpus...\n",
            "Trying to load: /content/Final-Project-Group-LexiCore/data/processed/cleaned_paper.parquet\n",
            "Trying to load: /content/Final-Project-Group-LexiCore/data/processed/cleaned_papers.parquet\n",
            "Loaded 500 rows from cleaned_papers.parquet\n",
            "    Loaded 500 papers.\n",
            "\n",
            "[2] Building digest (retrieval + summarization)...\n",
            "\n",
            "[3] Saving digest JSON...\n",
            "\n",
            " Done in 0.22 seconds.\n",
            "   Papers in digest: 3\n",
            "   JSON saved to:    Outputs/transformer_pipeline_digest.json\n",
            "\n",
            "Files in Outputs/:\n",
            "transformer_pipeline_digest.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"Outputs/transformer_pipeline_digest.json\", \"r\") as f:\n",
        "    digest = json.load(f)\n",
        "\n",
        "print(\"Query:\", digest[\"query\"])\n",
        "print(\"Num results:\", digest[\"num_results\"])\n",
        "print(\"\\nTop 2 paper titles:\")\n",
        "\n",
        "for p in digest[\"papers\"][:2]:\n",
        "    print(f\"- #{p['rank']} {p['title']}\")\n",
        "    if isinstance(p[\"summaries\"], dict):\n",
        "        print(\"  1-sentence summary:\", p[\"summaries\"].get(\"one_sentence\", \"\")[:200])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T30cvYoC3zV5",
        "outputId": "2a2bf8fd-9c12-437f-ef25-0b0a8fb4ef86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: transformer models\n",
            "Num results: 3\n",
            "\n",
            "Top 2 paper titles:\n",
            "- #1 Controlling changes to attention logits\n",
            "  1-sentence summary: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying \n",
            "\n",
            "- #2 IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference\n",
            "  1-sentence summary: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the\n",
            "\n"
          ]
        }
      ]
    }
  ]
}