[
  {
    "original_title": "Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks",
    "original_length": 1149,
    "summary": "We apply Random Matrix Theory to derive a closed-form expression for the asymptotic generalization error. We then apply this analysis to recurrent representations and obtain a formula that characterize their performance. Surprisingly, a linear ESN is equivalent to ridge regression with an exponentially time-weighted (''memory'') input covariance.",
    "summary_length": 348,
    "compression_ratio": 0.3028720626631854,
    "year": "2025"
  },
  {
    "original_title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation",
    "original_length": 1253,
    "summary": "Large models adaptation through Federated Learning (FL) is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) This distributed learning paradigm faces several security threats, particularly to its integrity. Backdoor attacks aim to inject malicious behavior during the local training steps of certain clients.",
    "summary_length": 347,
    "compression_ratio": 0.27693535514764567,
    "year": "2025"
  },
  {
    "original_title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning",
    "original_length": 1173,
    "summary": "Seer is a novel online context learning system. It exploits previously overlooked similarities in output lengths and generation patterns. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding.",
    "summary_length": 288,
    "compression_ratio": 0.24552429667519182,
    "year": "2025"
  },
  {
    "original_title": "OPT: Open Pre-trained Transformer Language Models",
    "original_length": 899,
    "summary": "OPT is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. We show that OPT-175B is comparable to GPT-3 while requiring only 1/7th the carbon footprint to develop.",
    "summary_length": 198,
    "compression_ratio": 0.22024471635150167,
    "year": "2022"
  },
  {
    "original_title": "RUBi: Reducing Unimodal Biases in Visual Question Answering",
    "original_length": 1393,
    "summary": "Some VQA models exploit unimodal biases to provide the correct answer without using the image information. We leverage a question-only model that captures the language biases. It prevents the base V QA model from learning them by influencing its predictions.",
    "summary_length": 258,
    "compression_ratio": 0.18521177315147164,
    "year": "2019"
  }
]