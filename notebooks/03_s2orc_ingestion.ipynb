{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2ORC (Semantic Scholar Open Research Corpus) Ingestion\n",
    "\n",
    "S2ORC is MASSIVE - it has millions of papers with full text and citation graphs.\n",
    "\n",
    "**Important:** We can't download the entire corpus (it's 100+ GB). We'll work with a subset.\n",
    "\n",
    "Options:\n",
    "1. Download specific slices (recommended)\n",
    "2. Use Semantic Scholar API for targeted queries\n",
    "3. Download sample datasets\n",
    "\n",
    "Let's go with option 2 (API) since it's most practical for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Semantic Scholar API\n",
    "\n",
    "They have a nice API that's free to use (with rate limits).\n",
    "\n",
    "Docs: https://api.semanticscholar.org/api-docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API base URL\n",
    "S2_API_BASE = \"https://api.semanticscholar.org/graph/v1\"\n",
    "\n",
    "# test the API with a simple query\n",
    "test_url = f\"{S2_API_BASE}/paper/search?query=natural+language+processing&limit=5\"\n",
    "\n",
    "response = requests.get(test_url)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f\"API works! Found {data.get('total', 0)} papers for test query\")\n",
    "    print(f\"\\nSample result:\")\n",
    "    if 'data' in data and len(data['data']) > 0:\n",
    "        print(json.dumps(data['data'][0], indent=2))\nelse:\n",
    "    print(f\"API error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_semantic_scholar(query, limit=100, fields=None):\n",
    "    \"\"\"\n",
    "    Search Semantic Scholar for papers matching query.\n",
    "    \n",
    "    Args:\n",
    "        query: search string\n",
    "        limit: max results (API allows up to 100 per request)\n",
    "        fields: list of fields to return (e.g., ['title', 'abstract', 'authors'])\n",
    "    \n",
    "    Returns:\n",
    "        list of paper dictionaries\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = ['paperId', 'title', 'abstract', 'authors', 'year', \n",
    "                  'venue', 'citationCount', 'referenceCount', 'fieldsOfStudy',\n",
    "                  'publicationDate', 'journal', 'externalIds']\n",
    "    \n",
    "    url = f\"{S2_API_BASE}/paper/search\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': min(limit, 100),\n",
    "        'fields': ','.join(fields)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('data', [])\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search queries for NLP-related topics\n",
    "# we'll search multiple topics to get diverse papers\n",
    "\n",
    "SEARCH_QUERIES = [\n",
    "    'natural language processing',\n",
    "    'transformers bert gpt',\n",
    "    'machine translation',\n",
    "    'sentiment analysis',\n",
    "    'named entity recognition',\n",
    "    'question answering',\n",
    "    'text summarization',\n",
    "    'language models',\n",
    "    'information extraction',\n",
    "    'semantic parsing',\n",
    "    'dialogue systems',\n",
    "    'text generation',\n",
    "]\n",
    "\n",
    "print(f\"Will search for {len(SEARCH_QUERIES)} different topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch papers for each query\n",
    "# note: API has rate limits, so we add delays\n",
    "\n",
    "all_papers = []\n",
    "papers_per_query = 100  # max allowed per request\n",
    "\n",
    "print(\"Fetching papers from Semantic Scholar...\\n\")\n",
    "\n",
    "for i, query in enumerate(SEARCH_QUERIES):\n",
    "    print(f\"{i+1}/{len(SEARCH_QUERIES)}: Searching for '{query}'...\", end=' ')\n",
    "    \n",
    "    papers = search_semantic_scholar(query, limit=papers_per_query)\n",
    "    all_papers.extend(papers)\n",
    "    \n",
    "    print(f\"got {len(papers)} papers\")\n",
    "    \n",
    "    # be nice to their API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nTotal papers fetched: {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates (same paper might appear in multiple searches)\n",
    "unique_ids = set()\n",
    "unique_papers = []\n",
    "\n",
    "for paper in all_papers:\n",
    "    paper_id = paper.get('paperId')\n",
    "    if paper_id and paper_id not in unique_ids:\n",
    "        unique_ids.add(paper_id)\n",
    "        unique_papers.append(paper)\n",
    "\n",
    "print(f\"Unique papers: {len(unique_papers)}\")\n",
    "print(f\"Duplicates removed: {len(all_papers) - len(unique_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what we got\n",
    "if unique_papers:\n",
    "    sample = unique_papers[0]\n",
    "    print(\"Sample paper structure:\")\n",
    "    print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize to our standard schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_s2orc_paper(paper):\n",
    "    \"\"\"\n",
    "    Convert S2 API response to our standard schema\n",
    "    \"\"\"\n",
    "    # extract author names\n",
    "    authors = []\n",
    "    if 'authors' in paper and paper['authors']:\n",
    "        authors = [a.get('name', '') for a in paper['authors'] if a.get('name')]\n",
    "    \n",
    "    # get venue info\n",
    "    venue = paper.get('venue', '')\n",
    "    if not venue and 'journal' in paper and paper['journal']:\n",
    "        venue = paper['journal'].get('name', '')\n",
    "    \n",
    "    # categories from fieldsOfStudy\n",
    "    categories = paper.get('fieldsOfStudy', [])\n",
    "    if not categories:\n",
    "        categories = []\n",
    "    \n",
    "    normalized = {\n",
    "        'paper_id': paper.get('paperId', ''),\n",
    "        'title': paper.get('title', ''),\n",
    "        'authors': authors,\n",
    "        'abstract': paper.get('abstract', ''),\n",
    "        'venue': venue,\n",
    "        'year': paper.get('year'),\n",
    "        'publication_date': paper.get('publicationDate', ''),\n",
    "        'citation_count': paper.get('citationCount', 0),\n",
    "        'reference_count': paper.get('referenceCount', 0),\n",
    "        'fields_of_study': categories,\n",
    "        'external_ids': paper.get('externalIds', {}),\n",
    "    }\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all papers\n",
    "normalized_papers = [normalize_s2orc_paper(p) for p in unique_papers]\n",
    "\n",
    "# filter out papers without titles or abstracts\n",
    "filtered_papers = [\n",
    "    p for p in normalized_papers \n",
    "    if p['title'] and p['abstract']\n",
    "]\n",
    "\n",
    "print(f\"Papers after filtering: {len(filtered_papers)}\")\n",
    "print(f\"Removed {len(normalized_papers) - len(filtered_papers)} papers without title/abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df = pd.DataFrame(filtered_papers)\n",
    "\n",
    "print(\"DataFrame info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some statistics\n",
    "print(\"Papers per year:\")\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "print(year_counts.tail(10))  # show last 10 years\n",
    "\n",
    "print(\"\\nCitation statistics:\")\n",
    "print(df['citation_count'].describe())\n",
    "\n",
    "print(\"\\nTop venues:\")\n",
    "print(df[df['venue'] != '']['venue'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check fields of study distribution\n",
    "from collections import Counter\n",
    "\n",
    "all_fields = []\n",
    "for fields in df['fields_of_study']:\n",
    "    if fields:\n",
    "        all_fields.extend(fields)\n",
    "\n",
    "field_counts = Counter(all_fields)\n",
    "print(\"Top fields of study:\")\n",
    "for field, count in field_counts.most_common(15):\n",
    "    print(f\"  {field}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to JSON and Parquet\n",
    "json_path = 'data/raw/s2orc_papers.json'\n",
    "parquet_path = 'data/raw/s2orc_papers.parquet'\n",
    "\n",
    "# JSON\n",
    "df.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
    "print(f\"Saved {len(df)} papers to {json_path}\")\n",
    "\n",
    "# Parquet\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "print(f\"Saved to {parquet_path}\")\n",
    "\n",
    "# file sizes\n",
    "json_size = os.path.getsize(json_path) / (1024 * 1024)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "print(f\"\\nJSON size: {json_size:.2f} MB\")\n",
    "print(f\"Parquet size: {parquet_size:.2f} MB\")\n",
    "print(f\"Compression: {json_size/parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nAbstract length stats:\")\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "print(df['abstract_length'].describe())\n",
    "\n",
    "# find very short abstracts\n",
    "short_abstracts = df[df['abstract_length'] < 100]\n",
    "print(f\"\\nPapers with abstracts < 100 chars: {len(short_abstracts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample papers\n",
    "print(\"Sample S2ORC papers:\\n\")\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['title']}\")\n",
    "    print(f\"   Year: {row['year']} | Venue: {row['venue']}\")\n",
    "    print(f\"   Citations: {row['citation_count']} | References: {row['reference_count']}\")\n",
    "    print(f\"   Authors: {', '.join(row['authors'][:2])}{'...' if len(row['authors']) > 2 else ''}\")\n",
    "    print(f\"   Fields: {', '.join(row['fields_of_study'][:3])}\")\n",
    "    print(f\"   Abstract: {row['abstract'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Fetch additional batches\n",
    "\n",
    "If we need more papers, we can use pagination or different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: fetch highly cited papers\n",
    "def get_highly_cited_papers(min_citations=100, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch highly cited NLP papers from S2.\n",
    "    Note: S2 API doesn't directly support citation count filtering,\n",
    "    so we fetch and filter.\n",
    "    \"\"\"\n",
    "    papers = search_semantic_scholar('natural language processing', limit=limit)\n",
    "    \n",
    "    # filter by citation count\n",
    "    highly_cited = [\n",
    "        p for p in papers \n",
    "        if p.get('citationCount', 0) >= min_citations\n",
    "    ]\n",
    "    \n",
    "    return highly_cited\n",
    "\n",
    "# uncomment to fetch\n",
    "# influential_papers = get_highly_cited_papers(min_citations=50)\n",
    "# print(f\"Found {len(influential_papers)} highly cited papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Used Semantic Scholar API to fetch papers  \n",
    "✅ Searched multiple NLP-related topics  \n",
    "✅ Collected ~1000+ papers with metadata  \n",
    "✅ Included citation counts and field classifications  \n",
    "✅ Normalized to standard schema  \n",
    "✅ Saved to JSON and Parquet  \n",
    "\n",
    "**Note:** We used API instead of bulk download due to size constraints. For production, consider:\n",
    "- Downloading specific S2ORC slices if full-text needed\n",
    "- Using S2 bulk metadata (much smaller than full corpus)\n",
    "- Implementing pagination for more comprehensive coverage\n",
    "\n",
    "**Next:** Combine all three datasets into unified format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
