{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Metadata Creation\n",
    "\n",
    "This notebook combines data from all three sources:\n",
    "- ArXiv\n",
    "- ACL Anthology\n",
    "- S2ORC (Semantic Scholar)\n",
    "\n",
    "**Goal:** Create a single unified dataset with consistent schema for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from all sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ArXiv data\n",
    "print(\"Loading ArXiv data...\")\n",
    "try:\n",
    "    arxiv_df = pd.read_parquet('data/raw/arxiv_papers.parquet')\n",
    "    print(f\"  Loaded {len(arxiv_df)} papers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"  ArXiv data not found! Run notebook 01 first.\")\n",
    "    arxiv_df = pd.DataFrame()\n",
    "\n",
    "# load ACL data\n",
    "print(\"\\nLoading ACL Anthology data...\")\n",
    "try:\n",
    "    acl_df = pd.read_parquet('data/raw/acl_anthology_papers.parquet')\n",
    "    print(f\"  Loaded {len(acl_df)} papers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"  ACL data not found! Run notebook 02 first.\")\n",
    "    acl_df = pd.DataFrame()\n",
    "\n",
    "# load S2ORC data\n",
    "print(\"\\nLoading S2ORC data...\")\n",
    "try:\n",
    "    s2orc_df = pd.read_parquet('data/raw/s2orc_papers.parquet')\n",
    "    print(f\"  Loaded {len(s2orc_df)} papers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"  S2ORC data not found! Run notebook 03 first.\")\n",
    "    s2orc_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns in each dataset\n",
    "print(\"ArXiv columns:\", arxiv_df.columns.tolist() if not arxiv_df.empty else \"N/A\")\n",
    "print(\"\\nACL columns:\", acl_df.columns.tolist() if not acl_df.empty else \"N/A\")\n",
    "print(\"\\nS2ORC columns:\", s2orc_df.columns.tolist() if not s2orc_df.empty else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define unified schema\n",
    "\n",
    "Our standard schema will have these fields:\n",
    "- `paper_id`: unique identifier\n",
    "- `title`: paper title\n",
    "- `authors`: list of author names\n",
    "- `abstract`: paper abstract\n",
    "- `venue`: publication venue\n",
    "- `year`: publication year\n",
    "- `categories`: topic categories/tags\n",
    "- `source`: which dataset it came from (arxiv/acl/s2orc)\n",
    "- `metadata`: any additional source-specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arxiv(df):\n",
    "    \"\"\"\n",
    "    Convert ArXiv dataframe to unified schema\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    normalized = pd.DataFrame({\n",
    "        'paper_id': 'arxiv_' + df['paper_id'].astype(str),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'],\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['categories'],\n",
    "        'source': 'arxiv',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'primary_category': row.get('primary_category', ''),\n",
    "            'published': row.get('published', ''),\n",
    "            'pdf_url': row.get('pdf_url', '')\n",
    "        }, axis=1)\n",
    "    })\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_acl(df):\n",
    "    \"\"\"\n",
    "    Convert ACL dataframe to unified schema\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # ACL papers might not have categories in the same format\n",
    "    # we'll infer it from venue\n",
    "    df['categories'] = df['venue'].apply(lambda x: ['NLP', 'Computational Linguistics'])\n",
    "    \n",
    "    normalized = pd.DataFrame({\n",
    "        'paper_id': 'acl_' + df['paper_id'].astype(str).str.replace('/', '_'),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'].fillna(''),  # some might be missing\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['categories'],\n",
    "        'source': 'acl',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'url': row.get('url', ''),\n",
    "            'doi': row.get('doi', ''),\n",
    "            'pages': row.get('pages', '')\n",
    "        }, axis=1)\n",
    "    })\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_s2orc(df):\n",
    "    \"\"\"\n",
    "    Convert S2ORC dataframe to unified schema\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # S2 has fields_of_study which we can use as categories\n",
    "    \n",
    "    normalized = pd.DataFrame({\n",
    "        'paper_id': 's2_' + df['paper_id'].astype(str),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'],\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['fields_of_study'],\n",
    "        'source': 's2orc',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'citation_count': row.get('citation_count', 0),\n",
    "            'reference_count': row.get('reference_count', 0),\n",
    "            'publication_date': row.get('publication_date', ''),\n",
    "            'external_ids': row.get('external_ids', {})\n",
    "        }, axis=1)\n",
    "    })\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each dataset\n",
    "print(\"Normalizing datasets...\\n\")\n",
    "\n",
    "arxiv_normalized = normalize_arxiv(arxiv_df)\n",
    "print(f\"ArXiv normalized: {len(arxiv_normalized)} papers\")\n",
    "\n",
    "acl_normalized = normalize_acl(acl_df)\n",
    "print(f\"ACL normalized: {len(acl_normalized)} papers\")\n",
    "\n",
    "s2orc_normalized = normalize_s2orc(s2orc_df)\n",
    "print(f\"S2ORC normalized: {len(s2orc_normalized)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all dataframes\n",
    "all_dfs = [arxiv_normalized, acl_normalized, s2orc_normalized]\n",
    "all_dfs = [df for df in all_dfs if not df.empty]  # filter out empty ones\n",
    "\n",
    "if all_dfs:\n",
    "    unified_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Combined dataset size: {len(unified_df)} papers\")\n",
    "else:\n",
    "    print(\"No data to combine!\")\n",
    "    unified_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the combined data\n",
    "if not unified_df.empty:\n",
    "    print(\"Unified dataset info:\")\n",
    "    print(f\"Shape: {unified_df.shape}\")\n",
    "    print(f\"\\nColumns: {unified_df.columns.tolist()}\")\n",
    "    print(f\"\\nSources distribution:\")\n",
    "    print(unified_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality checks and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial data quality checks:\\n\")\n",
    "\n",
    "# check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(unified_df.isnull().sum())\n",
    "\n",
    "# check for empty strings\n",
    "print(\"\\nEmpty titles:\", (unified_df['title'] == '').sum())\n",
    "print(\"Empty abstracts:\", (unified_df['abstract'] == '').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove papers without titles or abstracts\n",
    "print(f\"\\nBefore cleaning: {len(unified_df)} papers\")\n",
    "\n",
    "# filter requirements:\n",
    "# - must have title\n",
    "# - must have abstract (and not too short)\n",
    "# - must have at least one author\n",
    "\n",
    "cleaned_df = unified_df[\n",
    "    (unified_df['title'].str.len() > 10) &\n",
    "    (unified_df['abstract'].str.len() > 50) &\n",
    "    (unified_df['authors'].apply(len) > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(cleaned_df)} papers\")\n",
    "print(f\"Removed: {len(unified_df) - len(cleaned_df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for potential duplicates based on title similarity\n",
    "# exact title matches\n",
    "print(\"Checking for duplicate titles...\")\n",
    "duplicate_titles = cleaned_df[cleaned_df.duplicated(subset=['title'], keep=False)]\n",
    "print(f\"Found {len(duplicate_titles)} papers with duplicate titles\")\n",
    "\n",
    "if len(duplicate_titles) > 0:\n",
    "    print(\"\\nExample duplicates:\")\n",
    "    sample_dup = duplicate_titles.groupby('title').first().head(3)\n",
    "    for title in sample_dup.index:\n",
    "        dups = duplicate_titles[duplicate_titles['title'] == title]\n",
    "        print(f\"\\n'{title}'\")\n",
    "        print(f\"  Sources: {dups['source'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for duplicates, keep the one from the best source\n",
    "# priority: acl > s2orc > arxiv (ACL papers are peer-reviewed)\n",
    "\n",
    "source_priority = {'acl': 1, 's2orc': 2, 'arxiv': 3}\n",
    "cleaned_df['source_rank'] = cleaned_df['source'].map(source_priority)\n",
    "\n",
    "# sort by source rank and keep first (best source)\n",
    "deduped_df = cleaned_df.sort_values('source_rank').drop_duplicates(\n",
    "    subset=['title'], \n",
    "    keep='first'\n",
    ").drop('source_rank', axis=1)\n",
    "\n",
    "print(f\"\\nAfter deduplication: {len(deduped_df)} papers\")\n",
    "print(f\"Duplicates removed: {len(cleaned_df) - len(deduped_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add computed fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some useful computed fields\n",
    "print(\"Adding computed fields...\\n\")\n",
    "\n",
    "# text lengths\n",
    "deduped_df['title_length'] = deduped_df['title'].str.len()\n",
    "deduped_df['abstract_length'] = deduped_df['abstract'].str.len()\n",
    "\n",
    "# number of authors\n",
    "deduped_df['num_authors'] = deduped_df['authors'].apply(len)\n",
    "\n",
    "# number of categories\n",
    "deduped_df['num_categories'] = deduped_df['categories'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(\"Computed field statistics:\")\n",
    "print(f\"\\nTitle length: {deduped_df['title_length'].mean():.1f} chars (avg)\")\n",
    "print(f\"Abstract length: {deduped_df['abstract_length'].mean():.1f} chars (avg)\")\n",
    "print(f\"Authors per paper: {deduped_df['num_authors'].mean():.1f} (avg)\")\n",
    "print(f\"Categories per paper: {deduped_df['num_categories'].mean():.1f} (avg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year distribution\n",
    "print(\"\\nPapers per year:\")\n",
    "year_dist = deduped_df['year'].value_counts().sort_index()\n",
    "print(year_dist.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source distribution\n",
    "print(\"\\nPapers per source:\")\n",
    "print(deduped_df['source'].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print(deduped_df['source'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to processed directory\n",
    "output_json = 'data/processed/unified_papers.json'\n",
    "output_parquet = 'data/processed/unified_papers.parquet'\n",
    "output_csv = 'data/processed/unified_papers.csv'\n",
    "\n",
    "print(\"Saving unified dataset...\\n\")\n",
    "\n",
    "# JSON\n",
    "deduped_df.to_json(output_json, orient='records', indent=2, force_ascii=False)\n",
    "print(f\"Saved to {output_json}\")\n",
    "\n",
    "# Parquet (most efficient)\n",
    "deduped_df.to_parquet(output_parquet, index=False)\n",
    "print(f\"Saved to {output_parquet}\")\n",
    "\n",
    "# CSV (for easy inspection)\n",
    "# note: lists will be converted to strings in CSV\n",
    "csv_df = deduped_df.copy()\n",
    "csv_df['authors'] = csv_df['authors'].apply(lambda x: '|'.join(x))\n",
    "csv_df['categories'] = csv_df['categories'].apply(\n",
    "    lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "csv_df['metadata'] = csv_df['metadata'].apply(str)\n",
    "csv_df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check file sizes\n",
    "import os\n",
    "\n",
    "print(\"\\nFile sizes:\")\n",
    "for filepath in [output_json, output_parquet, output_csv]:\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"  {os.path.basename(filepath)}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a summary of the dataset\n",
    "summary = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'total_papers': len(deduped_df),\n",
    "    'sources': {\n",
    "        'arxiv': len(deduped_df[deduped_df['source'] == 'arxiv']),\n",
    "        'acl': len(deduped_df[deduped_df['source'] == 'acl']),\n",
    "        's2orc': len(deduped_df[deduped_df['source'] == 's2orc']),\n",
    "    },\n",
    "    'year_range': {\n",
    "        'min': int(deduped_df['year'].min()) if not deduped_df['year'].isna().all() else None,\n",
    "        'max': int(deduped_df['year'].max()) if not deduped_df['year'].isna().all() else None,\n",
    "    },\n",
    "    'statistics': {\n",
    "        'avg_title_length': float(deduped_df['title_length'].mean()),\n",
    "        'avg_abstract_length': float(deduped_df['abstract_length'].mean()),\n",
    "        'avg_authors_per_paper': float(deduped_df['num_authors'].mean()),\n",
    "        'avg_categories_per_paper': float(deduped_df['num_categories'].mean()),\n",
    "    },\n",
    "    'schema': {\n",
    "        'fields': deduped_df.columns.tolist(),\n",
    "        'description': 'Unified metadata from ArXiv, ACL Anthology, and S2ORC'\n",
    "    }\n",
    "}\n",
    "\n",
    "# save summary\n",
    "summary_path = 'data/processed/dataset_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample papers from final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show samples from each source\n",
    "print(\"Sample papers from unified dataset:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for source in ['arxiv', 'acl', 's2orc']:\n",
    "    source_papers = deduped_df[deduped_df['source'] == source]\n",
    "    if len(source_papers) > 0:\n",
    "        print(f\"\\n{source.upper()} Sample:\")\n",
    "        sample = source_papers.iloc[0]\n",
    "        print(f\"Title: {sample['title']}\")\n",
    "        print(f\"Authors: {', '.join(sample['authors'][:3])}{'...' if len(sample['authors']) > 3 else ''}\")\n",
    "        print(f\"Year: {sample['year']} | Venue: {sample['venue']}\")\n",
    "        print(f\"Categories: {sample['categories'][:3]}\")\n",
    "        print(f\"Abstract: {sample['abstract'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Loaded data from all three sources  \n",
    "✅ Normalized to unified schema  \n",
    "✅ Combined into single dataset  \n",
    "✅ Cleaned and deduplicated  \n",
    "✅ Added computed fields  \n",
    "✅ Saved in multiple formats (JSON, Parquet, CSV)  \n",
    "✅ Created dataset summary  \n",
    "\n",
    "**Files created:**\n",
    "- `data/processed/unified_papers.parquet` (main file for downstream tasks)\n",
    "- `data/processed/unified_papers.json` (for inspection)\n",
    "- `data/processed/unified_papers.csv` (for spreadsheet tools)\n",
    "- `data/processed/dataset_summary.json` (metadata)\n",
    "\n",
    "**Next steps:**\n",
    "- Data preprocessing (text cleaning, tokenization)\n",
    "- Embeddings generation\n",
    "- Classification and topic modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
