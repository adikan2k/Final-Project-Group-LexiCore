{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline: Data Ingestion ‚Üí Preprocessing\n",
    "\n",
    "This notebook combines all steps from notebooks 01-05 into a single pipeline:\n",
    "\n",
    "1. **ArXiv Ingestion** - Fetch papers from ArXiv API\n",
    "2. **ACL Anthology** - Download and parse ACL papers\n",
    "3. **S2ORC Ingestion** - Fetch from Semantic Scholar\n",
    "4. **Unified Metadata** - Combine and deduplicate\n",
    "5. **Preprocessing** - Clean and tokenize text\n",
    "\n",
    "**‚è±Ô∏è Total Runtime:** ~3-4 hours\n",
    "\n",
    "**üíæ Storage Required:** ~500MB-1GB\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "- [Setup & Installation](#setup)\n",
    "- [Part 1: ArXiv Ingestion](#part1)\n",
    "- [Part 2: ACL Anthology](#part2)\n",
    "- [Part 3: S2ORC Ingestion](#part3)\n",
    "- [Part 4: Unified Metadata](#part4)\n",
    "- [Part 5: Preprocessing](#part5)\n",
    "- [Final Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup & Installation\n",
    "\n",
    "Install all required packages upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all required packages\n",
    "!pip install pandas arxiv requests beautifulsoup4 lxml pyarrow spacy nltk tqdm -q\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# data ingestion\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# colab\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory structure\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('src/preprocessing', exist_ok=True)\n",
    "\n",
    "print(\"‚úì Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"‚úì NLP models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part1'></a>\n",
    "# Part 1: ArXiv Ingestion\n",
    "\n",
    "Fetch papers from ArXiv API for NLP-related categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_papers(categories, max_results_per_category=300):\n",
    "    \"\"\"\n",
    "    Fetch papers from arxiv for given categories.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(f\"Fetching papers from {cat}...\")\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query = f'cat:{cat}',\n",
    "            max_results = max_results_per_category,\n",
    "            sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "        \n",
    "        count = 0\n",
    "        for result in client.results(search):\n",
    "            paper = {\n",
    "                'paper_id': result.entry_id.split('/')[-1],\n",
    "                'title': result.title,\n",
    "                'authors': [author.name for author in result.authors],\n",
    "                'abstract': result.summary.replace('\\n', ' '),\n",
    "                'categories': result.categories,\n",
    "                'primary_category': result.primary_category,\n",
    "                'published': result.published.isoformat(),\n",
    "                'updated': result.updated.isoformat(),\n",
    "                'pdf_url': result.pdf_url,\n",
    "                'venue': 'arXiv',\n",
    "                'year': result.published.year\n",
    "            }\n",
    "            papers.append(paper)\n",
    "            count += 1\n",
    "            \n",
    "            if count % 50 == 0:\n",
    "                print(f\"  Fetched {count} papers from {cat}...\")\n",
    "        \n",
    "        print(f\"Completed {cat}: {count} papers fetched\\n\")\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch ArXiv papers\n",
    "TARGET_CATEGORIES = ['cs.CL', 'cs.LG', 'stat.ML']\n",
    "\n",
    "print(\"Starting ArXiv ingestion...\\n\")\n",
    "arxiv_papers = fetch_arxiv_papers(TARGET_CATEGORIES, max_results_per_category=300)\n",
    "\n",
    "print(f\"\\n‚úì Total ArXiv papers fetched: {len(arxiv_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ArXiv data\n",
    "arxiv_df = pd.DataFrame(arxiv_papers)\n",
    "arxiv_df.to_parquet('data/raw/arxiv_papers.parquet', index=False)\n",
    "\n",
    "print(f\"‚úì Saved {len(arxiv_df)} ArXiv papers to data/raw/arxiv_papers.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/raw/arxiv_papers.parquet') / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part2'></a>\n",
    "# Part 2: ACL Anthology Ingestion\n",
    "\n",
    "Download and parse ACL Anthology BibTeX dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ACL anthology BibTeX\n",
    "print(\"Downloading ACL Anthology BibTeX file...\")\n",
    "!wget https://aclanthology.org/anthology.bib.gz -O data/raw/acl_anthology.bib.gz -q\n",
    "!gunzip -f data/raw/acl_anthology.bib.gz\n",
    "\n",
    "print(\"‚úì Downloaded and extracted ACL Anthology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bibtex_file(filepath):\n",
    "    \"\"\"\n",
    "    Parse BibTeX file and extract paper metadata.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        current_entry = {}\n",
    "        in_entry = False\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('@'):\n",
    "                if current_entry and 'title' in current_entry:\n",
    "                    papers.append(current_entry)\n",
    "                \n",
    "                parts = line[1:].split('{')\n",
    "                if len(parts) == 2:\n",
    "                    entry_type = parts[0].lower()\n",
    "                    paper_id = parts[1].rstrip(',')\n",
    "                    current_entry = {'paper_id': paper_id, 'entry_type': entry_type}\n",
    "                    in_entry = True\n",
    "            \n",
    "            elif line.startswith('}'):\n",
    "                if current_entry and 'title' in current_entry:\n",
    "                    papers.append(current_entry)\n",
    "                current_entry = {}\n",
    "                in_entry = False\n",
    "            \n",
    "            elif in_entry and '=' in line:\n",
    "                field_parts = line.split('=', 1)\n",
    "                if len(field_parts) == 2:\n",
    "                    field_name = field_parts[0].strip()\n",
    "                    field_value = field_parts[1].strip().strip(',').strip('{}').strip('\"')\n",
    "                    current_entry[field_name] = field_value\n",
    "            \n",
    "            if len(papers) % 5000 == 0 and len(papers) > 0:\n",
    "                print(f\"  Parsed {len(papers)} papers...\")\n",
    "    \n",
    "    if current_entry and 'title' in current_entry:\n",
    "        papers.append(current_entry)\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse BibTeX file\n",
    "print(\"Parsing ACL BibTeX file (this takes a few minutes)...\\n\")\n",
    "acl_papers = parse_bibtex_file('data/raw/acl_anthology.bib')\n",
    "\n",
    "print(f\"\\n‚úì Total ACL papers parsed: {len(acl_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_acl_paper(paper):\n",
    "    \"\"\"Convert BibTeX entry to standard schema.\"\"\"\n",
    "    authors = []\n",
    "    if 'author' in paper:\n",
    "        authors = [a.strip() for a in paper['author'].split(' and ')]\n",
    "    \n",
    "    venue = paper.get('booktitle', paper.get('journal', 'ACL Anthology'))\n",
    "    \n",
    "    year = None\n",
    "    if 'year' in paper:\n",
    "        try:\n",
    "            year = int(paper['year'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        'paper_id': paper.get('paper_id', ''),\n",
    "        'title': paper.get('title', ''),\n",
    "        'authors': authors,\n",
    "        'abstract': paper.get('abstract', ''),\n",
    "        'venue': venue,\n",
    "        'year': year,\n",
    "        'url': paper.get('url', ''),\n",
    "        'doi': paper.get('doi', ''),\n",
    "        'pages': paper.get('pages', ''),\n",
    "    }\n",
    "\n",
    "# normalize and filter\n",
    "print(\"Normalizing ACL papers...\")\n",
    "acl_normalized = [normalize_acl_paper(p) for p in acl_papers]\n",
    "\n",
    "acl_df = pd.DataFrame(acl_normalized)\n",
    "acl_df = acl_df[acl_df['title'].str.len() > 0]\n",
    "acl_df = acl_df[acl_df['year'] >= 2015]  # filter to recent papers\n",
    "\n",
    "print(f\"‚úì Filtered to {len(acl_df)} ACL papers (2015+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ACL data\n",
    "acl_df.to_parquet('data/raw/acl_anthology_papers.parquet', index=False)\n",
    "\n",
    "print(f\"‚úì Saved {len(acl_df)} ACL papers to data/raw/acl_anthology_papers.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/raw/acl_anthology_papers.parquet') / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part3'></a>\n",
    "# Part 3: S2ORC Ingestion\n",
    "\n",
    "Fetch papers from Semantic Scholar API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_API_BASE = \"https://api.semanticscholar.org/graph/v1\"\n",
    "\n",
    "def search_semantic_scholar(query, limit=100):\n",
    "    \"\"\"\n",
    "    Search Semantic Scholar for papers.\n",
    "    \"\"\"\n",
    "    fields = ['paperId', 'title', 'abstract', 'authors', 'year', \n",
    "              'venue', 'citationCount', 'referenceCount', 'fieldsOfStudy',\n",
    "              'publicationDate', 'journal', 'externalIds']\n",
    "    \n",
    "    url = f\"{S2_API_BASE}/paper/search\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': min(limit, 100),\n",
    "        'fields': ','.join(fields)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('data', [])\n",
    "        else:\n",
    "            print(f\"  Error {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"  Request failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search queries\n",
    "SEARCH_QUERIES = [\n",
    "    'natural language processing',\n",
    "    'transformers bert gpt',\n",
    "    'machine translation',\n",
    "    'sentiment analysis',\n",
    "    'named entity recognition',\n",
    "    'question answering',\n",
    "    'text summarization',\n",
    "    'language models',\n",
    "]\n",
    "\n",
    "print(\"Fetching papers from Semantic Scholar...\\n\")\n",
    "\n",
    "all_s2_papers = []\n",
    "for i, query in enumerate(SEARCH_QUERIES):\n",
    "    print(f\"{i+1}/{len(SEARCH_QUERIES)}: '{query}'...\", end=' ')\n",
    "    papers = search_semantic_scholar(query, limit=100)\n",
    "    all_s2_papers.extend(papers)\n",
    "    print(f\"got {len(papers)} papers\")\n",
    "    time.sleep(1)  # respect API limits\n",
    "\n",
    "print(f\"\\n‚úì Total S2 papers fetched: {len(all_s2_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate\n",
    "unique_ids = set()\n",
    "unique_s2_papers = []\n",
    "\n",
    "for paper in all_s2_papers:\n",
    "    paper_id = paper.get('paperId')\n",
    "    if paper_id and paper_id not in unique_ids:\n",
    "        unique_ids.add(paper_id)\n",
    "        unique_s2_papers.append(paper)\n",
    "\n",
    "print(f\"‚úì Unique S2 papers: {len(unique_s2_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_s2orc_paper(paper):\n",
    "    \"\"\"Convert S2 API response to standard schema.\"\"\"\n",
    "    authors = []\n",
    "    if 'authors' in paper and paper['authors']:\n",
    "        authors = [a.get('name', '') for a in paper['authors'] if a.get('name')]\n",
    "    \n",
    "    venue = paper.get('venue', '')\n",
    "    if not venue and 'journal' in paper and paper['journal']:\n",
    "        venue = paper['journal'].get('name', '')\n",
    "    \n",
    "    categories = paper.get('fieldsOfStudy', [])\n",
    "    if not categories:\n",
    "        categories = []\n",
    "    \n",
    "    return {\n",
    "        'paper_id': paper.get('paperId', ''),\n",
    "        'title': paper.get('title', ''),\n",
    "        'authors': authors,\n",
    "        'abstract': paper.get('abstract', ''),\n",
    "        'venue': venue,\n",
    "        'year': paper.get('year'),\n",
    "        'publication_date': paper.get('publicationDate', ''),\n",
    "        'citation_count': paper.get('citationCount', 0),\n",
    "        'reference_count': paper.get('referenceCount', 0),\n",
    "        'fields_of_study': categories,\n",
    "        'external_ids': paper.get('externalIds', {}),\n",
    "    }\n",
    "\n",
    "# normalize and filter\n",
    "s2_normalized = [normalize_s2orc_paper(p) for p in unique_s2_papers]\n",
    "s2_filtered = [p for p in s2_normalized if p['title'] and p['abstract']]\n",
    "\n",
    "s2_df = pd.DataFrame(s2_filtered)\n",
    "\n",
    "print(f\"‚úì Filtered to {len(s2_df)} S2 papers with title and abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save S2ORC data\n",
    "s2_df.to_parquet('data/raw/s2orc_papers.parquet', index=False)\n",
    "\n",
    "print(f\"‚úì Saved {len(s2_df)} S2ORC papers to data/raw/s2orc_papers.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/raw/s2orc_papers.parquet') / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part4'></a>\n",
    "# Part 4: Unified Metadata\n",
    "\n",
    "Combine all three sources into a unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "arxiv_df = pd.read_parquet('data/raw/arxiv_papers.parquet')\n",
    "acl_df = pd.read_parquet('data/raw/acl_anthology_papers.parquet')\n",
    "s2_df = pd.read_parquet('data/raw/s2orc_papers.parquet')\n",
    "\n",
    "print(f\"  ArXiv: {len(arxiv_df)} papers\")\n",
    "print(f\"  ACL: {len(acl_df)} papers\")\n",
    "print(f\"  S2ORC: {len(s2_df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arxiv(df):\n",
    "    \"\"\"Normalize ArXiv to unified schema.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'paper_id': 'arxiv_' + df['paper_id'].astype(str),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'],\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['categories'],\n",
    "        'source': 'arxiv',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'primary_category': row.get('primary_category', ''),\n",
    "            'published': row.get('published', ''),\n",
    "            'pdf_url': row.get('pdf_url', '')\n",
    "        }, axis=1)\n",
    "    })\n",
    "\n",
    "def normalize_acl(df):\n",
    "    \"\"\"Normalize ACL to unified schema.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['categories'] = df['venue'].apply(lambda x: ['NLP', 'Computational Linguistics'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'paper_id': 'acl_' + df['paper_id'].astype(str).str.replace('/', '_'),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'].fillna(''),\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['categories'],\n",
    "        'source': 'acl',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'url': row.get('url', ''),\n",
    "            'doi': row.get('doi', ''),\n",
    "            'pages': row.get('pages', '')\n",
    "        }, axis=1)\n",
    "    })\n",
    "\n",
    "def normalize_s2orc(df):\n",
    "    \"\"\"Normalize S2ORC to unified schema.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'paper_id': 's2_' + df['paper_id'].astype(str),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'],\n",
    "        'venue': df['venue'],\n",
    "        'year': df['year'],\n",
    "        'categories': df['fields_of_study'],\n",
    "        'source': 's2orc',\n",
    "        'metadata': df.apply(lambda row: {\n",
    "            'citation_count': row.get('citation_count', 0),\n",
    "            'reference_count': row.get('reference_count', 0),\n",
    "            'publication_date': row.get('publication_date', ''),\n",
    "            'external_ids': row.get('external_ids', {})\n",
    "        }, axis=1)\n",
    "    })\n",
    "\n",
    "# normalize all\n",
    "print(\"\\nNormalizing datasets...\")\n",
    "arxiv_norm = normalize_arxiv(arxiv_df)\n",
    "acl_norm = normalize_acl(acl_df)\n",
    "s2_norm = normalize_s2orc(s2_df)\n",
    "\n",
    "print(f\"  ArXiv normalized: {len(arxiv_norm)}\")\n",
    "print(f\"  ACL normalized: {len(acl_norm)}\")\n",
    "print(f\"  S2ORC normalized: {len(s2_norm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all\n",
    "all_dfs = [arxiv_norm, acl_norm, s2_norm]\n",
    "all_dfs = [df for df in all_dfs if not df.empty]\n",
    "\n",
    "unified_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úì Combined dataset: {len(unified_df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and deduplicate\n",
    "print(\"\\nCleaning and deduplicating...\")\n",
    "\n",
    "# filter\n",
    "cleaned_df = unified_df[\n",
    "    (unified_df['title'].str.len() > 10) &\n",
    "    (unified_df['abstract'].str.len() > 50) &\n",
    "    (unified_df['authors'].apply(len) > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"  After filtering: {len(cleaned_df)} papers\")\n",
    "\n",
    "# deduplicate by title\n",
    "source_priority = {'acl': 1, 's2orc': 2, 'arxiv': 3}\n",
    "cleaned_df['source_rank'] = cleaned_df['source'].map(source_priority)\n",
    "\n",
    "deduped_df = cleaned_df.sort_values('source_rank').drop_duplicates(\n",
    "    subset=['title'], \n",
    "    keep='first'\n",
    ").drop('source_rank', axis=1)\n",
    "\n",
    "print(f\"  After deduplication: {len(deduped_df)} papers\")\n",
    "print(f\"  Duplicates removed: {len(cleaned_df) - len(deduped_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add computed fields\n",
    "deduped_df['title_length'] = deduped_df['title'].str.len()\n",
    "deduped_df['abstract_length'] = deduped_df['abstract'].str.len()\n",
    "deduped_df['num_authors'] = deduped_df['authors'].apply(len)\n",
    "deduped_df['num_categories'] = deduped_df['categories'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Added computed fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unified dataset\n",
    "deduped_df.to_parquet('data/processed/unified_papers.parquet', index=False)\n",
    "\n",
    "print(f\"\\n‚úì Saved {len(deduped_df)} papers to data/processed/unified_papers.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/processed/unified_papers.parquet') / (1024*1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nSource distribution:\")\n",
    "print(deduped_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part5'></a>\n",
    "# Part 5: Preprocessing\n",
    "\n",
    "Clean and preprocess text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing functions\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, emails, and extra whitespace.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"Remove special characters.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?-]', '', text)\n",
    "    text = re.sub(r'([.,!?-])\\1+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens.\"\"\"\n",
    "    text = ' '.join(tokens)\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def preprocess_text(text, lowercase=True, remove_stops=True, lemmatize=True):\n",
    "    \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return {\n",
    "            'cleaned_text': '',\n",
    "            'tokens': [],\n",
    "            'processed_text': ''\n",
    "        }\n",
    "    \n",
    "    # clean\n",
    "    text = clean_text(text)\n",
    "    text = remove_special_chars(text)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    cleaned_text = text\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stops:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # filter\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t.isalnum()]\n",
    "    \n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return {\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'tokens': tokens,\n",
    "        'processed_text': processed_text\n",
    "    }\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unified dataset and sample\n",
    "df = pd.read_parquet('data/processed/unified_papers.parquet')\n",
    "\n",
    "sample_size = 500\n",
    "if len(df) > sample_size:\n",
    "    sample_df = df.sample(n=sample_size, random_state=42).copy()\n",
    "else:\n",
    "    sample_df = df.copy()\n",
    "\n",
    "print(f\"‚úì Sampled {len(sample_df)} papers for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all abstracts\n",
    "print(f\"\\nProcessing {len(sample_df)} abstracts...\\n\")\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    result = preprocess_text(row['abstract'])\n",
    "    \n",
    "    processed_data.append({\n",
    "        'paper_id': row['paper_id'],\n",
    "        'title': row['title'],\n",
    "        'original_abstract': row['abstract'],\n",
    "        'cleaned_text': result['cleaned_text'],\n",
    "        'tokens': result['tokens'],\n",
    "        'processed_text': result['processed_text'],\n",
    "        'num_tokens': len(result['tokens']),\n",
    "        'source': row['source'],\n",
    "        'year': row['year']\n",
    "    })\n",
    "\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "print(\"\\nPreprocessing statistics:\")\n",
    "print(f\"  Average tokens per paper: {processed_df['num_tokens'].mean():.1f}\")\n",
    "print(f\"  Min tokens: {processed_df['num_tokens'].min()}\")\n",
    "print(f\"  Max tokens: {processed_df['num_tokens'].max()}\")\n",
    "\n",
    "# vocabulary\n",
    "all_tokens = []\n",
    "for tokens in processed_df['tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"\\n  Vocabulary size: {len(token_counts)}\")\n",
    "print(f\"  Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "print(\"\\n  Top 20 tokens:\")\n",
    "for token, count in token_counts.most_common(20):\n",
    "    print(f\"    {token}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "processed_df.to_parquet('data/processed/preprocessed_sample_500.parquet', index=False)\n",
    "\n",
    "# save vocabulary\n",
    "vocab_data = {\n",
    "    'vocab_size': len(token_counts),\n",
    "    'total_tokens': len(all_tokens),\n",
    "    'vocabulary': sorted(list(token_counts.keys())),\n",
    "    'token_frequencies': dict(token_counts.most_common(1000))\n",
    "}\n",
    "\n",
    "with open('data/processed/vocabulary.json', 'w') as f:\n",
    "    json.dump(vocab_data, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Saved preprocessed data to data/processed/preprocessed_sample_500.parquet\")\n",
    "print(\"‚úì Saved vocabulary to data/processed/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='summary'></a>\n",
    "# Final Summary\n",
    "\n",
    "Pipeline complete! Here's what we accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì• DATA INGESTION\")\n",
    "print(f\"  ArXiv papers:     {len(arxiv_df):,}\")\n",
    "print(f\"  ACL papers:       {len(acl_df):,}\")\n",
    "print(f\"  S2ORC papers:     {len(s2_df):,}\")\n",
    "print(f\"  Total ingested:   {len(arxiv_df) + len(acl_df) + len(s2_df):,}\")\n",
    "\n",
    "print(\"\\nüîó UNIFICATION\")\n",
    "print(f\"  Unified papers:   {len(deduped_df):,}\")\n",
    "print(f\"  Duplicates removed: {len(unified_df) - len(deduped_df):,}\")\n",
    "\n",
    "print(\"\\nüßπ PREPROCESSING\")\n",
    "print(f\"  Preprocessed:     {len(processed_df):,} papers\")\n",
    "print(f\"  Vocabulary size:  {len(token_counts):,} tokens\")\n",
    "print(f\"  Avg tokens/paper: {processed_df['num_tokens'].mean():.1f}\")\n",
    "\n",
    "print(\"\\nüìÅ OUTPUT FILES\")\n",
    "output_files = [\n",
    "    'data/raw/arxiv_papers.parquet',\n",
    "    'data/raw/acl_anthology_papers.parquet',\n",
    "    'data/raw/s2orc_papers.parquet',\n",
    "    'data/processed/unified_papers.parquet',\n",
    "    'data/processed/preprocessed_sample_500.parquet',\n",
    "    'data/processed/vocabulary.json'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for filepath in output_files:\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  ‚úì {filepath} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\n  Total storage: {total_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Next Steps:\")\n",
    "print(\"  1. Generate embeddings (Word2Vec, BERT)\")\n",
    "print(\"  2. Train classification models\")\n",
    "print(\"  3. Build topic models\")\n",
    "print(\"  4. Create retrieval system\")\n",
    "print(\"  5. Develop research digest interface\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
