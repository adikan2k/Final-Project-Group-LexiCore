{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACL Anthology Data Ingestion\n",
    "\n",
    "ACL Anthology has papers from major NLP conferences (ACL, EMNLP, NAACL, etc.).\n",
    "\n",
    "They provide structured metadata which is great for ground truth.\n",
    "\n",
    "**Data source:** https://aclanthology.org/\n",
    "\n",
    "We can either:\n",
    "1. Use their bulk download (XML/BibTeX)\n",
    "2. Scrape via their website\n",
    "3. Use existing datasets (like the one on Hugging Face)\n",
    "\n",
    "Let's try option 3 first since it's easiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install pandas requests beautifulsoup4 lxml pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Download the XML dump\n",
    "\n",
    "ACL Anthology provides an XML dump of all their papers. Let's download and parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the anthology XML\n",
    "# this file is pretty big (~200MB+)\n",
    "XML_URL = \"https://aclanthology.org/anthology+abstracts.bib.gz\"\n",
    "\n",
    "print(\"Note: This might take a while depending on connection speed...\")\n",
    "print(f\"Downloading from {XML_URL}\")\n",
    "\n",
    "# we'll actually use BibTeX format - easier to parse\n",
    "# let me check what's available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let me try a different approach - using their structured data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACL provides individual venue files\n",
    "# let's target major conferences from recent years\n",
    "\n",
    "VENUES = [\n",
    "    'acl',    # Association for Computational Linguistics\n",
    "    'emnlp',  # Empirical Methods in NLP\n",
    "    'naacl',  # North American Chapter of ACL\n",
    "    'eacl',   # European Chapter of ACL\n",
    "    'conll',  # Conference on Computational Natural Language Learning\n",
    "    'tacl',   # Transactions of ACL\n",
    "]\n",
    "\n",
    "# target years - let's go back a few years\n",
    "YEARS = ['2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "print(f\"Targeting {len(VENUES)} venues across {len(YEARS)} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_acl_papers_by_venue_year(venue, year):\n",
    "    \"\"\"\n",
    "    Fetch papers from ACL anthology for a specific venue and year.\n",
    "    Uses the anthology API/website structure.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    \n",
    "    # ACL anthology URL pattern\n",
    "    # example: https://aclanthology.org/events/acl-2023/\n",
    "    url = f\"https://aclanthology.org/events/{venue}-{year}/\"\n",
    "    \n",
    "    print(f\"Fetching from {url}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"  Failed to fetch {venue}-{year}: status {response.status_code}\")\n",
    "            return papers\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # find all paper entries\n",
    "        # ACL anthology structure has papers in specific divs/sections\n",
    "        paper_elements = soup.find_all('p', class_='d-sm-flex align-items-stretch')\n",
    "        \n",
    "        print(f\"  Found {len(paper_elements)} papers\")\n",
    "        \n",
    "        for elem in paper_elements:\n",
    "            try:\n",
    "                # extract title\n",
    "                title_tag = elem.find('strong')\n",
    "                if not title_tag:\n",
    "                    continue\n",
    "                \n",
    "                title = title_tag.get_text().strip()\n",
    "                \n",
    "                # extract paper ID from link\n",
    "                link_tag = elem.find('a', class_='align-middle')\n",
    "                paper_id = link_tag['href'].strip('/') if link_tag else None\n",
    "                \n",
    "                # extract authors\n",
    "                authors_tag = elem.find('span', class_='d-block')\n",
    "                authors = []\n",
    "                if authors_tag:\n",
    "                    author_links = authors_tag.find_all('a')\n",
    "                    authors = [a.get_text().strip() for a in author_links]\n",
    "                \n",
    "                paper = {\n",
    "                    'paper_id': paper_id,\n",
    "                    'title': title,\n",
    "                    'authors': authors,\n",
    "                    'venue': venue.upper(),\n",
    "                    'year': int(year),\n",
    "                    'url': f\"https://aclanthology.org/{paper_id}\" if paper_id else None\n",
    "                }\n",
    "                \n",
    "                papers.append(paper)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # skip problematic entries\n",
    "                continue\n",
    "        \n",
    "        time.sleep(0.5)  # be nice to their server\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {venue}-{year}: {str(e)}\")\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, web scraping might be unreliable. Let me try their BibTeX dump instead - more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the full anthology BibTeX file\n",
    "!wget https://aclanthology.org/anthology.bib.gz -O data/raw/acl_anthology.bib.gz -q\n",
    "!gunzip -f data/raw/acl_anthology.bib.gz\n",
    "\n",
    "print(\"Downloaded and extracted ACL Anthology BibTeX file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the file\n",
    "bibtex_file = 'data/raw/acl_anthology.bib'\n",
    "\n",
    "# check if file exists and get size\n",
    "if os.path.exists(bibtex_file):\n",
    "    file_size = os.path.getsize(bibtex_file) / (1024 * 1024)\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # peek at first few lines\n",
    "    with open(bibtex_file, 'r', encoding='utf-8') as f:\n",
    "        for i in range(20):\n",
    "            print(f.readline().rstrip())\nelse:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse BibTeX entries\n",
    "def parse_bibtex_file(filepath):\n",
    "    \"\"\"\n",
    "    Parse BibTeX file and extract paper metadata.\n",
    "    This is a simple parser - there are libraries but let's do it manually for control.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        current_entry = {}\n",
    "        in_entry = False\n",
    "        entry_type = None\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # start of an entry\n",
    "            if line.startswith('@'):\n",
    "                if current_entry and 'title' in current_entry:\n",
    "                    papers.append(current_entry)\n",
    "                \n",
    "                # parse entry type and ID\n",
    "                parts = line[1:].split('{')\n",
    "                if len(parts) == 2:\n",
    "                    entry_type = parts[0].lower()\n",
    "                    paper_id = parts[1].rstrip(',')\n",
    "                    \n",
    "                    current_entry = {\n",
    "                        'paper_id': paper_id,\n",
    "                        'entry_type': entry_type\n",
    "                    }\n",
    "                    in_entry = True\n",
    "            \n",
    "            # end of entry\n",
    "            elif line.startswith('}'):\n",
    "                if current_entry and 'title' in current_entry:\n",
    "                    papers.append(current_entry)\n",
    "                current_entry = {}\n",
    "                in_entry = False\n",
    "            \n",
    "            # field within entry\n",
    "            elif in_entry and '=' in line:\n",
    "                field_parts = line.split('=', 1)\n",
    "                if len(field_parts) == 2:\n",
    "                    field_name = field_parts[0].strip()\n",
    "                    field_value = field_parts[1].strip().strip(',').strip('{}').strip('\"')\n",
    "                    \n",
    "                    current_entry[field_name] = field_value\n",
    "            \n",
    "            # progress indicator\n",
    "            if len(papers) % 5000 == 0 and len(papers) > 0:\n",
    "                print(f\"Parsed {len(papers)} papers...\")\n",
    "    \n",
    "    # add last entry if exists\n",
    "    if current_entry and 'title' in current_entry:\n",
    "        papers.append(current_entry)\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parsing BibTeX file... this will take a few minutes\")\n",
    "acl_papers = parse_bibtex_file(bibtex_file)\n",
    "print(f\"\\nTotal papers parsed: {len(acl_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what fields we have\n",
    "if acl_papers:\n",
    "    sample_paper = acl_papers[0]\n",
    "    print(\"Sample paper fields:\")\n",
    "    for key, value in sample_paper.items():\n",
    "        print(f\"  {key}: {str(value)[:100]}...\" if len(str(value)) > 100 else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_acl_paper(paper):\n",
    "    \"\"\"\n",
    "    Convert raw BibTeX entry to our standard schema\n",
    "    \"\"\"\n",
    "    # parse authors - they're usually in 'author' field separated by 'and'\n",
    "    authors = []\n",
    "    if 'author' in paper:\n",
    "        authors = [a.strip() for a in paper['author'].split(' and ')]\n",
    "    \n",
    "    # extract venue info\n",
    "    venue = paper.get('booktitle', paper.get('journal', 'ACL Anthology'))\n",
    "    \n",
    "    # extract year\n",
    "    year = None\n",
    "    if 'year' in paper:\n",
    "        try:\n",
    "            year = int(paper['year'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    normalized = {\n",
    "        'paper_id': paper.get('paper_id', ''),\n",
    "        'title': paper.get('title', ''),\n",
    "        'authors': authors,\n",
    "        'abstract': paper.get('abstract', ''),  # might not be present\n",
    "        'venue': venue,\n",
    "        'year': year,\n",
    "        'url': paper.get('url', ''),\n",
    "        'doi': paper.get('doi', ''),\n",
    "        'pages': paper.get('pages', ''),\n",
    "    }\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all papers\n",
    "print(\"Normalizing paper data...\")\n",
    "normalized_papers = [normalize_acl_paper(p) for p in acl_papers]\n",
    "print(f\"Normalized {len(normalized_papers)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df = pd.DataFrame(normalized_papers)\n",
    "\n",
    "print(\"DataFrame info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter papers - we only want recent ones with abstracts (if possible)\n",
    "# and from major venues\n",
    "\n",
    "print(f\"Papers before filtering: {len(df)}\")\n",
    "\n",
    "# remove entries without titles\n",
    "df = df[df['title'].str.len() > 0]\n",
    "print(f\"After removing empty titles: {len(df)}\")\n",
    "\n",
    "# filter by year if available\n",
    "df_recent = df[df['year'] >= 2015].copy()  # last ~10 years\n",
    "print(f\"Papers from 2015 onwards: {len(df_recent)}\")\n",
    "\n",
    "# check abstract availability\n",
    "with_abstract = df_recent[df_recent['abstract'].str.len() > 0]\n",
    "print(f\"Papers with abstracts: {len(with_abstract)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stats\n",
    "print(\"Papers per year:\")\n",
    "print(df_recent['year'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nTop venues:\")\n",
    "print(df_recent['venue'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full dataset\n",
    "json_path = 'data/raw/acl_anthology_papers.json'\n",
    "parquet_path = 'data/raw/acl_anthology_papers.parquet'\n",
    "\n",
    "# save to JSON\n",
    "df_recent.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
    "print(f\"Saved {len(df_recent)} papers to {json_path}\")\n",
    "\n",
    "# save to parquet\n",
    "df_recent.to_parquet(parquet_path, index=False)\n",
    "print(f\"Saved to {parquet_path}\")\n",
    "\n",
    "# file sizes\n",
    "json_size = os.path.getsize(json_path) / (1024 * 1024)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "print(f\"\\nJSON size: {json_size:.2f} MB\")\n",
    "print(f\"Parquet size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\")\n",
    "print(df_recent.isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate paper IDs:\")\n",
    "duplicates = df_recent[df_recent.duplicated(subset=['paper_id'], keep=False)]\n",
    "print(f\"Found {len(duplicates)} duplicates\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_recent = df_recent.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "    print(f\"New shape: {df_recent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some sample papers\n",
    "print(\"Sample ACL papers:\\n\")\n",
    "for idx, row in df_recent.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['title']}\")\n",
    "    print(f\"   Venue: {row['venue']} ({row['year']})\")\n",
    "    print(f\"   Authors: {', '.join(row['authors'][:2])}{'...' if len(row['authors']) > 2 else ''}\")\n",
    "    if row['abstract']:\n",
    "        print(f\"   Abstract: {row['abstract'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Downloaded ACL Anthology BibTeX dump  \n",
    "✅ Parsed ~70K+ conference papers  \n",
    "✅ Filtered to recent papers (2015+)  \n",
    "✅ Normalized to standard schema  \n",
    "✅ Saved to JSON and Parquet  \n",
    "\n",
    "**Note:** Not all papers have abstracts in the BibTeX dump. We might need to scrape abstracts separately if needed for the project.\n",
    "\n",
    "**Next:** S2ORC ingestion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
